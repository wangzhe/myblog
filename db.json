{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"cc75fbdb977a72e3c33a32b977ec965c1597d5c5","modified":1546432944691},{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1523872364264},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1523872777084},{"_id":"themes/landscape/_config.yml","hash":"e1fa116f99b8ab7c23ab75b59932a1a5b9aeb1e3","modified":1523872777112},{"_id":"themes/landscape/README.md","hash":"c7e83cfe8f2c724fc9cac32bd71bb5faf9ceeddb","modified":1523872777111},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1523872777084},{"_id":"themes/landscape/package.json","hash":"85358dc34311c6662e841584e206a4679183943f","modified":1523872777111},{"_id":"themes/landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1523872364265},{"_id":"themes/landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1523872364265},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1523872364265},{"_id":"themes/landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1523872364265},{"_id":"themes/landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1523872364265},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1523872364266},{"_id":"themes/landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1523872364266},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1523872364266},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1523872364266},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1523872777112},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1523872364267},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1523872364266},{"_id":"themes/landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1523872364266},{"_id":"source/_posts/.DS_Store","hash":"f05079312a4923b8228686c2df1092057fe6ac8f","modified":1578320766125},{"_id":"source/_posts/about-data.md","hash":"9adfae56ca0721f027df396c9458de04d4db1ec6","modified":1523872777071},{"_id":"source/_posts/aliyun-server-less.md","hash":"afd1f5111a0bd230accc13830773fe470cf56bac","modified":1575010471841},{"_id":"source/_posts/build-testing-team.md","hash":"940813c1dee4532e43edf9fc7355ec348b81763d","modified":1523872777040},{"_id":"source/_posts/cto-summary.md","hash":"5517243713e64199be3fe67416b7d96e4e192ee2","modified":1523872777073},{"_id":"source/_posts/cto-study-summary-1516.md","hash":"264bb2199b4195abfed5ad0aa69d01a185d6d845","modified":1523872777032},{"_id":"source/_posts/datamodel.md","hash":"c603319588e0690d6b71aee283a1dba98c19c870","modified":1523872777080},{"_id":"source/_posts/deep-learning-ai-1.md","hash":"9bb949a3ce52c83dca32cadf53087f9952f0cbd7","modified":1525345327200},{"_id":"source/_posts/collection-n-set.md","hash":"a672c9eaccf6c5545d8fbb8ba10ebbef397c9097","modified":1523872777055},{"_id":"source/_posts/deep-learning-ai-2-1.md","hash":"8e78b795c81cd415d22f970c49c17077dff93c15","modified":1525346045130},{"_id":"source/_posts/deep-learning-ai-2.md","hash":"862acff413f1ecea808be582decdf98335b9aede","modified":1525344457284},{"_id":"source/_posts/deep-learning-sequence-models-w2.md","hash":"f29abc18462d71123e868e610f6ae998d56592b6","modified":1536468958167},{"_id":"source/_posts/deep-learning-sequence-models-w3.md","hash":"265fd8b00ebe55fc3ef2ca76be3d2e3d7931401a","modified":1541731151071},{"_id":"source/_posts/deep-learning-study.md","hash":"eb35bf581a9622ef5875f85d26e34ff932b53c3b","modified":1536385354551},{"_id":"source/_posts/deeplearning-3-1.md","hash":"4eae4de0c092e32437e7eff783c77a823b5fde74","modified":1526701025298},{"_id":"source/_posts/exo.md","hash":"313a280419683288977934dc299c9cab9ef57aca","modified":1523872777075},{"_id":"source/_posts/deeplearning-4-1.md","hash":"773c257d172ca75191eaf80ef8d2a3dc3c8d7d00","modified":1527428700718},{"_id":"source/_posts/deeplearning-course2-2-1.md","hash":"eb4e75c0c4162140f0fc74b2d358d9fc0eec69b2","modified":1527907392242},{"_id":"source/_posts/four-things.md","hash":"6f1e58fb26dd432b51eb3ebc12614ba62a50b581","modified":1523872777083},{"_id":"source/_posts/hello-world.md","hash":"7c04dc9f0d87919a090a5dd02b87207c68f561f7","modified":1523872777034},{"_id":"source/_posts/gcp-study.md","hash":"50a9ddc1936f7ab23eea5b28fdae2371edc9e721","modified":1575535749265},{"_id":"source/_posts/iso27001.md","hash":"4617106826b9db5602b9bc2debf6c100d3b40e58","modified":1523872777076},{"_id":"source/_posts/milestone.md","hash":"ef5942d81865b53cd411aab8449688f5f05a3192","modified":1523872777071},{"_id":"source/_posts/it-in-med.md","hash":"5caff7f721113f4fd66a5c773ff4bec2d2664464","modified":1523872777037},{"_id":"source/_posts/no-back-end-design.md","hash":"fe2c570f6660eebcd7cd0bd46fe11020c160e4be","modified":1523872777070},{"_id":"source/_posts/hello-world2.md","hash":"c5db39d176f94a481d96b158bb3160c628163e33","modified":1523872777082},{"_id":"source/_posts/object-detection.md","hash":"d7404959afb81423b72bbc96909bd13be2a7f06d","modified":1534090727041},{"_id":"source/_posts/ocr.md","hash":"a6d32ef5e92e284fa6e1a6e21adbffc393493ba7","modified":1523872777039},{"_id":"source/_posts/okr-review.md","hash":"4dcf1632014f8a6aa83b6075ca0ce43ab7bb84e9","modified":1523872777071},{"_id":"source/_posts/polar-review.md","hash":"7c0709e8d019b8079ae9c1de18442be603e53d1f","modified":1523872777056},{"_id":"source/_posts/positioning.md","hash":"33ed47a6fb3d5bd74fef1f9eedbcbf9c806f67bd","modified":1523872777072},{"_id":"source/_posts/prepare-design-thinking.md","hash":"019e8ffe62c16970b1ba762969424ac817e2295b","modified":1523872777034},{"_id":"source/_posts/hippa-part1.md","hash":"4ccda0ffe01f2e2f92012a1adc3c9911e6ceb24c","modified":1523872777034},{"_id":"source/_posts/html2ppt.md","hash":"11dcfadc08328938b7f19971f4e967669ccc47dd","modified":1524494779588},{"_id":"source/_posts/python-db-basic.md","hash":"ccd57c5b097fc84442b63fa014357e8f09bd7457","modified":1523872777082},{"_id":"source/_posts/python-misc.md","hash":"f35858bc09ec1752c08ae527c66b7205acdfa2ce","modified":1523872777039},{"_id":"source/_posts/python3upgrade.md","hash":"fd30f0e407464cfa44cf6d1ac33aea866137df2f","modified":1523872777033},{"_id":"source/_posts/tech-driven.md","hash":"4bcbf4fcb10faa3958f9a6aa14532b18dc403bee","modified":1523872777072},{"_id":"source/_posts/today.md","hash":"105ee372a611e116d1cf4fda34af32fa12a818d8","modified":1523872777049},{"_id":"source/_posts/wechat-content.md","hash":"82fe68a172e9554ace3a5cf266bea9354b2a1c5f","modified":1523872777074},{"_id":"source/_posts/tech-destruction-of-medicine.md","hash":"e11963684437a3e24a6c5e72f75f9d28fc9d4a91","modified":1523872777039},{"_id":"source/_posts/wechat.md","hash":"f8162e0366848fd66f1415e2664b8115c9c771df","modified":1523872777040},{"_id":"source/_posts/tensorflow-notes.md","hash":"b54ea43b57d06919e1f70e213514accda73087df","modified":1536490207301},{"_id":"source/_posts/wwdc.md","hash":"b7e7a8bc9075e9bd7cf5842966e89056378b68c2","modified":1523872777074},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1523872777086},{"_id":"source/_posts/unicode.md","hash":"aa268174461157a773d5fe1a3c58a71373941ec1","modified":1523872777073},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1523872777085},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1523872777085},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1523872777086},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1523872777093},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1523872777086},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1523872777085},{"_id":"source/_posts/no-back-end-sys.md","hash":"8b4856e01ee0fb3c195f8c5c1ddc43dd711913d6","modified":1523872777083},{"_id":"source/_posts/some-python-cmd.md","hash":"6321e53235460c500070c29b2ab10df429e051f5","modified":1523872777049},{"_id":"themes/landscape/source/css/_variables.styl","hash":"5e37a6571caf87149af83ac1cc0cdef99f117350","modified":1523872777103},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1523872777098},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1523872777099},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1523872777098},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1523872777110},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1523872777096},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1523872777098},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1523872777097},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1523872777096},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1523872777111},{"_id":"source/_posts/aliyun-server-less/aliyun.png","hash":"4e538636e38cadf009ba5b9b884eb07549ac292d","modified":1546432750252},{"_id":"source/_posts/collection-n-set/russell_paradox.png","hash":"85f8c0cd59c619ee6d0141b7aa9d748682155bf3","modified":1523872777033},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1523872777096},{"_id":"source/_posts/cto-summary/branding.png","hash":"a4734b7670f1a5859f0bc08e933e01c7a442c561","modified":1523872777077},{"_id":"source/_posts/cto-summary/client.png","hash":"ff3a8f62e2aae2c1103397063e09ba8f8e978514","modified":1523872777079},{"_id":"source/_posts/cto-summary/overall.png","hash":"fe572070f7f8621c11d5eeac558e21a872dfb8dc","modified":1523872777078},{"_id":"source/_posts/cto-summary/operation.png","hash":"00d4e492a49f9715b510954b22dc080ab0394b66","modified":1523872777079},{"_id":"themes/landscape/source/css/style.styl","hash":"278d1458b968a151c27b87643191d2d7a8129511","modified":1523872777108},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1523872777097},{"_id":"source/_posts/deep-learning-sequence-models-w2/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1536387306897},{"_id":"source/_posts/deeplearning-3-1/.DS_Store","hash":"d829bc37de5eb62ea92c6098bddad284da05e1d2","modified":1526700360595},{"_id":"source/_posts/exo/dashboard.png","hash":"56ef527ebf2bed551541682c553ec065b8c8f392","modified":1523872777057},{"_id":"source/_posts/exo/cover.jpeg","hash":"f46d8a0bc723502dc0618ddd5f9442416aa5cc49","modified":1523872777057},{"_id":"source/_posts/four-things/mgnt2.png","hash":"46e606ee2f1d16d1ae4ae4f438d561fc81ed06ca","modified":1523872777076},{"_id":"source/_posts/hippa-part1/evolution1996.jpg","hash":"d3bfa62cbb536ad5a8f36e755a745e91bdc8e99a","modified":1523872777051},{"_id":"source/_posts/hippa-part1/evolution2009.jpg","hash":"8a15aa0bb1e2d52067e72c530e8b9aa13f9e8bb5","modified":1523872777051},{"_id":"source/_posts/hippa-part1/evolution1997.jpg","hash":"8e49105a3a1a1626ee6e686a8898e7761237bce4","modified":1523872777050},{"_id":"source/_posts/hippa-part1/evolution2013.jpg","hash":"ada5ae1994698342c71b9648601c69dce8c1dc1b","modified":1523872777051},{"_id":"source/_posts/hippa-part1/hippa.jpg","hash":"0d5a59b342a615a5d7c257c08b8271b8443c246e","modified":1523872777052},{"_id":"source/_posts/html2ppt/test.h","hash":"1310fdc3d7e6f43bc705e6a6206369d668cdaaa3","modified":1523874294324},{"_id":"source/_posts/hippa-part1/security.jpg","hash":"64de7f939fc51cf407ccd9f65496574d38b49171","modified":1523872777052},{"_id":"source/_posts/no-back-end-sys/hubot.jpg","hash":"d876cb5255e8d3ce9fe1ed54f94980c47772f736","modified":1523872777041},{"_id":"source/_posts/no-back-end-sys/siri_cort_now.jpg","hash":"3ef414449da2c69495b67d60bf86f792da6e16cb","modified":1523872777041},{"_id":"source/_posts/datamodel/poisson_distribution.png","hash":"1e480e117952a143753239c0b9ae03d32145c1ea","modified":1523872777080},{"_id":"source/_posts/datamodel/efficency48h.png","hash":"26073db5c4d4d013c33e602a8c574e6e87e676ed","modified":1523872777081},{"_id":"source/_posts/okr-review/OKR_review.png","hash":"63267b7f9e62585e19aa63be8e7829e5d6829ce6","modified":1523872777048},{"_id":"source/_posts/prepare-design-thinking/幻灯片01.jpg","hash":"7ddd7281ce26fd05ac901b02ca4795a6f2999fcc","modified":1523872777066},{"_id":"source/_posts/prepare-design-thinking/幻灯片03.jpg","hash":"d7201bc7daf9fc2b446c15ddb1c4e2c787723ae7","modified":1523872777065},{"_id":"source/_posts/prepare-design-thinking/幻灯片04.jpg","hash":"2fe5d86363214014f1b9eeca83e661aea6cd4c5a","modified":1523872777067},{"_id":"source/_posts/prepare-design-thinking/幻灯片02.jpg","hash":"8d043f359d081abc02e8bc1a48e15f7fdc45f8d4","modified":1523872777064},{"_id":"source/_posts/prepare-design-thinking/幻灯片06.jpg","hash":"cbe39f777d34fc691d34415a94bdfa668210200a","modified":1523872777070},{"_id":"source/_posts/prepare-design-thinking/幻灯片07.jpg","hash":"e37d8829cd0dc934a75191c406cc3d4451174e33","modified":1523872777068},{"_id":"source/_posts/prepare-design-thinking/幻灯片08.jpg","hash":"742e9e6c3875641c52f7611dadfcf03a6d7f906f","modified":1523872777060},{"_id":"source/_posts/prepare-design-thinking/幻灯片10.jpg","hash":"533ee5379033beb30a1e44dd1ca6a114e54fd1a1","modified":1523872777067},{"_id":"source/_posts/prepare-design-thinking/幻灯片09.jpg","hash":"81884468c137f569e1de84446e86b51def031d12","modified":1523872777061},{"_id":"source/_posts/prepare-design-thinking/幻灯片11.jpg","hash":"8f68ac589c5ea6cc8d7b079252fe8b6a03229bde","modified":1523872777068},{"_id":"source/_posts/prepare-design-thinking/幻灯片12.jpg","hash":"ecca25e9a791ad6a76f00721fa138bbfa83efa5b","modified":1523872777069},{"_id":"source/_posts/prepare-design-thinking/幻灯片14.jpg","hash":"a8db00f85e3e9d04ef2e25b720e57d4fae327259","modified":1523872777066},{"_id":"source/_posts/prepare-design-thinking/幻灯片13.jpg","hash":"2756c749da7a482d8e024cd5bc1c90d94c673fe6","modified":1523872777069},{"_id":"source/_posts/prepare-design-thinking/幻灯片15.jpg","hash":"169534623a77926d56dc627382174c620cc6250a","modified":1523872777065},{"_id":"source/_posts/prepare-design-thinking/幻灯片16.jpg","hash":"b3d325bdfeef8677c5df44cb12df5fd1f0f0450e","modified":1523872777064},{"_id":"source/_posts/prepare-design-thinking/幻灯片17.jpg","hash":"0e85509bb2823fd4e4a8fc4e26e2dc2252dd0eee","modified":1523872777064},{"_id":"source/_posts/prepare-design-thinking/幻灯片18.jpg","hash":"8a6e3dbe0e69efe237f8d0b299f2b7e6df7742e2","modified":1523872777062},{"_id":"source/_posts/prepare-design-thinking/幻灯片19.jpg","hash":"9c899000c76601619a5a83f04d7ab317ce970a34","modified":1523872777061},{"_id":"source/_posts/prepare-design-thinking/幻灯片21.jpg","hash":"ccd4035ee6b0c4754d318994a5f60e6acf24f3b3","modified":1523872777061},{"_id":"source/_posts/prepare-design-thinking/幻灯片23.jpg","hash":"dd5af89ad78ffcd23660091a0dde4e7bcf894f8a","modified":1523872777059},{"_id":"source/_posts/prepare-design-thinking/幻灯片22.jpg","hash":"da50c14a6b616d05c681ae499b71d0ef5885b801","modified":1523872777059},{"_id":"source/_posts/prepare-design-thinking/幻灯片24.jpg","hash":"50fa2150a1a71263e7419dae01272fe1f9a2ee04","modified":1523872777062},{"_id":"source/_posts/prepare-design-thinking/幻灯片25.jpg","hash":"df934996cb163a9a2e8c844e19cffec25e29caac","modified":1523872777062},{"_id":"source/_posts/prepare-design-thinking/幻灯片27.jpg","hash":"44dd4e14265fa88ca9847e23c6f04c826592bd3a","modified":1523872777063},{"_id":"source/_posts/python3upgrade/unicode.png","hash":"5dad093f62ab5b7d5e149fe4bf69a5b30213f548","modified":1523872777082},{"_id":"source/_posts/unicode/example.png","hash":"1b589b89c29709f7f46be089144927c4aa299330","modified":1523872777047},{"_id":"source/_posts/unicode/utf8.png","hash":"b452971e5f3f67beeacc7764a675b9b56a5da678","modified":1523872777042},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1523872777089},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"1711d15c0ca561f8029a167fb7e7b00ff6b162a5","modified":1524492156713},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"d7de6421497ffaf65e4f5fe4bed71fcea51fde80","modified":1523872777090},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"01201725258ad2aaee606cd4dfdebc21b646def1","modified":1523872777092},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1523872777090},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"4c0fb5cd9179f60e443e6efa9734d5d755ef6893","modified":1523872777091},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"da9a4ab32efc44c098f317fe64e2335989929b0e","modified":1523872777091},{"_id":"themes/landscape/layout/_partial/gauges-analytics.ejs","hash":"aad6312ac197d6c5aaf2104ac863d7eba46b772a","modified":1523872364268},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"6387a93dad7c3d778eb91e3821852fbf6813880c","modified":1523872777090},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1523872777093},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1523872777092},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"36ab37878129d152e3cbdeb839c08e52af1acd58","modified":1523872777094},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"985fbeb01142b9d526cda8ebc372c1d361d69a6b","modified":1523872777094},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"feba7c00fa59ba13bf870b358a499fde4473d335","modified":1523872777095},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"34dc8cdd96cdb41dd11cb7513f13714373e5104a","modified":1523872777093},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"b3f321ddda6be2702a286d5b11af9533509506fb","modified":1523872777094},{"_id":"source/_posts/cto-summary/RnD.png","hash":"a7d87df5e102ea071880eebdf4c1b4c171dbdebc","modified":1523872777079},{"_id":"source/_posts/cto-summary/strategy.png","hash":"651df7d3712e452e1ad34c299c62d01b4a5d6ca2","modified":1523872777078},{"_id":"source/_posts/datamodel/efficency24n48.png","hash":"da7ef4477407de8284eb3472d15bbb706718c004","modified":1523872777081},{"_id":"source/_posts/deeplearning-course2-2-1/gradient_descent.png","hash":"0ec041168e2b77348633d6b4146fe10e4cc7c872","modified":1527906587635},{"_id":"source/_posts/deeplearning-course2-2-1/mini_batch_gradient_descent.png","hash":"0d260276c9f622437b5b089cca1ac8cb6cececf5","modified":1527906607801},{"_id":"source/_posts/deeplearning-course2-2-1/stochastic_gradient_descent.png","hash":"a0ecf4824beaa807dfc13c1a1d56183da87db189","modified":1527906594648},{"_id":"source/_posts/iso27001/doc_list.png","hash":"3d0dd74263b74c5a83d13e2d9ed4bd6e4f8b5c00","modified":1523872777053},{"_id":"source/_posts/no-back-end-sys/terminal.png","hash":"5ab1bfa5d44d8023a2bd144770f1b7f91b348b12","modified":1523872777041},{"_id":"source/_posts/okr-review/okr_timeline.png","hash":"b1f2b455a752a4c4459bc08b949f1794b9914737","modified":1523872777048},{"_id":"source/_posts/prepare-design-thinking/幻灯片20.jpg","hash":"aa849c29dda183a4cd0133c07957b5f16cc155d1","modified":1523872777060},{"_id":"source/_posts/prepare-design-thinking/幻灯片26.jpg","hash":"f70af2a0246fc73f8d7db93a808d7bdea31b5fcc","modified":1523872777063},{"_id":"source/_posts/unicode/CJK.png","hash":"94141ea15a15a7a19c4e0d687af3654a59be4c68","modified":1523872777042},{"_id":"source/_posts/unicode/character_in_word.png","hash":"0ff7132a215ded2ffaf491fe5d82156249c4ad93","modified":1523872777043},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1523872777106},{"_id":"source/_posts/tech-driven/report.jpg","hash":"0759a5901af55be334ccfd0e6b5a740ecf467c32","modified":1523872777038},{"_id":"source/_posts/unicode/ascii.png","hash":"c88b96c17d6092e940c1736a1fbcfdcc4900f51f","modified":1523872777043},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1523872777107},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1523872777106},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1523872777104},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"36eefe6332b86b66023a9884b754d305235846b4","modified":1523872777104},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1523872777107},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1523872777107},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"bc5487b9a0bfe5f745423331824d3f3637ccd430","modified":1523872777105},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1523872777102},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"b7bdc11effa98c6d88850eff75634e2ea9207c14","modified":1523872777106},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1523872777102},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1523872777109},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1523872777110},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1523872777110},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1523872777099},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1523872777101},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1523872777101},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1523872777100},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1523872777100},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1523872777100},{"_id":"source/_posts/cto-summary/team.png","hash":"27a77eaa65657e23f51c0775d21a617e0c8abd6c","modified":1523872777077},{"_id":"source/_posts/deep-learning-ai-1/nerual_network.png","hash":"31b7e1e61517bc89adeb2dee610e90e944b0c868","modified":1525344879289},{"_id":"source/_posts/deep-learning-ai-2-1/jupyter.png","hash":"8563a7857a45be24823a382b6835514d3b922951","modified":1524725412136},{"_id":"source/_posts/deep-learning-ai-1/why_takeoff.png","hash":"e87bb4b4c51260601be51f2c55930dd5c3d59f53","modified":1525345260228},{"_id":"source/_posts/deep-learning-sequence-models-w2/sentiment.png","hash":"c379ee2edf2336a7fce107077c0f6961cca4100b","modified":1536466094656},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1523872777087},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1523872777089},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1523872777088},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"f26d30355ba9144c51e700e8edc6a4ab6144ff9a","modified":1523872777088},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1523872777088},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1523872777089},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1523872777109},{"_id":"source/_posts/deep-learning-ai-1/stuctured_unstructured.png","hash":"5bb371c37217e90f3f5bebf168d34e0eb862d372","modified":1525344658144},{"_id":"source/_posts/deep-learning-ai-1/supervised_learning.png","hash":"daad0b60d63e166269ab0bee8b526cb82778547e","modified":1525344633174},{"_id":"source/_posts/deeplearning-3-1/hl50.png","hash":"1efc91f90d01e623f59b78d9d2dd73c91999a763","modified":1526697966017},{"_id":"source/_posts/prepare-design-thinking/幻灯片05.jpg","hash":"807d832f12938d3e9ad7ce80c5bf611898134504","modified":1523872777068},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1523872777105},{"_id":"source/_posts/iso27001/doc.png","hash":"fd62e19557d817e6cb2958847832cef1abb664fe","modified":1523872777053},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1523872777108},{"_id":"source/_posts/deep-learning-sequence-models-w2/rnn.png","hash":"f8ecccc285d3e30753637adfb758562fe1f55903","modified":1536467725061},{"_id":"source/_posts/okr-review/fortynight_review.png","hash":"ce99356b0e61bc721d84c9de7bddfdd4dec3b7c6","modified":1523872777049},{"_id":"source/_posts/hello-world2/img2.jpg","hash":"4e3f3c7c8eaef91a4c75eb186333e4dfe996ab49","modified":1523872777035},{"_id":"source/_posts/hello-world2/img1.jpg","hash":"da08a0d4d9fd2c7694b64c4c8ebfdc5e9b3a502b","modified":1523872777036},{"_id":"source/_posts/four-things/mgnt.png","hash":"90651ccfb25d0d836c5267afb6948a88ecc5e52b","modified":1523872777075},{"_id":"source/_posts/deeplearning-3-1/hl5_20.png","hash":"70eec566b3f93acd4c3b1d6827d588815978aaa9","modified":1526697953834},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"843d9d47bf2b7b75495db11b3d765efaaae442a9","modified":1523872777103},{"_id":"source/_posts/iso27001/list.png","hash":"3beeb301009eef35b11f1d26fe47205492e53977","modified":1523872777054},{"_id":"source/_posts/deep-learning-sequence-models-w2/featurized.png","hash":"2d8149f1a7c841a331cbb110a22782ee6e0dce72","modified":1536387264000},{"_id":"source/_posts/tech-driven/ocr.png","hash":"a9666567cde860cd833157cd3310f539655b942c","modified":1523872777038},{"_id":"source/_posts/deeplearning-3-1/hl3_4.png","hash":"ebdc16a71edbb7d5b3b452cb7bd4191b5ffc61ba","modified":1526697941632},{"_id":"source/_posts/about-data/data.png","hash":"93ef508ecca690ee8f59126e2db2b1c03f2971c8","modified":1523872777074},{"_id":"source/_posts/exo/core.jpg","hash":"0d1bec360910713928abf5bcc9af4f7e35c0e38b","modified":1523872777058},{"_id":"source/_posts/object-detection/startyolo.png","hash":"aea344cbabc5fdebe3c306bff2c0af4bf79053bd","modified":1534079352081},{"_id":"source/_posts/hello-world2/cover.jpg","hash":"bd2449b4b5a04a73756ede58e71eede4b436fba8","modified":1523872777036},{"_id":"source/_posts/deep-learning-sequence-models-w3/error_analysis.png","hash":"2980d62c42a7ebad04d150717d9c6c8393d50cbd","modified":1537021915151},{"_id":"source/_posts/deep-learning-ai-2-1/closing.jpg","hash":"c0844769875e4be6017a760b680af5678f068a43","modified":1525335157034},{"_id":"source/_posts/.gcp-study.md.swp","hash":"2b28bb9badac6bf9f491bdaf9e8ead7c4c71890d","modified":1575607114547},{"_id":"source/_posts/gcp-study/sql_network.png","hash":"113939199aebbc2136a11ba5a00d542da670de12","modified":1575344383975},{"_id":"source/_posts/summary2019.md","hash":"07e7c8714acab606ceb18ea3cc7ea4f307512cd6","modified":1578321067291},{"_id":"source/_posts/summary2019/stability_4.png","hash":"a5a929bdc2db8302347984a6f4310eb3f9031fc3","modified":1578320643953},{"_id":"source/_posts/summary2019/stability_1.png","hash":"f4424111c2f22884b63ebb7528b647fe332020a4","modified":1578320364287},{"_id":"source/_posts/summary2019/stability_4_2.png","hash":"93a68554bb7d2a4beb9bf8ec2be0cf1a99379bcd","modified":1578320528375},{"_id":"source/_posts/summary2019/stability_4_1.png","hash":"7dca0652f7705b3fdcda94dfc8949578bb2075d1","modified":1578320511363},{"_id":"source/_posts/summary2019/stability_4_4.png","hash":"7d9e1a8f08c2ea1d68d1a5f10859a7716cda8813","modified":1578320549319},{"_id":"source/_posts/summary2019/stability_3.png","hash":"521e8a0e47e2e9c3394eee572567dbff6d2e595a","modified":1578320501416},{"_id":"source/_posts/summary2019/stability_2.png","hash":"1a7ea3d81b4bf18141d3c28dc12772f9e4221ce1","modified":1578320378683},{"_id":"source/_posts/summary2019/stability_4_3.png","hash":"1591dc948d6b118fc41b1a65581a25427a2cb9ac","modified":1578320539240},{"_id":"public/atom.xml","hash":"fa018377ec3fc11dc8af170699151d366492301e","modified":1578321102399},{"_id":"public/sitemap.xml","hash":"e3e557dbd22af4f1f1e6527695ca8769ae28e49b","modified":1578321102413},{"_id":"public/2019/01/02/aliyun-server-less/index.html","hash":"b25a04bbd80d8b2f0bcf4d6ddd6b1996d313d380","modified":1578321102462},{"_id":"public/2018/04/16/html2ppt/index.html","hash":"092ab07a303d0cb35b4d1443f738278b82ebff3b","modified":1578321102463},{"_id":"public/2016/07/03/no-back-end-design/index.html","hash":"26f1489eebb964d7e6e0f65a156b53c3870a5899","modified":1578321102463},{"_id":"public/2015/06/01/some-python-cmd/index.html","hash":"ad58b42db684efe6f8cffaf88c49782309ce9a53","modified":1578321102463},{"_id":"public/2014/12/19/ocr/index.html","hash":"6dda08591e995b7737a4a7b58234756661d2318b","modified":1578321102463},{"_id":"public/2014/11/27/today/index.html","hash":"a07cb8c6ae8eb13e741945778a5e5a0079d2a419","modified":1578321102463},{"_id":"public/archives/2014/11/index.html","hash":"f704e775dfd51afe11c796550067eaa71cba2a24","modified":1578321102463},{"_id":"public/archives/2015/06/index.html","hash":"d210d5431bd9d4c13e95465413b7885f22df7072","modified":1578321102463},{"_id":"public/archives/2015/08/index.html","hash":"32a4069034d5cd9099cf110d59324e26a8e7e184","modified":1578321102463},{"_id":"public/archives/2016/09/index.html","hash":"d21a5422e88c0c04ea895d44ebee97cb862f69ad","modified":1578321102463},{"_id":"public/archives/2019/01/index.html","hash":"f0c5a85064c471e3b35d045d9dbeab549c1d9de4","modified":1578321102463},{"_id":"public/archives/2018/06/index.html","hash":"eb818f5af5624c4469223bd6b034a88d7bc05e6b","modified":1578321102463},{"_id":"public/tags/deeplearning/page/2/index.html","hash":"5c6d5b51b2f760136821d30c7af0b5cfa8b610a1","modified":1578321102463},{"_id":"public/tags/math/index.html","hash":"cdb8ce76acee8304af0b7611246b061994f93cc6","modified":1578321102464},{"_id":"public/tags/operation/index.html","hash":"982698232f78b8b76dc3ab1105fca36f89c60e04","modified":1578321102464},{"_id":"public/2020/01/06/summary2019/index.html","hash":"cdde9c1cecc7d89782ffec5d38c15fc76ec941ca","modified":1578321102464},{"_id":"public/2019/12/02/gcp-study/index.html","hash":"8408f64e4c01fc3a8892f17b57390bee15951274","modified":1578321102464},{"_id":"public/2018/09/13/deep-learning-sequence-models-w3/index.html","hash":"10e51f140edfd1ab7b53a84d1840b3db2a13f553","modified":1578321102464},{"_id":"public/2018/09/08/deep-learning-sequence-models-w2/index.html","hash":"b69b23ff09e457a968cef8e8ada8d3e1835ccb91","modified":1578321102464},{"_id":"public/2018/08/12/object-detection/index.html","hash":"c070dfd7d1294aa375ce70afaed68596295cac1c","modified":1578321102464},{"_id":"public/2018/08/12/tensorflow-notes/index.html","hash":"c1d131dbe25ab334ad095347f718cdc65d4b4146","modified":1578321102464},{"_id":"public/2018/06/01/deeplearning-course2-2-1/index.html","hash":"cf7b867221e2e47e391b8de190098c087c755175","modified":1578321102464},{"_id":"public/2018/05/18/deeplearning-4-1/index.html","hash":"23c51502aaaf86ff334f0aa91e4e052f503b54cb","modified":1578321102464},{"_id":"public/2018/05/04/deeplearning-3-1/index.html","hash":"b2008a918fe52e14bf7e490a5a33d04c31984f64","modified":1578321102464},{"_id":"public/2018/04/25/deep-learning-ai-2-1/index.html","hash":"78c4399839a3956a50d260fc061c6ae58754a6f6","modified":1578321102464},{"_id":"public/2018/04/25/deep-learning-ai-2/index.html","hash":"500cf3b87183a3d519796fe2d7db0e4c0d6fb9ae","modified":1578321102464},{"_id":"public/2018/04/24/deep-learning-ai-1/index.html","hash":"75c30f7f516254e9253fe8795fa9ad702605283e","modified":1578321102465},{"_id":"public/2018/04/23/deep-learning-study/index.html","hash":"bda5a16b1a20688f0304f7e8af6ddc8e35d9fe24","modified":1578321102465},{"_id":"public/2016/10/07/cto-study-summary-1516/index.html","hash":"a742226f758985f863e57d93d92288fe55151c36","modified":1578321102465},{"_id":"public/2016/09/13/milestone/index.html","hash":"b3e9a12775c44dec10de320f7b3e64b3d633d783","modified":1578321102465},{"_id":"public/2016/07/24/cto-summary/index.html","hash":"15e7506d529dca80f5bc7d1c4b237923a33f2c00","modified":1578321102465},{"_id":"public/2016/07/03/iso27001/index.html","hash":"e0d3cedaf901698503ae640ac941c843eae84f3d","modified":1578321102465},{"_id":"public/2016/07/02/no-back-end-sys/index.html","hash":"ada5d43671673fb66982c49d274946709bbcb9a7","modified":1578321102465},{"_id":"public/2016/06/14/tech-driven/index.html","hash":"b65590dece0050916dd9438faa523600a021d26f","modified":1578321102465},{"_id":"public/2016/06/04/unicode/index.html","hash":"9c953ed0a1b716ae4c8ab86d3959b299b16c2e0c","modified":1578321102465},{"_id":"public/2016/05/22/python3upgrade/index.html","hash":"b8aa041535c95b1785543c1cde0178c4c6642dff","modified":1578321102465},{"_id":"public/2016/04/25/it-in-med/index.html","hash":"12ed6097f7b4e92fc108a45f15a6eaf279d29df0","modified":1578321102465},{"_id":"public/2016/04/10/datamodel/index.html","hash":"59257a6fab6ca7426559087e3ccb94cc34ba2c1a","modified":1578321102465},{"_id":"public/2016/03/27/okr-review/index.html","hash":"0e7fa58d714441d314ba3d843d8ed10d734e4271","modified":1578321102465},{"_id":"public/2016/03/13/four-things/index.html","hash":"b79a8f97ad3cce84db4eed07f474aca7e9ff2ea1","modified":1578321102465},{"_id":"public/2016/02/26/exo/index.html","hash":"f53e03ca0e3293abdad0feb71709360d5d4054b1","modified":1578321102465},{"_id":"public/2016/01/04/prepare-design-thinking/index.html","hash":"5d58c0dd17eb7a9ebeb78c38314d5215dc4b29fe","modified":1578321102465},{"_id":"public/2015/12/17/about-data/index.html","hash":"d2938bdebcad5dad1b7671222540df806d34f3eb","modified":1578321102466},{"_id":"public/2015/12/17/hippa-part1/index.html","hash":"7e8f1deba751b9b6084533c66482af7a192b0da5","modified":1578321102466},{"_id":"public/2015/08/29/collection-n-set/index.html","hash":"adc604937728c5e16b130a22b54e8493a3262bf4","modified":1578321102466},{"_id":"public/2015/07/08/polar-review/index.html","hash":"e39d2cbed64d77e83fa268e3a3400143671a212a","modified":1578321102466},{"_id":"public/2015/07/03/wwdc/index.html","hash":"33dfe7428510cca072cee56e2fd186835ae6e527","modified":1578321102466},{"_id":"public/2015/05/19/tech-destruction-of-medicine/index.html","hash":"5bf709ac3229de0ff01ef253f780811b25714774","modified":1578321102466},{"_id":"public/2015/01/19/hello-world2/index.html","hash":"7fb5d07ada39f45d225e5b952181cd7dfa4a76b7","modified":1578321102466},{"_id":"public/2015/01/16/python-db-basic/index.html","hash":"033057f7fc2c68ee8df6005e1dafa8b673020de1","modified":1578321102466},{"_id":"public/2015/01/05/wechat-content/index.html","hash":"f61cf258b5c7395d46d456cdb1b4de690c5739d6","modified":1578321102466},{"_id":"public/2014/12/31/wechat/index.html","hash":"dbd884055b4bcaec1cc3642dee5bd4428eba7810","modified":1578321102466},{"_id":"public/2014/12/23/build-testing-team/index.html","hash":"1556ec73d3691161c4ff5d684b5dd50f75923986","modified":1578321102466},{"_id":"public/2014/12/22/python-misc/index.html","hash":"8952196d0ce05c5e13ec19e4c89c6d55afcdf5c7","modified":1578321102466},{"_id":"public/2014/12/08/positioning/index.html","hash":"27a3261b842ff531b6379f7572a44ec38110283f","modified":1578321102466},{"_id":"public/2014/12/01/hello-world/index.html","hash":"98c915d40128d45e3b19dccf85fffa5b560fc612","modified":1578321102466},{"_id":"public/archives/index.html","hash":"9d522b0e25da2a7acb911dbf15aab11a7d3be0e3","modified":1578321102466},{"_id":"public/archives/page/2/index.html","hash":"da325dbb27c6a0cd25894601b8adeb8eafd1da20","modified":1578321102466},{"_id":"public/archives/page/3/index.html","hash":"b2ad395b33507d5aa82496a30037a4f1b3c969d6","modified":1578321102467},{"_id":"public/archives/page/4/index.html","hash":"f79f40d0d19ff8600bd024a66f4dbc0a6ec8d3de","modified":1578321102467},{"_id":"public/archives/page/5/index.html","hash":"ae8473b3ca7777b83dc7d24dd3dd35805ad9440c","modified":1578321102467},{"_id":"public/archives/2014/index.html","hash":"e43982b71659ceb9449b30e40066ab6d0ac4505e","modified":1578321102467},{"_id":"public/archives/2014/12/index.html","hash":"249b1ddb73439735951dbad4c3eccb7c93fb9fe3","modified":1578321102467},{"_id":"public/archives/2015/index.html","hash":"a3b76d5b6d5e0824a31d280f8f88406c49640804","modified":1578321102467},{"_id":"public/archives/2015/01/index.html","hash":"417c1383b53105ebaf181cef8c7bad6a3d9ac16e","modified":1578321102467},{"_id":"public/archives/2015/05/index.html","hash":"d731092436bf3af498a0bd3d335a49337ebd10e7","modified":1578321102467},{"_id":"public/archives/2015/12/index.html","hash":"77661267d59c9bb0ace9f386ca489aa214bd9592","modified":1578321102467},{"_id":"public/archives/2015/07/index.html","hash":"93349fc50de3b1479dc3fafd9f6fbd5c69a37c16","modified":1578321102467},{"_id":"public/archives/2016/index.html","hash":"256127f587d74c69993e7a498e3f57774ddd2022","modified":1578321102467},{"_id":"public/archives/2016/page/2/index.html","hash":"18b0bb7a925e101937820ddfca4de6b36e8315e6","modified":1578321102467},{"_id":"public/archives/2016/01/index.html","hash":"d6b1b345b8a7afb77b411c66ec24656b5cfe8683","modified":1578321102468},{"_id":"public/archives/2016/03/index.html","hash":"fc7018d8f6209a64a40cced9ac6d2df56924e02f","modified":1578321102468},{"_id":"public/archives/2016/04/index.html","hash":"502634e8e0e9b1dee1100f069f809ce92319d938","modified":1578321102468},{"_id":"public/archives/2016/05/index.html","hash":"e12bec4a7e377fcfea9233bed9b0d3579caaac35","modified":1578321102468},{"_id":"public/archives/2016/06/index.html","hash":"1f39b62fe549c148459fcac0028929e940e503a1","modified":1578321102468},{"_id":"public/archives/2016/07/index.html","hash":"d1b0429d4861a0788097d96bc533c3f4e9501114","modified":1578321102468},{"_id":"public/archives/2016/02/index.html","hash":"5ce70224815903867c19b580ae6328dd90959d2f","modified":1578321102468},{"_id":"public/archives/2016/10/index.html","hash":"c9fa66d129a59cca2863a72c1a115e1e7f41e1af","modified":1578321102468},{"_id":"public/archives/2018/index.html","hash":"df88dea3c6ee78b4792c7cb8226e788fa37c96bc","modified":1578321102468},{"_id":"public/archives/2018/page/2/index.html","hash":"80257d55a9bdbe699931949f4733fd3a6f0012e1","modified":1578321102468},{"_id":"public/archives/2018/04/index.html","hash":"733a162fada4bc5b4455a40b47d63dcfaba2c916","modified":1578321102468},{"_id":"public/archives/2018/05/index.html","hash":"5988aa121173d79b37a56e9a595e78014ec6c504","modified":1578321102468},{"_id":"public/archives/2018/08/index.html","hash":"78df1bc18d2861ec9579c151d3a8d63e0378d3ec","modified":1578321102469},{"_id":"public/archives/2018/09/index.html","hash":"98b4f984e5a30ff421119ed62f2530e65842f7fd","modified":1578321102469},{"_id":"public/archives/2019/index.html","hash":"9b21e4578355197cb82aac6b3f64cefac55a64ff","modified":1578321102469},{"_id":"public/archives/2019/12/index.html","hash":"6a9379792a89c083b962efc065a0cbc04cc46105","modified":1578321102469},{"_id":"public/archives/2020/index.html","hash":"5ce34e0ecb6150d8906d3bab7d1f6d35da39da17","modified":1578321102469},{"_id":"public/archives/2020/01/index.html","hash":"ad744ebd56cb096792c3e64b55c90b8220faa40e","modified":1578321102469},{"_id":"public/index.html","hash":"da3160ecc8b80afbe87f5fa1d08a00dd4622e65d","modified":1578321102469},{"_id":"public/page/2/index.html","hash":"4d30ed460265e03fb0f0d5e0f96d0fa9529b08f6","modified":1578321102469},{"_id":"public/page/3/index.html","hash":"18f11d413f61ce9fb3b8239a95740e770f2e746c","modified":1578321102469},{"_id":"public/page/4/index.html","hash":"c975524510005a1a767ff62149dbb8b8270d6b1b","modified":1578321102469},{"_id":"public/page/5/index.html","hash":"ac95cd5f0f128b9c82a738dd3b90f8e5e37debef","modified":1578321102469},{"_id":"public/categories/Technology/index.html","hash":"49c7ff3162112e7e0a445563e9dd77d70fecbf40","modified":1578321102469},{"_id":"public/categories/Management/index.html","hash":"f17bef81dbcdec8128fb058ec38febfe77df286c","modified":1578321102469},{"_id":"public/categories/Technology/page/2/index.html","hash":"5c4f610f8be78f1812efb11b74475e22e2a40f05","modified":1578321102470},{"_id":"public/categories/Technology/page/3/index.html","hash":"5f72140bf498f2a22b0451c4b94492d834ee259c","modified":1578321102470},{"_id":"public/categories/Diary/index.html","hash":"6d6a05fe31ed2c7a429d8eec06e973cfee5df14e","modified":1578321102470},{"_id":"public/categories/Business/index.html","hash":"acb1b5750b0c8285ed1a34a1c213dc91f3c55ad6","modified":1578321102470},{"_id":"public/tags/mgnt/index.html","hash":"4f17b1c4b6c5013e2a03cbe06a60c98293550dbd","modified":1578321102470},{"_id":"public/tags/tech/index.html","hash":"259528e0cbaf454e7cc8005e9cd8fe5eea0d0c1c","modified":1578321102470},{"_id":"public/categories/Business-Strategy/index.html","hash":"30e955342a58de473c486602ae530f71c5e742c8","modified":1578321102470},{"_id":"public/tags/tech/page/2/index.html","hash":"30cfe3eb7a30106941823625fb08ad6b785b264d","modified":1578321102470},{"_id":"public/tags/tech/page/3/index.html","hash":"aefec262901294a376445427616127f775aecd80","modified":1578321102471},{"_id":"public/tags/tech/page/4/index.html","hash":"26cd4a48dc7134cbc921c319e987b1453d4afd29","modified":1578321102472},{"_id":"public/tags/cloud/index.html","hash":"cf3fac3f4278c13ab14826870651d7ec3cb6aa11","modified":1578321102472},{"_id":"public/tags/book/index.html","hash":"3000a9f89bae28abee66487bb4f5e0d2f534ddfb","modified":1578321102472},{"_id":"public/tags/deeplearning/index.html","hash":"1dfaaab330d022860272a4978fa91dbbe6645aa7","modified":1578321102472},{"_id":"public/tags/Security/index.html","hash":"3d77ca6d2c8ff92e9d7b5e600003a0f2962c6f00","modified":1578321102472},{"_id":"public/tags/misc/index.html","hash":"bdb7009f94116885947429862ecb9c9418c4237c","modified":1578321102473},{"_id":"public/tags/career/index.html","hash":"f748c84dcc2d90397437022d03e04eaa6a88e49d","modified":1578321102473},{"_id":"public/tags/python/index.html","hash":"13fb232a9b4c745f2619104ca247dec5a9f57fc8","modified":1578321102473},{"_id":"public/tags/security/index.html","hash":"545207623b145b0cd69db8366e1de0f33c65d8e5","modified":1578321102473},{"_id":"public/tags/med/index.html","hash":"48b266d2088e141cde5e15eaf3994d13e143cf77","modified":1578321102473}],"Category":[{"name":"Management","_id":"ck3offcqb001arlfy70uip1z8"},{"name":"Technology","_id":"ck3offcqm001crlfy903zrlxh"},{"name":"Diary","_id":"ck3offcr7002grlfyw5r5j0i9"},{"name":"Business Strategy","_id":"ck3offcrf002urlfyf3eo8fi1"},{"name":"Business","_id":"ck3offcrl0038rlfy8vs04tf6"},{"name":"Management","parent":"ck3offcrf002urlfyf3eo8fi1","_id":"ck52jqb9m0001mxz4113q6kt7"}],"Data":[],"Page":[],"Post":[{"title":"最近一次错误讨论方法的反思","date":"2015-12-17T06:09:48.000Z","_content":"本周三XTA讨论，发生了一件事情，让我不禁感慨有些东西说起来容易，真做起来却没那么简单，或者说比较容易被忽视。事情的大体经过是这样的：\n\nXTA每周三例行周会上，我们聊到了一个话题，是否将部分诊疗圈repo牵往github上。主要原因表述为，目前武鹏和文迪在诊疗圈推行一种更先进的部署方法，部署可以直接从repo里已用写在其他repo上的依赖modules（如果我理解没错的话）。目前青云环境到公司内网环境之间的数据并没有打通，所以导致在内网使用的gitlab，无法被青云访问到。所以得到了将部分repo牵往github上的一个方案，交由XTA共同讨论。\n\n在会上，大家为了是否迁移这件事情讨论了许多。包括是否采取架设青云到内网的访问管道，迁移github带来的权限管理成本，整体gitlab到github迁移的可行性等等。事后，虽然大家名义上达成了一致，认为可以在青云上搭设一套gitlab环境，交由ldap进行统一权限管理。但回过头来想想这个决策的讨论过程，总觉得不太对。\n\n*\t讨论的基础\n\t这里就不得不回到一直以来给团队设定的基线是：\n\t1.\t科学的做事儿\n\t2.\t使用数据和并努力作出合理优化\n\n\t之所以定义了这两条基线是源于读那本著名的《How Google Works》，我认为这本书对我的影响是相当深刻的。其中，关于文化一章有这样一个说法\n\t\n\t>“Establish a culture of Yes\n    >\n\t>We are both parents, so we understand through years of firsthand experience the dispiriting parental habit of the reflexive no. “Can I have a soda?” No. “Can I get two scoops of ice cream instead of one?” No. “Can I play video games even though my homework isn’t done?” No. “Can I put the cat in the dryer?” NO!\n\t>\n\t>The “Just Say No” syndrome can creep into the workplace too. Companies come up with elaborate, often passive-aggressive ways to say no: processes to follow, approvals to get, meetings to attend. No is like a tiny death to smart creatives. No is a signal that the company has lost its start-up verve, that it’s too corporate. Enough no’s, and smart creatives stop asking and start heading to the exits.”\n    >\n\t>Excerpt From: Eric Schmidt. “How Google Works.” iBooks.\n\t\n\t对于一个公司而言，树立Say Yes的文化是很重要的。那么如何树立这种文化，书里也给了比较明确的解答，那就是书里基线，一个大家都认同，并愿意为之遵守的基础。同样是参考这本书里，关于\n\t\n\t>“Decide with data\n    >\n\t>One of the most transformative developments of the Internet Century is the ability to quantify almost any aspect of business. Decisions once based on subjective opinion and anecdotal evidence now rely primarily on data. Companies like ours aggregate anonymous signals from mobile phones to provide accurate traffic data in real time. London’s water pipes are monitored by thousands of sensors, reducing leakage by 25 percent.119 Ranchers embed sensors into their cattle that transmit information about the animals’ health and location; each cow transmits about 200 megabytes of data per year,120 allowing ranchers to fine-tune what, when, and how much they feed their cattle. That’s a cattle list for change!”\n    >\n\t>Excerpt From: Eric Schmidt. “How Google Works.” iBooks.\n\t\n\t基于事情和数据的决策，我认为是非常好的一种工作方式。在书里写到Google在任何一个会议室都会有两个投影仪，一个用来交流，另一个用来展示数据。这充分显示了一个公司对数据的重视。问什么呢？曾经听《罗辑思维》讲，关于数据这部分，深感认同，数据是人类进行合作的重要工具，是跨时空合作得以展开的基础。有了数据其实大家都是聪明人，也就明白很多事情的决策是什么原因了。\n\t{% asset_img data.png \"引用自《你怎么还相信中医》\" %}\n\t\n\n所以，说回本次周三XTA的讨论。其实我们是没有做好讨论准备的。对于这样的一个迁移，一定是选取几套方案，并根据“运维时效”、“工作时效”共同加起来，完成一个比较好的讨论。让讨论的多方理解，为什么要做这个决定，为什么是那个决定。我觉得坚持以事为中心，坚持那数据来做决策是对的，应该在任何情况下必须坚持的。接下来应该更进一步讲数据细化，2016，我一定要让运维有一整套能拿得出来的数据，2016，要让杏树林的人都拿数据来讨论问题，我们的数据平台，也要更加为主的建立起来。把这个形成公司的行为习惯。在这点上，我一定要坚持，不动摇，并且学习更好的实践。\n\n哦，对了，也给自己和所有人说，2016，新年快乐！\n\n","source":"_posts/about-data.md","raw":"title: 最近一次错误讨论方法的反思\ndate: 2015-12-17 14:09:48\ncategories:\n- Management\ntags:\n- mgnt\n---\n本周三XTA讨论，发生了一件事情，让我不禁感慨有些东西说起来容易，真做起来却没那么简单，或者说比较容易被忽视。事情的大体经过是这样的：\n\nXTA每周三例行周会上，我们聊到了一个话题，是否将部分诊疗圈repo牵往github上。主要原因表述为，目前武鹏和文迪在诊疗圈推行一种更先进的部署方法，部署可以直接从repo里已用写在其他repo上的依赖modules（如果我理解没错的话）。目前青云环境到公司内网环境之间的数据并没有打通，所以导致在内网使用的gitlab，无法被青云访问到。所以得到了将部分repo牵往github上的一个方案，交由XTA共同讨论。\n\n在会上，大家为了是否迁移这件事情讨论了许多。包括是否采取架设青云到内网的访问管道，迁移github带来的权限管理成本，整体gitlab到github迁移的可行性等等。事后，虽然大家名义上达成了一致，认为可以在青云上搭设一套gitlab环境，交由ldap进行统一权限管理。但回过头来想想这个决策的讨论过程，总觉得不太对。\n\n*\t讨论的基础\n\t这里就不得不回到一直以来给团队设定的基线是：\n\t1.\t科学的做事儿\n\t2.\t使用数据和并努力作出合理优化\n\n\t之所以定义了这两条基线是源于读那本著名的《How Google Works》，我认为这本书对我的影响是相当深刻的。其中，关于文化一章有这样一个说法\n\t\n\t>“Establish a culture of Yes\n    >\n\t>We are both parents, so we understand through years of firsthand experience the dispiriting parental habit of the reflexive no. “Can I have a soda?” No. “Can I get two scoops of ice cream instead of one?” No. “Can I play video games even though my homework isn’t done?” No. “Can I put the cat in the dryer?” NO!\n\t>\n\t>The “Just Say No” syndrome can creep into the workplace too. Companies come up with elaborate, often passive-aggressive ways to say no: processes to follow, approvals to get, meetings to attend. No is like a tiny death to smart creatives. No is a signal that the company has lost its start-up verve, that it’s too corporate. Enough no’s, and smart creatives stop asking and start heading to the exits.”\n    >\n\t>Excerpt From: Eric Schmidt. “How Google Works.” iBooks.\n\t\n\t对于一个公司而言，树立Say Yes的文化是很重要的。那么如何树立这种文化，书里也给了比较明确的解答，那就是书里基线，一个大家都认同，并愿意为之遵守的基础。同样是参考这本书里，关于\n\t\n\t>“Decide with data\n    >\n\t>One of the most transformative developments of the Internet Century is the ability to quantify almost any aspect of business. Decisions once based on subjective opinion and anecdotal evidence now rely primarily on data. Companies like ours aggregate anonymous signals from mobile phones to provide accurate traffic data in real time. London’s water pipes are monitored by thousands of sensors, reducing leakage by 25 percent.119 Ranchers embed sensors into their cattle that transmit information about the animals’ health and location; each cow transmits about 200 megabytes of data per year,120 allowing ranchers to fine-tune what, when, and how much they feed their cattle. That’s a cattle list for change!”\n    >\n\t>Excerpt From: Eric Schmidt. “How Google Works.” iBooks.\n\t\n\t基于事情和数据的决策，我认为是非常好的一种工作方式。在书里写到Google在任何一个会议室都会有两个投影仪，一个用来交流，另一个用来展示数据。这充分显示了一个公司对数据的重视。问什么呢？曾经听《罗辑思维》讲，关于数据这部分，深感认同，数据是人类进行合作的重要工具，是跨时空合作得以展开的基础。有了数据其实大家都是聪明人，也就明白很多事情的决策是什么原因了。\n\t{% asset_img data.png \"引用自《你怎么还相信中医》\" %}\n\t\n\n所以，说回本次周三XTA的讨论。其实我们是没有做好讨论准备的。对于这样的一个迁移，一定是选取几套方案，并根据“运维时效”、“工作时效”共同加起来，完成一个比较好的讨论。让讨论的多方理解，为什么要做这个决定，为什么是那个决定。我觉得坚持以事为中心，坚持那数据来做决策是对的，应该在任何情况下必须坚持的。接下来应该更进一步讲数据细化，2016，我一定要让运维有一整套能拿得出来的数据，2016，要让杏树林的人都拿数据来讨论问题，我们的数据平台，也要更加为主的建立起来。把这个形成公司的行为习惯。在这点上，我一定要坚持，不动摇，并且学习更好的实践。\n\n哦，对了，也给自己和所有人说，2016，新年快乐！\n\n","slug":"about-data","published":1,"updated":"2018-04-16T09:59:37.071Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcnx0000rlfy7xr4m9ix","content":"<p>本周三XTA讨论，发生了一件事情，让我不禁感慨有些东西说起来容易，真做起来却没那么简单，或者说比较容易被忽视。事情的大体经过是这样的：</p>\n<p>XTA每周三例行周会上，我们聊到了一个话题，是否将部分诊疗圈repo牵往github上。主要原因表述为，目前武鹏和文迪在诊疗圈推行一种更先进的部署方法，部署可以直接从repo里已用写在其他repo上的依赖modules（如果我理解没错的话）。目前青云环境到公司内网环境之间的数据并没有打通，所以导致在内网使用的gitlab，无法被青云访问到。所以得到了将部分repo牵往github上的一个方案，交由XTA共同讨论。</p>\n<p>在会上，大家为了是否迁移这件事情讨论了许多。包括是否采取架设青云到内网的访问管道，迁移github带来的权限管理成本，整体gitlab到github迁移的可行性等等。事后，虽然大家名义上达成了一致，认为可以在青云上搭设一套gitlab环境，交由ldap进行统一权限管理。但回过头来想想这个决策的讨论过程，总觉得不太对。</p>\n<ul>\n<li><p>讨论的基础<br>这里就不得不回到一直以来给团队设定的基线是：</p>\n<ol>\n<li>科学的做事儿</li>\n<li>使用数据和并努力作出合理优化</li>\n</ol>\n<p>之所以定义了这两条基线是源于读那本著名的《How Google Works》，我认为这本书对我的影响是相当深刻的。其中，关于文化一章有这样一个说法</p>\n<blockquote>\n<p>“Establish a culture of Yes</p>\n<p>We are both parents, so we understand through years of firsthand experience the dispiriting parental habit of the reflexive no. “Can I have a soda?” No. “Can I get two scoops of ice cream instead of one?” No. “Can I play video games even though my homework isn’t done?” No. “Can I put the cat in the dryer?” NO!</p>\n<p>The “Just Say No” syndrome can creep into the workplace too. Companies come up with elaborate, often passive-aggressive ways to say no: processes to follow, approvals to get, meetings to attend. No is like a tiny death to smart creatives. No is a signal that the company has lost its start-up verve, that it’s too corporate. Enough no’s, and smart creatives stop asking and start heading to the exits.”</p>\n<p>Excerpt From: Eric Schmidt. “How Google Works.” iBooks.</p>\n</blockquote>\n<p>对于一个公司而言，树立Say Yes的文化是很重要的。那么如何树立这种文化，书里也给了比较明确的解答，那就是书里基线，一个大家都认同，并愿意为之遵守的基础。同样是参考这本书里，关于</p>\n<blockquote>\n<p>“Decide with data</p>\n<p>One of the most transformative developments of the Internet Century is the ability to quantify almost any aspect of business. Decisions once based on subjective opinion and anecdotal evidence now rely primarily on data. Companies like ours aggregate anonymous signals from mobile phones to provide accurate traffic data in real time. London’s water pipes are monitored by thousands of sensors, reducing leakage by 25 percent.119 Ranchers embed sensors into their cattle that transmit information about the animals’ health and location; each cow transmits about 200 megabytes of data per year,120 allowing ranchers to fine-tune what, when, and how much they feed their cattle. That’s a cattle list for change!”</p>\n<p>Excerpt From: Eric Schmidt. “How Google Works.” iBooks.</p>\n</blockquote>\n<p>基于事情和数据的决策，我认为是非常好的一种工作方式。在书里写到Google在任何一个会议室都会有两个投影仪，一个用来交流，另一个用来展示数据。这充分显示了一个公司对数据的重视。问什么呢？曾经听《罗辑思维》讲，关于数据这部分，深感认同，数据是人类进行合作的重要工具，是跨时空合作得以展开的基础。有了数据其实大家都是聪明人，也就明白很多事情的决策是什么原因了。</p>\n<img src=\"/2015/12/17/about-data/data.png\" title=\"引用自《你怎么还相信中医》\">\n</li>\n</ul>\n<p>所以，说回本次周三XTA的讨论。其实我们是没有做好讨论准备的。对于这样的一个迁移，一定是选取几套方案，并根据“运维时效”、“工作时效”共同加起来，完成一个比较好的讨论。让讨论的多方理解，为什么要做这个决定，为什么是那个决定。我觉得坚持以事为中心，坚持那数据来做决策是对的，应该在任何情况下必须坚持的。接下来应该更进一步讲数据细化，2016，我一定要让运维有一整套能拿得出来的数据，2016，要让杏树林的人都拿数据来讨论问题，我们的数据平台，也要更加为主的建立起来。把这个形成公司的行为习惯。在这点上，我一定要坚持，不动摇，并且学习更好的实践。</p>\n<p>哦，对了，也给自己和所有人说，2016，新年快乐！</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本周三XTA讨论，发生了一件事情，让我不禁感慨有些东西说起来容易，真做起来却没那么简单，或者说比较容易被忽视。事情的大体经过是这样的：</p>\n<p>XTA每周三例行周会上，我们聊到了一个话题，是否将部分诊疗圈repo牵往github上。主要原因表述为，目前武鹏和文迪在诊疗圈推行一种更先进的部署方法，部署可以直接从repo里已用写在其他repo上的依赖modules（如果我理解没错的话）。目前青云环境到公司内网环境之间的数据并没有打通，所以导致在内网使用的gitlab，无法被青云访问到。所以得到了将部分repo牵往github上的一个方案，交由XTA共同讨论。</p>\n<p>在会上，大家为了是否迁移这件事情讨论了许多。包括是否采取架设青云到内网的访问管道，迁移github带来的权限管理成本，整体gitlab到github迁移的可行性等等。事后，虽然大家名义上达成了一致，认为可以在青云上搭设一套gitlab环境，交由ldap进行统一权限管理。但回过头来想想这个决策的讨论过程，总觉得不太对。</p>\n<ul>\n<li><p>讨论的基础<br>这里就不得不回到一直以来给团队设定的基线是：</p>\n<ol>\n<li>科学的做事儿</li>\n<li>使用数据和并努力作出合理优化</li>\n</ol>\n<p>之所以定义了这两条基线是源于读那本著名的《How Google Works》，我认为这本书对我的影响是相当深刻的。其中，关于文化一章有这样一个说法</p>\n<blockquote>\n<p>“Establish a culture of Yes</p>\n<p>We are both parents, so we understand through years of firsthand experience the dispiriting parental habit of the reflexive no. “Can I have a soda?” No. “Can I get two scoops of ice cream instead of one?” No. “Can I play video games even though my homework isn’t done?” No. “Can I put the cat in the dryer?” NO!</p>\n<p>The “Just Say No” syndrome can creep into the workplace too. Companies come up with elaborate, often passive-aggressive ways to say no: processes to follow, approvals to get, meetings to attend. No is like a tiny death to smart creatives. No is a signal that the company has lost its start-up verve, that it’s too corporate. Enough no’s, and smart creatives stop asking and start heading to the exits.”</p>\n<p>Excerpt From: Eric Schmidt. “How Google Works.” iBooks.</p>\n</blockquote>\n<p>对于一个公司而言，树立Say Yes的文化是很重要的。那么如何树立这种文化，书里也给了比较明确的解答，那就是书里基线，一个大家都认同，并愿意为之遵守的基础。同样是参考这本书里，关于</p>\n<blockquote>\n<p>“Decide with data</p>\n<p>One of the most transformative developments of the Internet Century is the ability to quantify almost any aspect of business. Decisions once based on subjective opinion and anecdotal evidence now rely primarily on data. Companies like ours aggregate anonymous signals from mobile phones to provide accurate traffic data in real time. London’s water pipes are monitored by thousands of sensors, reducing leakage by 25 percent.119 Ranchers embed sensors into their cattle that transmit information about the animals’ health and location; each cow transmits about 200 megabytes of data per year,120 allowing ranchers to fine-tune what, when, and how much they feed their cattle. That’s a cattle list for change!”</p>\n<p>Excerpt From: Eric Schmidt. “How Google Works.” iBooks.</p>\n</blockquote>\n<p>基于事情和数据的决策，我认为是非常好的一种工作方式。在书里写到Google在任何一个会议室都会有两个投影仪，一个用来交流，另一个用来展示数据。这充分显示了一个公司对数据的重视。问什么呢？曾经听《罗辑思维》讲，关于数据这部分，深感认同，数据是人类进行合作的重要工具，是跨时空合作得以展开的基础。有了数据其实大家都是聪明人，也就明白很多事情的决策是什么原因了。</p>\n<img src=\"/2015/12/17/about-data/data.png\" title=\"引用自《你怎么还相信中医》\">\n</li>\n</ul>\n<p>所以，说回本次周三XTA的讨论。其实我们是没有做好讨论准备的。对于这样的一个迁移，一定是选取几套方案，并根据“运维时效”、“工作时效”共同加起来，完成一个比较好的讨论。让讨论的多方理解，为什么要做这个决定，为什么是那个决定。我觉得坚持以事为中心，坚持那数据来做决策是对的，应该在任何情况下必须坚持的。接下来应该更进一步讲数据细化，2016，我一定要让运维有一整套能拿得出来的数据，2016，要让杏树林的人都拿数据来讨论问题，我们的数据平台，也要更加为主的建立起来。把这个形成公司的行为习惯。在这点上，我一定要坚持，不动摇，并且学习更好的实践。</p>\n<p>哦，对了，也给自己和所有人说，2016，新年快乐！</p>\n"},{"title":"阿里云函数计算","date":"2019-01-02T06:43:20.000Z","_content":"\n年前终于做完了自己的阿里云函数计算的一个小项目，总结一下。\n\n{% asset_img aliyun.png [Aliyun Logo] %}\n","source":"_posts/aliyun-server-less.md","raw":"title: 阿里云函数计算\ndate: 2019-01-02 14:43:20\ncategories:\n- Technology\ntags:\n- tech\n- cloud\n---\n\n年前终于做完了自己的阿里云函数计算的一个小项目，总结一下。\n\n{% asset_img aliyun.png [Aliyun Logo] %}\n","slug":"aliyun-server-less","published":1,"updated":"2019-11-29T06:54:31.841Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offco20001rlfyuas4uis2","content":"<p>年前终于做完了自己的阿里云函数计算的一个小项目，总结一下。</p>\n<img src=\"/2019/01/02/aliyun-server-less/aliyun.png\" title=\"[Aliyun Logo]\">\n","site":{"data":{}},"excerpt":"","more":"<p>年前终于做完了自己的阿里云函数计算的一个小项目，总结一下。</p>\n<img src=\"/2019/01/02/aliyun-server-less/aliyun.png\" title=\"[Aliyun Logo]\">\n"},{"title":"build_testing_team","date":"2014-12-23T07:44:29.000Z","_content":"对于测试团队管理是个循序渐进的过程，传统的测试团队，尤其是从大公司来的，比较习惯于一体化的测试团队。所有测试人员和开发相对立，测试人员有一套独立的考核体系（经常是发现bug多，就越好）。但是这样的方式，实际上是在一个团队中树立了一层层的壁垒和对立关系，这与产品和运营的对立，技术和产品的对立，测试和技术的对立一样。所以作为管理者，作为新时代的团队管理模式，是无论如何要消除这种壁垒的。而这个过程中面对的第一个问题，就是测试团队管理者的思想问题。\n\n要解决这样的问题其实并非容易，我做的方法就是尝试分开，首先，如果你想把一个群人分开，就要去让他们地理上分开。于是把测试团队分开成两个小块儿，让一块儿的测试团队和项目组坐在一起。并不是完全，而是有一点点间隔的坐在一起，这样开始的时候会好接受学多。然后再通过给测试团队的压力，这种压力，往往可以做成多头压力，即让不同的产品在同一时间发布和讨论需求，往往这个时候管理者一个人很难做到兼顾，所以渐渐形成有人带劳的事实。第三个就是交流，站会scrum里面交流的一个最主要形式之一，将站会和某个特定测试人员联系在一起，可以大大增加该测试人员的归属感。\n\n无论如何，这些尝试都不是为了分解测试团队，而是让这个团队更加的高效，这本身慢慢也希望测试团度的管理者能够学习和理解","source":"_posts/build-testing-team.md","raw":"title: build_testing_team\ndate: 2014-12-23 15:44:29\ncategories:\n- Management\ntags:\n- mgnt\n---\n对于测试团队管理是个循序渐进的过程，传统的测试团队，尤其是从大公司来的，比较习惯于一体化的测试团队。所有测试人员和开发相对立，测试人员有一套独立的考核体系（经常是发现bug多，就越好）。但是这样的方式，实际上是在一个团队中树立了一层层的壁垒和对立关系，这与产品和运营的对立，技术和产品的对立，测试和技术的对立一样。所以作为管理者，作为新时代的团队管理模式，是无论如何要消除这种壁垒的。而这个过程中面对的第一个问题，就是测试团队管理者的思想问题。\n\n要解决这样的问题其实并非容易，我做的方法就是尝试分开，首先，如果你想把一个群人分开，就要去让他们地理上分开。于是把测试团队分开成两个小块儿，让一块儿的测试团队和项目组坐在一起。并不是完全，而是有一点点间隔的坐在一起，这样开始的时候会好接受学多。然后再通过给测试团队的压力，这种压力，往往可以做成多头压力，即让不同的产品在同一时间发布和讨论需求，往往这个时候管理者一个人很难做到兼顾，所以渐渐形成有人带劳的事实。第三个就是交流，站会scrum里面交流的一个最主要形式之一，将站会和某个特定测试人员联系在一起，可以大大增加该测试人员的归属感。\n\n无论如何，这些尝试都不是为了分解测试团队，而是让这个团队更加的高效，这本身慢慢也希望测试团度的管理者能够学习和理解","slug":"build-testing-team","published":1,"updated":"2018-04-16T09:59:37.040Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offco30002rlfyu2bqncx4","content":"<p>对于测试团队管理是个循序渐进的过程，传统的测试团队，尤其是从大公司来的，比较习惯于一体化的测试团队。所有测试人员和开发相对立，测试人员有一套独立的考核体系（经常是发现bug多，就越好）。但是这样的方式，实际上是在一个团队中树立了一层层的壁垒和对立关系，这与产品和运营的对立，技术和产品的对立，测试和技术的对立一样。所以作为管理者，作为新时代的团队管理模式，是无论如何要消除这种壁垒的。而这个过程中面对的第一个问题，就是测试团队管理者的思想问题。</p>\n<p>要解决这样的问题其实并非容易，我做的方法就是尝试分开，首先，如果你想把一个群人分开，就要去让他们地理上分开。于是把测试团队分开成两个小块儿，让一块儿的测试团队和项目组坐在一起。并不是完全，而是有一点点间隔的坐在一起，这样开始的时候会好接受学多。然后再通过给测试团队的压力，这种压力，往往可以做成多头压力，即让不同的产品在同一时间发布和讨论需求，往往这个时候管理者一个人很难做到兼顾，所以渐渐形成有人带劳的事实。第三个就是交流，站会scrum里面交流的一个最主要形式之一，将站会和某个特定测试人员联系在一起，可以大大增加该测试人员的归属感。</p>\n<p>无论如何，这些尝试都不是为了分解测试团队，而是让这个团队更加的高效，这本身慢慢也希望测试团度的管理者能够学习和理解</p>\n","site":{"data":{}},"excerpt":"","more":"<p>对于测试团队管理是个循序渐进的过程，传统的测试团队，尤其是从大公司来的，比较习惯于一体化的测试团队。所有测试人员和开发相对立，测试人员有一套独立的考核体系（经常是发现bug多，就越好）。但是这样的方式，实际上是在一个团队中树立了一层层的壁垒和对立关系，这与产品和运营的对立，技术和产品的对立，测试和技术的对立一样。所以作为管理者，作为新时代的团队管理模式，是无论如何要消除这种壁垒的。而这个过程中面对的第一个问题，就是测试团队管理者的思想问题。</p>\n<p>要解决这样的问题其实并非容易，我做的方法就是尝试分开，首先，如果你想把一个群人分开，就要去让他们地理上分开。于是把测试团队分开成两个小块儿，让一块儿的测试团队和项目组坐在一起。并不是完全，而是有一点点间隔的坐在一起，这样开始的时候会好接受学多。然后再通过给测试团队的压力，这种压力，往往可以做成多头压力，即让不同的产品在同一时间发布和讨论需求，往往这个时候管理者一个人很难做到兼顾，所以渐渐形成有人带劳的事实。第三个就是交流，站会scrum里面交流的一个最主要形式之一，将站会和某个特定测试人员联系在一起，可以大大增加该测试人员的归属感。</p>\n<p>无论如何，这些尝试都不是为了分解测试团队，而是让这个团队更加的高效，这本身慢慢也希望测试团度的管理者能够学习和理解</p>\n"},{"title":"CTO工作总结（15-16年度）--工作篇","date":"2016-07-24T04:29:42.000Z","_content":"\n通常我会在每个年中对从上一年的7月到这一年的6月的工作做一个总结。主要原因是年终的时候大家都在总结，但绩效、年终奖、年会、圣诞、新年等等，其实是一年中时间最紧张的时候。我不喜把事情都堆到一块儿做。刚好英联邦的财年就是年中，所以索性就选了这个时间做总结。\n\n本次总结分成两大部分，第一个是工作篇，主要陈述上一个总结中的预期工作在这一年里的开展情况和变化情况。本次总结多增加了一个学习篇，主要是觉得进入创业中期的工作状态是日新月异，学习到的东西越来越多，而不知到的东西也越来越多。希望通过总结暨给自己一个交代，也给未来一年一种鞭策。在一个创业公司，如果你的成长速度慢过于公司，那就坐等淘汰。\n\n*工作篇*\n\n一般来说，我对CTO的工作理解都会在我的xmind里面不断增加，也会根据实际的公司运行情况做优先级的处理。下面的总图基本反映了我各部分工作优先级预期和状态。其中数字代表在15年6月份的时候我对这几部分工作的优先级安排。而脸谱代表了对所有工作我现在自己看来的满意度。作为一个overall的评价来说，我觉得还是挺满意的。尤其是在商业战略、团队、研发和技术团队运营这几部分。\n\n对于用户，这一年碰的主要是一些企业用户，大体有所了解，更多的接触还是间接的，通过销售或者面试过程中谈话。医生用户几乎没有接触过。所以这块儿我对自己的工作是相当不满意的。任何事情不站在前台需求的角度，即使我现在有了数据的武器，也不会产生全面的印象和理解。在品牌塑造方面，写了些文章，但是感觉效果不佳，无论是传播力度还是广度并没有达到我预期的效果。\n\n{% asset_img overall.png \"总图\" %}\n\n下面会针对每一个做逐一分析。\n\n+\t先从优先级最高的商业战略开始。这里在新的一年我改一下名字，应该叫做技术战略，是指应对公司发展需要而做出的技术类总方向的决策。之前预期这一年的三个大事儿基本上都做了。\n\n\t{% asset_img strategy.png \"技术战略\" %} \t\n\t\n\t第一个是安全管理，这块儿在15年的下半年里，引入了安全宝的安全审计，做了渗透测试，确实发现了不少问题。服务端和运维一起最终将安全审计出来的问题，一一解决掉。让杏树林在安全防范上迈出了一个小步，但同时也是安全体系化的重要一步。接下来的安全工作出过一次事故，但是主要原因不是技术本身的安全隐患，而是出现在业务上的密码过于简单而产生的暴力破解。根据这个，对公司的VPN网络，请求时间间隔，各个系统验证码服务做了升级。接下来一年这一块儿肯定还是要做，但是可以从安全管理，上升到安全体系的高度，对一些安全工作进行常规化管理。\n\t\n\t第二块儿是团队的结构化。终于在杏树林走到50人技术团队的时候，我们迎来了业务化改造。这是我一直希望的事情。在我看来，一个团队，若想保持高效的战斗力，必须团队数量足够小，团队关注点足够专一。只有这样，才有可能把事情打透，把业务做到底，所以我在年度之初就策划了这个改革，好在的是，我们的COO也足够给力，快速的推动了我的改革。让我这个目标得以实现。\n\t\n\t第三块儿是核心技术能力的建立。这里包含了纯工程技术能力和基于业务的技术能力两大部分。纯工程是对原有公司的技术升级。包括了JS大方向的引入，Docker化和AB测试。但是很可惜，AB测试并没有完成，我觉得原因有两个，第一个是如果做AB测试，工具是一方面，但最重要的另外两个方面是基础数据，还有业务需求。这两块儿目前都在建立之中，随着数据驱动的演进，杏树林开始有计划的加功能，减功能。AB测试的需求正在起步，但是在过去的一年，因为数据驱动不到位，所以这部分没有太多需求。所以，这块儿掌握的一个核心是\n\t\n\t\t>**只有有了数据驱动，才有AB测试的需要**。\n\t\n\t另外，除了工程技术外，业务技术当时15年中订立的是OCR的核心技术能力，以及数据结构化。前一个OCR能力基本上已经形成规模，有人持续为之进行优化。后一个数据结构化目前并没有很好的方法，这也是新一年的重点工作。在去年数据结构化里，学到的重要教训是，数据其实本来不需要结构化，关键是有效的检索和回馈。因此搜索引擎，将会成为新一年工作重点。\n\t\n+\t优先级第二的是R&D，研发体系，无论何时都是技术研发的重点部分。基本上当时的计划都完成了。公司从一个对技术仅是零散开发，到如今有了比较成型的各方面技术研发体系。新的一年，研发体系会往基础架构方面进行比较多的拓延。通过公共服务体系持续提升研发能力。但是这块儿，其实很多公司干得并不好，至于我们能不能，拭目以待。\n\n\t{% asset_img RnD.png \"研发体系\" %} \t\n\n\n+\t同为第二优先级的是Client，这里其实是一个对用户和客户的共同说法。15-16年度，主要的工作在客户那里。15年下半年做了最重要的事情就是云学院产品的研发，经过内部产品和外部销售的反复沟通，完成了第一版云学院的基本体系和工作方式。从此云学院摆脱了无形态，minisite各类的方式，开始往产品化方式发展。但是在用户方面，确实没做什么，这部分将会作为接下来工作的方向\n\n\t{% asset_img client.png \"用户客户类工作\" %} \t\n\n\n+\t第三优先级的是品牌建设，这块儿这一年里花了不少时间。写了前后10片左右的文章，但是文章覆盖面主要是医疗这边，更加偏向于给行业和VC这类人看。但是并没有起到很好的作用。品牌建设的核心应该是面向技术人员。所以，接下来的思考是将之前类的文章数量降下来，像“大数据分析报告”这类的对企业具有一定价值报告。还是考虑在技术类演讲上增加曝光度，并且把团队加入进去。\n\n\t{% asset_img branding.png \"品牌建设类工作\" %} \t\n\n\n+\t第四优先级的是团队，这里团队之所以被列为较低的优先级，主要是因为一个相对完整的体系已经建立起来。大家可以看看作为参考吧。我觉得以目前团队的规模，短时间在超过100以前，这套体系应该不会有大的变化。还有一个最关键的，不想做大团队。坚信小而精的团队才能爆发最强的小宇宙。\n\n\t{% asset_img team.png \"团队体系\" %} \t\n\n+\t最后的部分是团队运营。团队运营主要指的是一些和基础财、物相关的工作。这一块儿属于维持，本来的工作就不是重点。主要是在流程上做了一些努力，其中包括了线上事故和Bug的处理流程，这部分对公司有一定的影响。\n\n\t{% asset_img operation.png \"团队运营体系\" %} \t\n\n学习篇\n\n埋个伏笔吧，上关键词，“做业务的技术原则：远景－目标－指标－做事；对应工作方法：监控－报警－处理－优化”\n\n\n","source":"_posts/cto-summary.md","raw":"title: CTO工作总结（15-16年度）--工作篇\ndate: 2016-07-24 12:29:42\ncategories:\n- Management\ntags:\n- tech\n- mgnt\n- career\n---\n\n通常我会在每个年中对从上一年的7月到这一年的6月的工作做一个总结。主要原因是年终的时候大家都在总结，但绩效、年终奖、年会、圣诞、新年等等，其实是一年中时间最紧张的时候。我不喜把事情都堆到一块儿做。刚好英联邦的财年就是年中，所以索性就选了这个时间做总结。\n\n本次总结分成两大部分，第一个是工作篇，主要陈述上一个总结中的预期工作在这一年里的开展情况和变化情况。本次总结多增加了一个学习篇，主要是觉得进入创业中期的工作状态是日新月异，学习到的东西越来越多，而不知到的东西也越来越多。希望通过总结暨给自己一个交代，也给未来一年一种鞭策。在一个创业公司，如果你的成长速度慢过于公司，那就坐等淘汰。\n\n*工作篇*\n\n一般来说，我对CTO的工作理解都会在我的xmind里面不断增加，也会根据实际的公司运行情况做优先级的处理。下面的总图基本反映了我各部分工作优先级预期和状态。其中数字代表在15年6月份的时候我对这几部分工作的优先级安排。而脸谱代表了对所有工作我现在自己看来的满意度。作为一个overall的评价来说，我觉得还是挺满意的。尤其是在商业战略、团队、研发和技术团队运营这几部分。\n\n对于用户，这一年碰的主要是一些企业用户，大体有所了解，更多的接触还是间接的，通过销售或者面试过程中谈话。医生用户几乎没有接触过。所以这块儿我对自己的工作是相当不满意的。任何事情不站在前台需求的角度，即使我现在有了数据的武器，也不会产生全面的印象和理解。在品牌塑造方面，写了些文章，但是感觉效果不佳，无论是传播力度还是广度并没有达到我预期的效果。\n\n{% asset_img overall.png \"总图\" %}\n\n下面会针对每一个做逐一分析。\n\n+\t先从优先级最高的商业战略开始。这里在新的一年我改一下名字，应该叫做技术战略，是指应对公司发展需要而做出的技术类总方向的决策。之前预期这一年的三个大事儿基本上都做了。\n\n\t{% asset_img strategy.png \"技术战略\" %} \t\n\t\n\t第一个是安全管理，这块儿在15年的下半年里，引入了安全宝的安全审计，做了渗透测试，确实发现了不少问题。服务端和运维一起最终将安全审计出来的问题，一一解决掉。让杏树林在安全防范上迈出了一个小步，但同时也是安全体系化的重要一步。接下来的安全工作出过一次事故，但是主要原因不是技术本身的安全隐患，而是出现在业务上的密码过于简单而产生的暴力破解。根据这个，对公司的VPN网络，请求时间间隔，各个系统验证码服务做了升级。接下来一年这一块儿肯定还是要做，但是可以从安全管理，上升到安全体系的高度，对一些安全工作进行常规化管理。\n\t\n\t第二块儿是团队的结构化。终于在杏树林走到50人技术团队的时候，我们迎来了业务化改造。这是我一直希望的事情。在我看来，一个团队，若想保持高效的战斗力，必须团队数量足够小，团队关注点足够专一。只有这样，才有可能把事情打透，把业务做到底，所以我在年度之初就策划了这个改革，好在的是，我们的COO也足够给力，快速的推动了我的改革。让我这个目标得以实现。\n\t\n\t第三块儿是核心技术能力的建立。这里包含了纯工程技术能力和基于业务的技术能力两大部分。纯工程是对原有公司的技术升级。包括了JS大方向的引入，Docker化和AB测试。但是很可惜，AB测试并没有完成，我觉得原因有两个，第一个是如果做AB测试，工具是一方面，但最重要的另外两个方面是基础数据，还有业务需求。这两块儿目前都在建立之中，随着数据驱动的演进，杏树林开始有计划的加功能，减功能。AB测试的需求正在起步，但是在过去的一年，因为数据驱动不到位，所以这部分没有太多需求。所以，这块儿掌握的一个核心是\n\t\n\t\t>**只有有了数据驱动，才有AB测试的需要**。\n\t\n\t另外，除了工程技术外，业务技术当时15年中订立的是OCR的核心技术能力，以及数据结构化。前一个OCR能力基本上已经形成规模，有人持续为之进行优化。后一个数据结构化目前并没有很好的方法，这也是新一年的重点工作。在去年数据结构化里，学到的重要教训是，数据其实本来不需要结构化，关键是有效的检索和回馈。因此搜索引擎，将会成为新一年工作重点。\n\t\n+\t优先级第二的是R&D，研发体系，无论何时都是技术研发的重点部分。基本上当时的计划都完成了。公司从一个对技术仅是零散开发，到如今有了比较成型的各方面技术研发体系。新的一年，研发体系会往基础架构方面进行比较多的拓延。通过公共服务体系持续提升研发能力。但是这块儿，其实很多公司干得并不好，至于我们能不能，拭目以待。\n\n\t{% asset_img RnD.png \"研发体系\" %} \t\n\n\n+\t同为第二优先级的是Client，这里其实是一个对用户和客户的共同说法。15-16年度，主要的工作在客户那里。15年下半年做了最重要的事情就是云学院产品的研发，经过内部产品和外部销售的反复沟通，完成了第一版云学院的基本体系和工作方式。从此云学院摆脱了无形态，minisite各类的方式，开始往产品化方式发展。但是在用户方面，确实没做什么，这部分将会作为接下来工作的方向\n\n\t{% asset_img client.png \"用户客户类工作\" %} \t\n\n\n+\t第三优先级的是品牌建设，这块儿这一年里花了不少时间。写了前后10片左右的文章，但是文章覆盖面主要是医疗这边，更加偏向于给行业和VC这类人看。但是并没有起到很好的作用。品牌建设的核心应该是面向技术人员。所以，接下来的思考是将之前类的文章数量降下来，像“大数据分析报告”这类的对企业具有一定价值报告。还是考虑在技术类演讲上增加曝光度，并且把团队加入进去。\n\n\t{% asset_img branding.png \"品牌建设类工作\" %} \t\n\n\n+\t第四优先级的是团队，这里团队之所以被列为较低的优先级，主要是因为一个相对完整的体系已经建立起来。大家可以看看作为参考吧。我觉得以目前团队的规模，短时间在超过100以前，这套体系应该不会有大的变化。还有一个最关键的，不想做大团队。坚信小而精的团队才能爆发最强的小宇宙。\n\n\t{% asset_img team.png \"团队体系\" %} \t\n\n+\t最后的部分是团队运营。团队运营主要指的是一些和基础财、物相关的工作。这一块儿属于维持，本来的工作就不是重点。主要是在流程上做了一些努力，其中包括了线上事故和Bug的处理流程，这部分对公司有一定的影响。\n\n\t{% asset_img operation.png \"团队运营体系\" %} \t\n\n学习篇\n\n埋个伏笔吧，上关键词，“做业务的技术原则：远景－目标－指标－做事；对应工作方法：监控－报警－处理－优化”\n\n\n","slug":"cto-summary","published":1,"updated":"2020-01-06T14:35:13.345Z","_id":"ck3offco40003rlfy401c9ncz","comments":1,"layout":"post","photos":[],"link":"","content":"<p>通常我会在每个年中对从上一年的7月到这一年的6月的工作做一个总结。主要原因是年终的时候大家都在总结，但绩效、年终奖、年会、圣诞、新年等等，其实是一年中时间最紧张的时候。我不喜把事情都堆到一块儿做。刚好英联邦的财年就是年中，所以索性就选了这个时间做总结。</p>\n<p>本次总结分成两大部分，第一个是工作篇，主要陈述上一个总结中的预期工作在这一年里的开展情况和变化情况。本次总结多增加了一个学习篇，主要是觉得进入创业中期的工作状态是日新月异，学习到的东西越来越多，而不知到的东西也越来越多。希望通过总结暨给自己一个交代，也给未来一年一种鞭策。在一个创业公司，如果你的成长速度慢过于公司，那就坐等淘汰。</p>\n<p><em>工作篇</em></p>\n<p>一般来说，我对CTO的工作理解都会在我的xmind里面不断增加，也会根据实际的公司运行情况做优先级的处理。下面的总图基本反映了我各部分工作优先级预期和状态。其中数字代表在15年6月份的时候我对这几部分工作的优先级安排。而脸谱代表了对所有工作我现在自己看来的满意度。作为一个overall的评价来说，我觉得还是挺满意的。尤其是在商业战略、团队、研发和技术团队运营这几部分。</p>\n<p>对于用户，这一年碰的主要是一些企业用户，大体有所了解，更多的接触还是间接的，通过销售或者面试过程中谈话。医生用户几乎没有接触过。所以这块儿我对自己的工作是相当不满意的。任何事情不站在前台需求的角度，即使我现在有了数据的武器，也不会产生全面的印象和理解。在品牌塑造方面，写了些文章，但是感觉效果不佳，无论是传播力度还是广度并没有达到我预期的效果。</p>\n<img src=\"/2016/07/24/cto-summary/overall.png\" title=\"总图\">\n<p>下面会针对每一个做逐一分析。</p>\n<ul>\n<li><p>先从优先级最高的商业战略开始。这里在新的一年我改一下名字，应该叫做技术战略，是指应对公司发展需要而做出的技术类总方向的决策。之前预期这一年的三个大事儿基本上都做了。</p>\n<img src=\"/2016/07/24/cto-summary/strategy.png\" title=\"技术战略\">     \n<p>第一个是安全管理，这块儿在15年的下半年里，引入了安全宝的安全审计，做了渗透测试，确实发现了不少问题。服务端和运维一起最终将安全审计出来的问题，一一解决掉。让杏树林在安全防范上迈出了一个小步，但同时也是安全体系化的重要一步。接下来的安全工作出过一次事故，但是主要原因不是技术本身的安全隐患，而是出现在业务上的密码过于简单而产生的暴力破解。根据这个，对公司的VPN网络，请求时间间隔，各个系统验证码服务做了升级。接下来一年这一块儿肯定还是要做，但是可以从安全管理，上升到安全体系的高度，对一些安全工作进行常规化管理。</p>\n<p>第二块儿是团队的结构化。终于在杏树林走到50人技术团队的时候，我们迎来了业务化改造。这是我一直希望的事情。在我看来，一个团队，若想保持高效的战斗力，必须团队数量足够小，团队关注点足够专一。只有这样，才有可能把事情打透，把业务做到底，所以我在年度之初就策划了这个改革，好在的是，我们的COO也足够给力，快速的推动了我的改革。让我这个目标得以实现。</p>\n<p>第三块儿是核心技术能力的建立。这里包含了纯工程技术能力和基于业务的技术能力两大部分。纯工程是对原有公司的技术升级。包括了JS大方向的引入，Docker化和AB测试。但是很可惜，AB测试并没有完成，我觉得原因有两个，第一个是如果做AB测试，工具是一方面，但最重要的另外两个方面是基础数据，还有业务需求。这两块儿目前都在建立之中，随着数据驱动的演进，杏树林开始有计划的加功能，减功能。AB测试的需求正在起步，但是在过去的一年，因为数据驱动不到位，所以这部分没有太多需求。所以，这块儿掌握的一个核心是</p>\n<blockquote>\n<p><strong>只有有了数据驱动，才有AB测试的需要</strong>。</p>\n</blockquote>\n<p>另外，除了工程技术外，业务技术当时15年中订立的是OCR的核心技术能力，以及数据结构化。前一个OCR能力基本上已经形成规模，有人持续为之进行优化。后一个数据结构化目前并没有很好的方法，这也是新一年的重点工作。在去年数据结构化里，学到的重要教训是，数据其实本来不需要结构化，关键是有效的检索和回馈。因此搜索引擎，将会成为新一年工作重点。</p>\n</li>\n<li><p>优先级第二的是R&amp;D，研发体系，无论何时都是技术研发的重点部分。基本上当时的计划都完成了。公司从一个对技术仅是零散开发，到如今有了比较成型的各方面技术研发体系。新的一年，研发体系会往基础架构方面进行比较多的拓延。通过公共服务体系持续提升研发能力。但是这块儿，其实很多公司干得并不好，至于我们能不能，拭目以待。</p>\n<img src=\"/2016/07/24/cto-summary/RnD.png\" title=\"研发体系\">     \n</li>\n</ul>\n<ul>\n<li><p>同为第二优先级的是Client，这里其实是一个对用户和客户的共同说法。15-16年度，主要的工作在客户那里。15年下半年做了最重要的事情就是云学院产品的研发，经过内部产品和外部销售的反复沟通，完成了第一版云学院的基本体系和工作方式。从此云学院摆脱了无形态，minisite各类的方式，开始往产品化方式发展。但是在用户方面，确实没做什么，这部分将会作为接下来工作的方向</p>\n<img src=\"/2016/07/24/cto-summary/client.png\" title=\"用户客户类工作\">     \n</li>\n</ul>\n<ul>\n<li><p>第三优先级的是品牌建设，这块儿这一年里花了不少时间。写了前后10片左右的文章，但是文章覆盖面主要是医疗这边，更加偏向于给行业和VC这类人看。但是并没有起到很好的作用。品牌建设的核心应该是面向技术人员。所以，接下来的思考是将之前类的文章数量降下来，像“大数据分析报告”这类的对企业具有一定价值报告。还是考虑在技术类演讲上增加曝光度，并且把团队加入进去。</p>\n<img src=\"/2016/07/24/cto-summary/branding.png\" title=\"品牌建设类工作\">     \n</li>\n</ul>\n<ul>\n<li><p>第四优先级的是团队，这里团队之所以被列为较低的优先级，主要是因为一个相对完整的体系已经建立起来。大家可以看看作为参考吧。我觉得以目前团队的规模，短时间在超过100以前，这套体系应该不会有大的变化。还有一个最关键的，不想做大团队。坚信小而精的团队才能爆发最强的小宇宙。</p>\n<img src=\"/2016/07/24/cto-summary/team.png\" title=\"团队体系\">     \n</li>\n<li><p>最后的部分是团队运营。团队运营主要指的是一些和基础财、物相关的工作。这一块儿属于维持，本来的工作就不是重点。主要是在流程上做了一些努力，其中包括了线上事故和Bug的处理流程，这部分对公司有一定的影响。</p>\n<img src=\"/2016/07/24/cto-summary/operation.png\" title=\"团队运营体系\">     \n</li>\n</ul>\n<p>学习篇</p>\n<p>埋个伏笔吧，上关键词，“做业务的技术原则：远景－目标－指标－做事；对应工作方法：监控－报警－处理－优化”</p>\n","site":{"data":{}},"excerpt":"","more":"<p>通常我会在每个年中对从上一年的7月到这一年的6月的工作做一个总结。主要原因是年终的时候大家都在总结，但绩效、年终奖、年会、圣诞、新年等等，其实是一年中时间最紧张的时候。我不喜把事情都堆到一块儿做。刚好英联邦的财年就是年中，所以索性就选了这个时间做总结。</p>\n<p>本次总结分成两大部分，第一个是工作篇，主要陈述上一个总结中的预期工作在这一年里的开展情况和变化情况。本次总结多增加了一个学习篇，主要是觉得进入创业中期的工作状态是日新月异，学习到的东西越来越多，而不知到的东西也越来越多。希望通过总结暨给自己一个交代，也给未来一年一种鞭策。在一个创业公司，如果你的成长速度慢过于公司，那就坐等淘汰。</p>\n<p><em>工作篇</em></p>\n<p>一般来说，我对CTO的工作理解都会在我的xmind里面不断增加，也会根据实际的公司运行情况做优先级的处理。下面的总图基本反映了我各部分工作优先级预期和状态。其中数字代表在15年6月份的时候我对这几部分工作的优先级安排。而脸谱代表了对所有工作我现在自己看来的满意度。作为一个overall的评价来说，我觉得还是挺满意的。尤其是在商业战略、团队、研发和技术团队运营这几部分。</p>\n<p>对于用户，这一年碰的主要是一些企业用户，大体有所了解，更多的接触还是间接的，通过销售或者面试过程中谈话。医生用户几乎没有接触过。所以这块儿我对自己的工作是相当不满意的。任何事情不站在前台需求的角度，即使我现在有了数据的武器，也不会产生全面的印象和理解。在品牌塑造方面，写了些文章，但是感觉效果不佳，无论是传播力度还是广度并没有达到我预期的效果。</p>\n<img src=\"/2016/07/24/cto-summary/overall.png\" title=\"总图\">\n<p>下面会针对每一个做逐一分析。</p>\n<ul>\n<li><p>先从优先级最高的商业战略开始。这里在新的一年我改一下名字，应该叫做技术战略，是指应对公司发展需要而做出的技术类总方向的决策。之前预期这一年的三个大事儿基本上都做了。</p>\n<img src=\"/2016/07/24/cto-summary/strategy.png\" title=\"技术战略\">     \n<p>第一个是安全管理，这块儿在15年的下半年里，引入了安全宝的安全审计，做了渗透测试，确实发现了不少问题。服务端和运维一起最终将安全审计出来的问题，一一解决掉。让杏树林在安全防范上迈出了一个小步，但同时也是安全体系化的重要一步。接下来的安全工作出过一次事故，但是主要原因不是技术本身的安全隐患，而是出现在业务上的密码过于简单而产生的暴力破解。根据这个，对公司的VPN网络，请求时间间隔，各个系统验证码服务做了升级。接下来一年这一块儿肯定还是要做，但是可以从安全管理，上升到安全体系的高度，对一些安全工作进行常规化管理。</p>\n<p>第二块儿是团队的结构化。终于在杏树林走到50人技术团队的时候，我们迎来了业务化改造。这是我一直希望的事情。在我看来，一个团队，若想保持高效的战斗力，必须团队数量足够小，团队关注点足够专一。只有这样，才有可能把事情打透，把业务做到底，所以我在年度之初就策划了这个改革，好在的是，我们的COO也足够给力，快速的推动了我的改革。让我这个目标得以实现。</p>\n<p>第三块儿是核心技术能力的建立。这里包含了纯工程技术能力和基于业务的技术能力两大部分。纯工程是对原有公司的技术升级。包括了JS大方向的引入，Docker化和AB测试。但是很可惜，AB测试并没有完成，我觉得原因有两个，第一个是如果做AB测试，工具是一方面，但最重要的另外两个方面是基础数据，还有业务需求。这两块儿目前都在建立之中，随着数据驱动的演进，杏树林开始有计划的加功能，减功能。AB测试的需求正在起步，但是在过去的一年，因为数据驱动不到位，所以这部分没有太多需求。所以，这块儿掌握的一个核心是</p>\n<blockquote>\n<p><strong>只有有了数据驱动，才有AB测试的需要</strong>。</p>\n</blockquote>\n<p>另外，除了工程技术外，业务技术当时15年中订立的是OCR的核心技术能力，以及数据结构化。前一个OCR能力基本上已经形成规模，有人持续为之进行优化。后一个数据结构化目前并没有很好的方法，这也是新一年的重点工作。在去年数据结构化里，学到的重要教训是，数据其实本来不需要结构化，关键是有效的检索和回馈。因此搜索引擎，将会成为新一年工作重点。</p>\n</li>\n<li><p>优先级第二的是R&amp;D，研发体系，无论何时都是技术研发的重点部分。基本上当时的计划都完成了。公司从一个对技术仅是零散开发，到如今有了比较成型的各方面技术研发体系。新的一年，研发体系会往基础架构方面进行比较多的拓延。通过公共服务体系持续提升研发能力。但是这块儿，其实很多公司干得并不好，至于我们能不能，拭目以待。</p>\n<img src=\"/2016/07/24/cto-summary/RnD.png\" title=\"研发体系\">     \n</li>\n</ul>\n<ul>\n<li><p>同为第二优先级的是Client，这里其实是一个对用户和客户的共同说法。15-16年度，主要的工作在客户那里。15年下半年做了最重要的事情就是云学院产品的研发，经过内部产品和外部销售的反复沟通，完成了第一版云学院的基本体系和工作方式。从此云学院摆脱了无形态，minisite各类的方式，开始往产品化方式发展。但是在用户方面，确实没做什么，这部分将会作为接下来工作的方向</p>\n<img src=\"/2016/07/24/cto-summary/client.png\" title=\"用户客户类工作\">     \n</li>\n</ul>\n<ul>\n<li><p>第三优先级的是品牌建设，这块儿这一年里花了不少时间。写了前后10片左右的文章，但是文章覆盖面主要是医疗这边，更加偏向于给行业和VC这类人看。但是并没有起到很好的作用。品牌建设的核心应该是面向技术人员。所以，接下来的思考是将之前类的文章数量降下来，像“大数据分析报告”这类的对企业具有一定价值报告。还是考虑在技术类演讲上增加曝光度，并且把团队加入进去。</p>\n<img src=\"/2016/07/24/cto-summary/branding.png\" title=\"品牌建设类工作\">     \n</li>\n</ul>\n<ul>\n<li><p>第四优先级的是团队，这里团队之所以被列为较低的优先级，主要是因为一个相对完整的体系已经建立起来。大家可以看看作为参考吧。我觉得以目前团队的规模，短时间在超过100以前，这套体系应该不会有大的变化。还有一个最关键的，不想做大团队。坚信小而精的团队才能爆发最强的小宇宙。</p>\n<img src=\"/2016/07/24/cto-summary/team.png\" title=\"团队体系\">     \n</li>\n<li><p>最后的部分是团队运营。团队运营主要指的是一些和基础财、物相关的工作。这一块儿属于维持，本来的工作就不是重点。主要是在流程上做了一些努力，其中包括了线上事故和Bug的处理流程，这部分对公司有一定的影响。</p>\n<img src=\"/2016/07/24/cto-summary/operation.png\" title=\"团队运营体系\">     \n</li>\n</ul>\n<p>学习篇</p>\n<p>埋个伏笔吧，上关键词，“做业务的技术原则：远景－目标－指标－做事；对应工作方法：监控－报警－处理－优化”</p>\n"},{"title":"CTO工作总结（15-16年度）--学习篇","date":"2016-10-07T02:22:41.000Z","_content":"\n上回留了个伏笔说：“做业务的技术原则：远景－目标－指标－做事；对应工作方法：监控－报警－处理－优化”\n\n如果说，之前我懂得的更多是如何根据不同的业务发展节奏，搭建与之相配套的技术体系的话。15-16年度，可以说是我说是重要的一年，让我对于什么叫做“工程技术体系”有了一层更佳深入的认识。这层抽象，让我更懂得回到技术的本源去看事情和问题。从这里一层层扩展开去。\n\n###工程技术的本质\n\n我最近特别喜欢在技术前面加上工程两个字。因为这些年的工作我其实经历了一个去工程化的过程，那就是太多的人开始说到互联网，说起研发开始弱化工程师这个概念，更多的是在突出“技术”这个说法。但正是对“工程”的弱化，让我们在很多时候忽视了从传统工程领域借鉴经验的机会。事实上，传统工程和如今的IT工程确实存在区别，但本质上，作为工程化理论，就永远摆脱不了一个话题：一个关于“质量”和“速度”之间永恒的矛盾。如果细想想，对于每一个Tech Lead而言，其实就是在不断的针对这两个问题进行抉择和平衡。没有什么需求是真的做不出来，只是怎么做，才能在质量和速度之间寻求一种平衡。我们知道，在相同产能条件的情况下，\n+\t第一、质量越高，要求的精细度越高，对应生产难度也就越大，速度也会因此而降下来。人不是机器会疲倦，会出错。\n+\t第二、而如果对生产速度要求较高，那么质量的下降往往不可避免。但是可以大大提高产出结果\n+\t第三、虽然是最后，但是很重要，那就是长期低质量对产品速度必然产生负面影线。\n\n由这三点，演化出来一系列方法和理论和话题。我在这里把“质量”与“速度”的矛盾叫做“工程技术本质”，以说明它的地位不可撼动性。\n\n### 针对业务的技术原则\n\n那么既然Tech Lead们需要不断解决的是“质量”和“速度”的矛盾问题。于是乎，这里面就衍生出一个很有意思的话题--如何做技术决策，如何判定自己的决策是合理。在我的这个理论里，若要做好这个决策，第一部分就是要了解业务。可能这对许多人来讲不可理解，作为技术人员，我做好自己的事情就好了，为什么要了解业务呢。这件事源于以人为中心的互联网模式。随着物质的极大丰富，供给模式，从最早的粗放型生产、分销、渠道，转变通过网络快速的寻找人的需求，根据需求进行快速迭代演进从而精细化的扩充人们在某一个或几个点上的消费习惯。在探寻过程中，其实对于互联网业务来讲，就是不断的实验过程。既然是实验，那么快速看到实验结果和实验深浅的把握就显得尤为重要。所谓实验结果，就是“速度”，即越快速度看到结果，越好进行决策是更进一步实验还是改换新的方向。实验深浅，对于技术而言就是实验的“质量”\n\n那么作为每天坐在IDE前面思考代码和逻辑思路的技术人员而言，我们无法接触真实的用户，没有时间思考和观察用户在干什么，如何才能有效的了解业务呢？我把技术人员了解业务的方法分成了四个基本过程：\n\n+\t远景，大多指一个业务长远的，或者一段时间的方向性目标。远景的很重要，它是指导一切的原则。当然，对于一个大的业务方向来说，形成商业收支目标是核心。但是拆解下来，不同时期，对于形成收支目标的要求不一样，有的是有也不一定是要在财务上展现。比如作为如今的滴滴，当下的最大远景就是实现盈利，但是对于在Nasdaq上市的京东而言，快速增长可能比盈利更重要。作为一个公司而言，远景往往和公司的核心管理人员以及投资人的想法相关。但是，对于一个业务的远景，往往会小许多。比如病历夹工具业务线，他当下远景就是用户活跃的持续增长。所以，绝大多数功能让用户有更强的黏着性。\n\n+\t目标，通常和当下所做的工作直接相关。为了实现远景，任何的业务都会拆成几个小步工作。那么目标，就是每个小步要完成的结果。这个结果可能是让用户在上传病历过程中使用不会产生疑问，也可能是让用户更快捷的在医口袋中搜到药品。作为每一个小步而言，多数业务会设立唯一的目标。当然如果优化功能都比较小，也不排除建立几个目标的可能。目标往往是团队的负责人做的定义，但是对于远景的优化角度不同，思考方式不同，小步目标也不一定是单一的。在互联网的实际环境中，目标也是可以被拿来讨论的。从这里开始，有兴趣的技术工程师，便可以参与深入的讨论。\n\n+\t指标，既然订立了目标，就一定要寻找可以进行衡量的目标完成好坏的指标。我们拿“让用户在上传病历过程中使用不会产生疑问”这个目标来说，这里可以建立的核心指标是“用户上传病历数增加”，这是核心业务指标。分解下来，可能还有一些其他指标，比如上传服务器反应时间在200ms以下，上传成功了70%以上等等。做任何一件事情，都应该是可衡量的，没有量化的工作，就是刷流氓好么:)。定义指标的工作，需要技术工程师的重度参与，这里包括订立那些指标，以及如何将指标反馈给BI系统或者其他数据报告系统。这一点非常重要。也是后期业务、技术团队跟踪做事好坏的核心。\n\n+\t做事，在了解远景，目标达成一致，以及订立好合适的指标后，大家就要开始基于指标工作了。那么指标从某种角度上讲，就是质量的衡量方法，这个一般说来，叫做质量底线标准。任何的速度，在底线标准前，都必须服从。而做事，就是考虑，如何在保证质量的基础上，尽量的提升速度。这里就是Tech Lead们的专业技能体现了。具体的方法有很多，比如XP，Scrum等的工程实践，比如新技术的引用提升研发效率，再比如团队的文化气氛等。\n\n总结一下，技术工程师，Tech Lead都很难真的完全深入业务细节去思考逻辑关系和用户使用习惯。但是经过以上的四个步骤，可以大体梳理每一个小步的背后的逻辑，并通过一系列指标进行有效的质量监控，用一系列工程方法指导做事，提升速度。\n\n### 针对工程的技术原则\n\n上面谈到了做事，谈到了比如工程方法、新技术的引用、团队文化等，那么针对技术工程，是不是可以衡量呢？答案当然是肯定的。任何工作都应该是可以衡量的，只是指标不同。这里我总结下来还是四个步骤：监控－报警－处理－优化\n\n**为什么要衡量？**\n\n在解释具体工程上的四步方法之前，先解释一个问题。可能又不少人会问，为什么你总是说衡量。看看我说的做事顺序，其中最后一步叫做“优化”。人类进行工程实践可能有几千年的历史了，很难说哪些事情是没有干过的。但是，问题的关键在于，对于每一次“质量”和“速度”博弈的实验中，如何进行优化，才是人类和动物最大的分别。我们在不断积累经验，寻找更好的方法。这就是实验的本质，也是任何事物前进的源头。“做事”的方法许多，根据团队、实践、节奏、个人知识能力等等不同，有太多的优秀实践可以应用，但是哪种是最好的，只有在指标定一下进行有效优化，才能得到越来越好的效果。所以下面说的四步，不仅仅适用于业务开发，也适用于技术工程人员对一切问题的抽象解决方案中。\n\n+\t监控，在针对业务的技术原则中，我们说到了指标。我们说技术人员要深度参与指标的定义，并做好指标的数据向BI或其他数据报告系统反馈工作，这就是监控的一种，主要应用在业务的指标监控中。其实，针对技术本身也有一系列的可以做的监控指标，比如线上Bug的数量、Crash率、服务器Apex指标，应用可用性等等。但是这里需要需要特别注意的是，在我看来，监控不是目的，是为了解决问题而存在的，特别是解决上面说的业务目标。比如有段时间业务目标是满足双11的客流访问量，那么Apex、网络带宽、负载均衡的资源使用这些就是重要的监控，可能需要秒级别的信息收集。那么这个时段，其他的比如CPU数量、客户端Crash率、网页兼容性，在这个目标面前，就不需要被严密监控，可能是天级别监控就可以了。所以监控是分级别，也是分时段的。\n\t人曾经问我，我是一个Android起家的Tech Lead，对于iOS、Java服务端，我不是很了解，要怎么才能做到很好对团队工作进行有效的带领和优化。这里我想说，通过指标监控，就可以达到目的。与相关的Tech Lead讨论针对特殊目的而细化的监控指标。\n\t\n+\t报警，许多人谈论监控，更多的是一套漂亮的报表，线图、饼图、柱图、箱图各种上。这是我以前常犯的错误，以为报表意味着监控。其实，在过去一年，我学会的最重要的知识就是，展现的目的更多是为了给别人看的直观，用以表明优化的成果，是一种回朔和预测型的工具。但是做事的时候，却不应该作为工具，回到计算机或者生物的本质上，0和1才是我们认识事件最直观的方法。因此，报警在我看来是工程监控最有效的手段（工具）。这种被动式相应能够大大加强响应效率和工作效率，根据响应的情况再去寻找相应的报表或者内容数据进行分析，从而得出有效的处理方案。\n\t设立报警的过程本身并不难，最难的是适当报警阀值的斟酌。太高起不到监控的效果，太低虚报误报使得报警无法有效应用。这个时候需要对技术有比较深入理解，对不同阶段对应指标有比较深入理解的技术人员（这里就不是一个纯工程问题，而上升到技术本身能力上），对报警阀值进行准确的把控，并持续优化。\n\t\n+\t处理，这块儿没太多可说的，这是每一个工程师的老本行，根据报警问题，使用数据综合分析，数据不全可能需要进行场景复现，如果是紧急问题可能还需要采取紧急方案、备份方案、临时方案等进行解决。总之是确保报警状态恢复到正常的水平。这个时间会根据不同业务和级别的不同，处理时间也会不同。\n\n+\t优化，最后来到了前面说的，工程技术原则的核心，就是优化。前面说到互联网的最大特点就是实验，不断的、快速的实验，了解和提升用户的体验，帮助用户解决问题。所以，无论是对业务指标的优化，还是技术指标，或者别的什么指标的优化，总之优化是技术工程师做事的终点，也是新的一轮工作的起点。研发工程由此不断迭代向前，帮助解决一个又一个业务目标。\n\n\n最后做个总结，在过去的一年里，对于上面方法的理解和应用真正改变了我将近十年的认识。过去的我，一直在学习，学习各种技术、学习各种最佳时间、学习各种方法论来解决一个又一个的研发问题。记得刚进微软那会儿，我的最大收获是如何通过互联网快速解决问题，把Google用的炉火纯青。在英国的4年里，我明白了从0到1的建设过程和研究方法。ThoughtWorks的3年让我懂得什么叫做研发体系和最佳实践。如今，在杏树林，很高兴能遇到一群志同道合的人，至少现在吧，感觉自己的理解又上升了一个新的高度，实践因什么而存在，要解决哪些问题，如何才算真正解决，解决问题的为了什么。这是我2015-16年度，最有收获的部分，共勉。","source":"_posts/cto-study-summary-1516.md","raw":"title: CTO工作总结（15-16年度）--学习篇\ndate: 2016-10-07 10:22:41\ncategories:\n- Management\ntags:\n- tech\n- mgnt\n---\n\n上回留了个伏笔说：“做业务的技术原则：远景－目标－指标－做事；对应工作方法：监控－报警－处理－优化”\n\n如果说，之前我懂得的更多是如何根据不同的业务发展节奏，搭建与之相配套的技术体系的话。15-16年度，可以说是我说是重要的一年，让我对于什么叫做“工程技术体系”有了一层更佳深入的认识。这层抽象，让我更懂得回到技术的本源去看事情和问题。从这里一层层扩展开去。\n\n###工程技术的本质\n\n我最近特别喜欢在技术前面加上工程两个字。因为这些年的工作我其实经历了一个去工程化的过程，那就是太多的人开始说到互联网，说起研发开始弱化工程师这个概念，更多的是在突出“技术”这个说法。但正是对“工程”的弱化，让我们在很多时候忽视了从传统工程领域借鉴经验的机会。事实上，传统工程和如今的IT工程确实存在区别，但本质上，作为工程化理论，就永远摆脱不了一个话题：一个关于“质量”和“速度”之间永恒的矛盾。如果细想想，对于每一个Tech Lead而言，其实就是在不断的针对这两个问题进行抉择和平衡。没有什么需求是真的做不出来，只是怎么做，才能在质量和速度之间寻求一种平衡。我们知道，在相同产能条件的情况下，\n+\t第一、质量越高，要求的精细度越高，对应生产难度也就越大，速度也会因此而降下来。人不是机器会疲倦，会出错。\n+\t第二、而如果对生产速度要求较高，那么质量的下降往往不可避免。但是可以大大提高产出结果\n+\t第三、虽然是最后，但是很重要，那就是长期低质量对产品速度必然产生负面影线。\n\n由这三点，演化出来一系列方法和理论和话题。我在这里把“质量”与“速度”的矛盾叫做“工程技术本质”，以说明它的地位不可撼动性。\n\n### 针对业务的技术原则\n\n那么既然Tech Lead们需要不断解决的是“质量”和“速度”的矛盾问题。于是乎，这里面就衍生出一个很有意思的话题--如何做技术决策，如何判定自己的决策是合理。在我的这个理论里，若要做好这个决策，第一部分就是要了解业务。可能这对许多人来讲不可理解，作为技术人员，我做好自己的事情就好了，为什么要了解业务呢。这件事源于以人为中心的互联网模式。随着物质的极大丰富，供给模式，从最早的粗放型生产、分销、渠道，转变通过网络快速的寻找人的需求，根据需求进行快速迭代演进从而精细化的扩充人们在某一个或几个点上的消费习惯。在探寻过程中，其实对于互联网业务来讲，就是不断的实验过程。既然是实验，那么快速看到实验结果和实验深浅的把握就显得尤为重要。所谓实验结果，就是“速度”，即越快速度看到结果，越好进行决策是更进一步实验还是改换新的方向。实验深浅，对于技术而言就是实验的“质量”\n\n那么作为每天坐在IDE前面思考代码和逻辑思路的技术人员而言，我们无法接触真实的用户，没有时间思考和观察用户在干什么，如何才能有效的了解业务呢？我把技术人员了解业务的方法分成了四个基本过程：\n\n+\t远景，大多指一个业务长远的，或者一段时间的方向性目标。远景的很重要，它是指导一切的原则。当然，对于一个大的业务方向来说，形成商业收支目标是核心。但是拆解下来，不同时期，对于形成收支目标的要求不一样，有的是有也不一定是要在财务上展现。比如作为如今的滴滴，当下的最大远景就是实现盈利，但是对于在Nasdaq上市的京东而言，快速增长可能比盈利更重要。作为一个公司而言，远景往往和公司的核心管理人员以及投资人的想法相关。但是，对于一个业务的远景，往往会小许多。比如病历夹工具业务线，他当下远景就是用户活跃的持续增长。所以，绝大多数功能让用户有更强的黏着性。\n\n+\t目标，通常和当下所做的工作直接相关。为了实现远景，任何的业务都会拆成几个小步工作。那么目标，就是每个小步要完成的结果。这个结果可能是让用户在上传病历过程中使用不会产生疑问，也可能是让用户更快捷的在医口袋中搜到药品。作为每一个小步而言，多数业务会设立唯一的目标。当然如果优化功能都比较小，也不排除建立几个目标的可能。目标往往是团队的负责人做的定义，但是对于远景的优化角度不同，思考方式不同，小步目标也不一定是单一的。在互联网的实际环境中，目标也是可以被拿来讨论的。从这里开始，有兴趣的技术工程师，便可以参与深入的讨论。\n\n+\t指标，既然订立了目标，就一定要寻找可以进行衡量的目标完成好坏的指标。我们拿“让用户在上传病历过程中使用不会产生疑问”这个目标来说，这里可以建立的核心指标是“用户上传病历数增加”，这是核心业务指标。分解下来，可能还有一些其他指标，比如上传服务器反应时间在200ms以下，上传成功了70%以上等等。做任何一件事情，都应该是可衡量的，没有量化的工作，就是刷流氓好么:)。定义指标的工作，需要技术工程师的重度参与，这里包括订立那些指标，以及如何将指标反馈给BI系统或者其他数据报告系统。这一点非常重要。也是后期业务、技术团队跟踪做事好坏的核心。\n\n+\t做事，在了解远景，目标达成一致，以及订立好合适的指标后，大家就要开始基于指标工作了。那么指标从某种角度上讲，就是质量的衡量方法，这个一般说来，叫做质量底线标准。任何的速度，在底线标准前，都必须服从。而做事，就是考虑，如何在保证质量的基础上，尽量的提升速度。这里就是Tech Lead们的专业技能体现了。具体的方法有很多，比如XP，Scrum等的工程实践，比如新技术的引用提升研发效率，再比如团队的文化气氛等。\n\n总结一下，技术工程师，Tech Lead都很难真的完全深入业务细节去思考逻辑关系和用户使用习惯。但是经过以上的四个步骤，可以大体梳理每一个小步的背后的逻辑，并通过一系列指标进行有效的质量监控，用一系列工程方法指导做事，提升速度。\n\n### 针对工程的技术原则\n\n上面谈到了做事，谈到了比如工程方法、新技术的引用、团队文化等，那么针对技术工程，是不是可以衡量呢？答案当然是肯定的。任何工作都应该是可以衡量的，只是指标不同。这里我总结下来还是四个步骤：监控－报警－处理－优化\n\n**为什么要衡量？**\n\n在解释具体工程上的四步方法之前，先解释一个问题。可能又不少人会问，为什么你总是说衡量。看看我说的做事顺序，其中最后一步叫做“优化”。人类进行工程实践可能有几千年的历史了，很难说哪些事情是没有干过的。但是，问题的关键在于，对于每一次“质量”和“速度”博弈的实验中，如何进行优化，才是人类和动物最大的分别。我们在不断积累经验，寻找更好的方法。这就是实验的本质，也是任何事物前进的源头。“做事”的方法许多，根据团队、实践、节奏、个人知识能力等等不同，有太多的优秀实践可以应用，但是哪种是最好的，只有在指标定一下进行有效优化，才能得到越来越好的效果。所以下面说的四步，不仅仅适用于业务开发，也适用于技术工程人员对一切问题的抽象解决方案中。\n\n+\t监控，在针对业务的技术原则中，我们说到了指标。我们说技术人员要深度参与指标的定义，并做好指标的数据向BI或其他数据报告系统反馈工作，这就是监控的一种，主要应用在业务的指标监控中。其实，针对技术本身也有一系列的可以做的监控指标，比如线上Bug的数量、Crash率、服务器Apex指标，应用可用性等等。但是这里需要需要特别注意的是，在我看来，监控不是目的，是为了解决问题而存在的，特别是解决上面说的业务目标。比如有段时间业务目标是满足双11的客流访问量，那么Apex、网络带宽、负载均衡的资源使用这些就是重要的监控，可能需要秒级别的信息收集。那么这个时段，其他的比如CPU数量、客户端Crash率、网页兼容性，在这个目标面前，就不需要被严密监控，可能是天级别监控就可以了。所以监控是分级别，也是分时段的。\n\t人曾经问我，我是一个Android起家的Tech Lead，对于iOS、Java服务端，我不是很了解，要怎么才能做到很好对团队工作进行有效的带领和优化。这里我想说，通过指标监控，就可以达到目的。与相关的Tech Lead讨论针对特殊目的而细化的监控指标。\n\t\n+\t报警，许多人谈论监控，更多的是一套漂亮的报表，线图、饼图、柱图、箱图各种上。这是我以前常犯的错误，以为报表意味着监控。其实，在过去一年，我学会的最重要的知识就是，展现的目的更多是为了给别人看的直观，用以表明优化的成果，是一种回朔和预测型的工具。但是做事的时候，却不应该作为工具，回到计算机或者生物的本质上，0和1才是我们认识事件最直观的方法。因此，报警在我看来是工程监控最有效的手段（工具）。这种被动式相应能够大大加强响应效率和工作效率，根据响应的情况再去寻找相应的报表或者内容数据进行分析，从而得出有效的处理方案。\n\t设立报警的过程本身并不难，最难的是适当报警阀值的斟酌。太高起不到监控的效果，太低虚报误报使得报警无法有效应用。这个时候需要对技术有比较深入理解，对不同阶段对应指标有比较深入理解的技术人员（这里就不是一个纯工程问题，而上升到技术本身能力上），对报警阀值进行准确的把控，并持续优化。\n\t\n+\t处理，这块儿没太多可说的，这是每一个工程师的老本行，根据报警问题，使用数据综合分析，数据不全可能需要进行场景复现，如果是紧急问题可能还需要采取紧急方案、备份方案、临时方案等进行解决。总之是确保报警状态恢复到正常的水平。这个时间会根据不同业务和级别的不同，处理时间也会不同。\n\n+\t优化，最后来到了前面说的，工程技术原则的核心，就是优化。前面说到互联网的最大特点就是实验，不断的、快速的实验，了解和提升用户的体验，帮助用户解决问题。所以，无论是对业务指标的优化，还是技术指标，或者别的什么指标的优化，总之优化是技术工程师做事的终点，也是新的一轮工作的起点。研发工程由此不断迭代向前，帮助解决一个又一个业务目标。\n\n\n最后做个总结，在过去的一年里，对于上面方法的理解和应用真正改变了我将近十年的认识。过去的我，一直在学习，学习各种技术、学习各种最佳时间、学习各种方法论来解决一个又一个的研发问题。记得刚进微软那会儿，我的最大收获是如何通过互联网快速解决问题，把Google用的炉火纯青。在英国的4年里，我明白了从0到1的建设过程和研究方法。ThoughtWorks的3年让我懂得什么叫做研发体系和最佳实践。如今，在杏树林，很高兴能遇到一群志同道合的人，至少现在吧，感觉自己的理解又上升了一个新的高度，实践因什么而存在，要解决哪些问题，如何才算真正解决，解决问题的为了什么。这是我2015-16年度，最有收获的部分，共勉。","slug":"cto-study-summary-1516","published":1,"updated":"2018-04-16T09:59:37.032Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offco50004rlfyrn3blxpw","content":"<p>上回留了个伏笔说：“做业务的技术原则：远景－目标－指标－做事；对应工作方法：监控－报警－处理－优化”</p>\n<p>如果说，之前我懂得的更多是如何根据不同的业务发展节奏，搭建与之相配套的技术体系的话。15-16年度，可以说是我说是重要的一年，让我对于什么叫做“工程技术体系”有了一层更佳深入的认识。这层抽象，让我更懂得回到技术的本源去看事情和问题。从这里一层层扩展开去。</p>\n<h3 id=\"工程技术的本质\"><a href=\"#工程技术的本质\" class=\"headerlink\" title=\"工程技术的本质\"></a>工程技术的本质</h3><p>我最近特别喜欢在技术前面加上工程两个字。因为这些年的工作我其实经历了一个去工程化的过程，那就是太多的人开始说到互联网，说起研发开始弱化工程师这个概念，更多的是在突出“技术”这个说法。但正是对“工程”的弱化，让我们在很多时候忽视了从传统工程领域借鉴经验的机会。事实上，传统工程和如今的IT工程确实存在区别，但本质上，作为工程化理论，就永远摆脱不了一个话题：一个关于“质量”和“速度”之间永恒的矛盾。如果细想想，对于每一个Tech Lead而言，其实就是在不断的针对这两个问题进行抉择和平衡。没有什么需求是真的做不出来，只是怎么做，才能在质量和速度之间寻求一种平衡。我们知道，在相同产能条件的情况下，</p>\n<ul>\n<li>第一、质量越高，要求的精细度越高，对应生产难度也就越大，速度也会因此而降下来。人不是机器会疲倦，会出错。</li>\n<li>第二、而如果对生产速度要求较高，那么质量的下降往往不可避免。但是可以大大提高产出结果</li>\n<li>第三、虽然是最后，但是很重要，那就是长期低质量对产品速度必然产生负面影线。</li>\n</ul>\n<p>由这三点，演化出来一系列方法和理论和话题。我在这里把“质量”与“速度”的矛盾叫做“工程技术本质”，以说明它的地位不可撼动性。</p>\n<h3 id=\"针对业务的技术原则\"><a href=\"#针对业务的技术原则\" class=\"headerlink\" title=\"针对业务的技术原则\"></a>针对业务的技术原则</h3><p>那么既然Tech Lead们需要不断解决的是“质量”和“速度”的矛盾问题。于是乎，这里面就衍生出一个很有意思的话题—如何做技术决策，如何判定自己的决策是合理。在我的这个理论里，若要做好这个决策，第一部分就是要了解业务。可能这对许多人来讲不可理解，作为技术人员，我做好自己的事情就好了，为什么要了解业务呢。这件事源于以人为中心的互联网模式。随着物质的极大丰富，供给模式，从最早的粗放型生产、分销、渠道，转变通过网络快速的寻找人的需求，根据需求进行快速迭代演进从而精细化的扩充人们在某一个或几个点上的消费习惯。在探寻过程中，其实对于互联网业务来讲，就是不断的实验过程。既然是实验，那么快速看到实验结果和实验深浅的把握就显得尤为重要。所谓实验结果，就是“速度”，即越快速度看到结果，越好进行决策是更进一步实验还是改换新的方向。实验深浅，对于技术而言就是实验的“质量”</p>\n<p>那么作为每天坐在IDE前面思考代码和逻辑思路的技术人员而言，我们无法接触真实的用户，没有时间思考和观察用户在干什么，如何才能有效的了解业务呢？我把技术人员了解业务的方法分成了四个基本过程：</p>\n<ul>\n<li><p>远景，大多指一个业务长远的，或者一段时间的方向性目标。远景的很重要，它是指导一切的原则。当然，对于一个大的业务方向来说，形成商业收支目标是核心。但是拆解下来，不同时期，对于形成收支目标的要求不一样，有的是有也不一定是要在财务上展现。比如作为如今的滴滴，当下的最大远景就是实现盈利，但是对于在Nasdaq上市的京东而言，快速增长可能比盈利更重要。作为一个公司而言，远景往往和公司的核心管理人员以及投资人的想法相关。但是，对于一个业务的远景，往往会小许多。比如病历夹工具业务线，他当下远景就是用户活跃的持续增长。所以，绝大多数功能让用户有更强的黏着性。</p>\n</li>\n<li><p>目标，通常和当下所做的工作直接相关。为了实现远景，任何的业务都会拆成几个小步工作。那么目标，就是每个小步要完成的结果。这个结果可能是让用户在上传病历过程中使用不会产生疑问，也可能是让用户更快捷的在医口袋中搜到药品。作为每一个小步而言，多数业务会设立唯一的目标。当然如果优化功能都比较小，也不排除建立几个目标的可能。目标往往是团队的负责人做的定义，但是对于远景的优化角度不同，思考方式不同，小步目标也不一定是单一的。在互联网的实际环境中，目标也是可以被拿来讨论的。从这里开始，有兴趣的技术工程师，便可以参与深入的讨论。</p>\n</li>\n<li><p>指标，既然订立了目标，就一定要寻找可以进行衡量的目标完成好坏的指标。我们拿“让用户在上传病历过程中使用不会产生疑问”这个目标来说，这里可以建立的核心指标是“用户上传病历数增加”，这是核心业务指标。分解下来，可能还有一些其他指标，比如上传服务器反应时间在200ms以下，上传成功了70%以上等等。做任何一件事情，都应该是可衡量的，没有量化的工作，就是刷流氓好么:)。定义指标的工作，需要技术工程师的重度参与，这里包括订立那些指标，以及如何将指标反馈给BI系统或者其他数据报告系统。这一点非常重要。也是后期业务、技术团队跟踪做事好坏的核心。</p>\n</li>\n<li><p>做事，在了解远景，目标达成一致，以及订立好合适的指标后，大家就要开始基于指标工作了。那么指标从某种角度上讲，就是质量的衡量方法，这个一般说来，叫做质量底线标准。任何的速度，在底线标准前，都必须服从。而做事，就是考虑，如何在保证质量的基础上，尽量的提升速度。这里就是Tech Lead们的专业技能体现了。具体的方法有很多，比如XP，Scrum等的工程实践，比如新技术的引用提升研发效率，再比如团队的文化气氛等。</p>\n</li>\n</ul>\n<p>总结一下，技术工程师，Tech Lead都很难真的完全深入业务细节去思考逻辑关系和用户使用习惯。但是经过以上的四个步骤，可以大体梳理每一个小步的背后的逻辑，并通过一系列指标进行有效的质量监控，用一系列工程方法指导做事，提升速度。</p>\n<h3 id=\"针对工程的技术原则\"><a href=\"#针对工程的技术原则\" class=\"headerlink\" title=\"针对工程的技术原则\"></a>针对工程的技术原则</h3><p>上面谈到了做事，谈到了比如工程方法、新技术的引用、团队文化等，那么针对技术工程，是不是可以衡量呢？答案当然是肯定的。任何工作都应该是可以衡量的，只是指标不同。这里我总结下来还是四个步骤：监控－报警－处理－优化</p>\n<p><strong>为什么要衡量？</strong></p>\n<p>在解释具体工程上的四步方法之前，先解释一个问题。可能又不少人会问，为什么你总是说衡量。看看我说的做事顺序，其中最后一步叫做“优化”。人类进行工程实践可能有几千年的历史了，很难说哪些事情是没有干过的。但是，问题的关键在于，对于每一次“质量”和“速度”博弈的实验中，如何进行优化，才是人类和动物最大的分别。我们在不断积累经验，寻找更好的方法。这就是实验的本质，也是任何事物前进的源头。“做事”的方法许多，根据团队、实践、节奏、个人知识能力等等不同，有太多的优秀实践可以应用，但是哪种是最好的，只有在指标定一下进行有效优化，才能得到越来越好的效果。所以下面说的四步，不仅仅适用于业务开发，也适用于技术工程人员对一切问题的抽象解决方案中。</p>\n<ul>\n<li><p>监控，在针对业务的技术原则中，我们说到了指标。我们说技术人员要深度参与指标的定义，并做好指标的数据向BI或其他数据报告系统反馈工作，这就是监控的一种，主要应用在业务的指标监控中。其实，针对技术本身也有一系列的可以做的监控指标，比如线上Bug的数量、Crash率、服务器Apex指标，应用可用性等等。但是这里需要需要特别注意的是，在我看来，监控不是目的，是为了解决问题而存在的，特别是解决上面说的业务目标。比如有段时间业务目标是满足双11的客流访问量，那么Apex、网络带宽、负载均衡的资源使用这些就是重要的监控，可能需要秒级别的信息收集。那么这个时段，其他的比如CPU数量、客户端Crash率、网页兼容性，在这个目标面前，就不需要被严密监控，可能是天级别监控就可以了。所以监控是分级别，也是分时段的。<br>人曾经问我，我是一个Android起家的Tech Lead，对于iOS、Java服务端，我不是很了解，要怎么才能做到很好对团队工作进行有效的带领和优化。这里我想说，通过指标监控，就可以达到目的。与相关的Tech Lead讨论针对特殊目的而细化的监控指标。</p>\n</li>\n<li><p>报警，许多人谈论监控，更多的是一套漂亮的报表，线图、饼图、柱图、箱图各种上。这是我以前常犯的错误，以为报表意味着监控。其实，在过去一年，我学会的最重要的知识就是，展现的目的更多是为了给别人看的直观，用以表明优化的成果，是一种回朔和预测型的工具。但是做事的时候，却不应该作为工具，回到计算机或者生物的本质上，0和1才是我们认识事件最直观的方法。因此，报警在我看来是工程监控最有效的手段（工具）。这种被动式相应能够大大加强响应效率和工作效率，根据响应的情况再去寻找相应的报表或者内容数据进行分析，从而得出有效的处理方案。<br>设立报警的过程本身并不难，最难的是适当报警阀值的斟酌。太高起不到监控的效果，太低虚报误报使得报警无法有效应用。这个时候需要对技术有比较深入理解，对不同阶段对应指标有比较深入理解的技术人员（这里就不是一个纯工程问题，而上升到技术本身能力上），对报警阀值进行准确的把控，并持续优化。</p>\n</li>\n<li><p>处理，这块儿没太多可说的，这是每一个工程师的老本行，根据报警问题，使用数据综合分析，数据不全可能需要进行场景复现，如果是紧急问题可能还需要采取紧急方案、备份方案、临时方案等进行解决。总之是确保报警状态恢复到正常的水平。这个时间会根据不同业务和级别的不同，处理时间也会不同。</p>\n</li>\n<li><p>优化，最后来到了前面说的，工程技术原则的核心，就是优化。前面说到互联网的最大特点就是实验，不断的、快速的实验，了解和提升用户的体验，帮助用户解决问题。所以，无论是对业务指标的优化，还是技术指标，或者别的什么指标的优化，总之优化是技术工程师做事的终点，也是新的一轮工作的起点。研发工程由此不断迭代向前，帮助解决一个又一个业务目标。</p>\n</li>\n</ul>\n<p>最后做个总结，在过去的一年里，对于上面方法的理解和应用真正改变了我将近十年的认识。过去的我，一直在学习，学习各种技术、学习各种最佳时间、学习各种方法论来解决一个又一个的研发问题。记得刚进微软那会儿，我的最大收获是如何通过互联网快速解决问题，把Google用的炉火纯青。在英国的4年里，我明白了从0到1的建设过程和研究方法。ThoughtWorks的3年让我懂得什么叫做研发体系和最佳实践。如今，在杏树林，很高兴能遇到一群志同道合的人，至少现在吧，感觉自己的理解又上升了一个新的高度，实践因什么而存在，要解决哪些问题，如何才算真正解决，解决问题的为了什么。这是我2015-16年度，最有收获的部分，共勉。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>上回留了个伏笔说：“做业务的技术原则：远景－目标－指标－做事；对应工作方法：监控－报警－处理－优化”</p>\n<p>如果说，之前我懂得的更多是如何根据不同的业务发展节奏，搭建与之相配套的技术体系的话。15-16年度，可以说是我说是重要的一年，让我对于什么叫做“工程技术体系”有了一层更佳深入的认识。这层抽象，让我更懂得回到技术的本源去看事情和问题。从这里一层层扩展开去。</p>\n<h3 id=\"工程技术的本质\"><a href=\"#工程技术的本质\" class=\"headerlink\" title=\"工程技术的本质\"></a>工程技术的本质</h3><p>我最近特别喜欢在技术前面加上工程两个字。因为这些年的工作我其实经历了一个去工程化的过程，那就是太多的人开始说到互联网，说起研发开始弱化工程师这个概念，更多的是在突出“技术”这个说法。但正是对“工程”的弱化，让我们在很多时候忽视了从传统工程领域借鉴经验的机会。事实上，传统工程和如今的IT工程确实存在区别，但本质上，作为工程化理论，就永远摆脱不了一个话题：一个关于“质量”和“速度”之间永恒的矛盾。如果细想想，对于每一个Tech Lead而言，其实就是在不断的针对这两个问题进行抉择和平衡。没有什么需求是真的做不出来，只是怎么做，才能在质量和速度之间寻求一种平衡。我们知道，在相同产能条件的情况下，</p>\n<ul>\n<li>第一、质量越高，要求的精细度越高，对应生产难度也就越大，速度也会因此而降下来。人不是机器会疲倦，会出错。</li>\n<li>第二、而如果对生产速度要求较高，那么质量的下降往往不可避免。但是可以大大提高产出结果</li>\n<li>第三、虽然是最后，但是很重要，那就是长期低质量对产品速度必然产生负面影线。</li>\n</ul>\n<p>由这三点，演化出来一系列方法和理论和话题。我在这里把“质量”与“速度”的矛盾叫做“工程技术本质”，以说明它的地位不可撼动性。</p>\n<h3 id=\"针对业务的技术原则\"><a href=\"#针对业务的技术原则\" class=\"headerlink\" title=\"针对业务的技术原则\"></a>针对业务的技术原则</h3><p>那么既然Tech Lead们需要不断解决的是“质量”和“速度”的矛盾问题。于是乎，这里面就衍生出一个很有意思的话题—如何做技术决策，如何判定自己的决策是合理。在我的这个理论里，若要做好这个决策，第一部分就是要了解业务。可能这对许多人来讲不可理解，作为技术人员，我做好自己的事情就好了，为什么要了解业务呢。这件事源于以人为中心的互联网模式。随着物质的极大丰富，供给模式，从最早的粗放型生产、分销、渠道，转变通过网络快速的寻找人的需求，根据需求进行快速迭代演进从而精细化的扩充人们在某一个或几个点上的消费习惯。在探寻过程中，其实对于互联网业务来讲，就是不断的实验过程。既然是实验，那么快速看到实验结果和实验深浅的把握就显得尤为重要。所谓实验结果，就是“速度”，即越快速度看到结果，越好进行决策是更进一步实验还是改换新的方向。实验深浅，对于技术而言就是实验的“质量”</p>\n<p>那么作为每天坐在IDE前面思考代码和逻辑思路的技术人员而言，我们无法接触真实的用户，没有时间思考和观察用户在干什么，如何才能有效的了解业务呢？我把技术人员了解业务的方法分成了四个基本过程：</p>\n<ul>\n<li><p>远景，大多指一个业务长远的，或者一段时间的方向性目标。远景的很重要，它是指导一切的原则。当然，对于一个大的业务方向来说，形成商业收支目标是核心。但是拆解下来，不同时期，对于形成收支目标的要求不一样，有的是有也不一定是要在财务上展现。比如作为如今的滴滴，当下的最大远景就是实现盈利，但是对于在Nasdaq上市的京东而言，快速增长可能比盈利更重要。作为一个公司而言，远景往往和公司的核心管理人员以及投资人的想法相关。但是，对于一个业务的远景，往往会小许多。比如病历夹工具业务线，他当下远景就是用户活跃的持续增长。所以，绝大多数功能让用户有更强的黏着性。</p>\n</li>\n<li><p>目标，通常和当下所做的工作直接相关。为了实现远景，任何的业务都会拆成几个小步工作。那么目标，就是每个小步要完成的结果。这个结果可能是让用户在上传病历过程中使用不会产生疑问，也可能是让用户更快捷的在医口袋中搜到药品。作为每一个小步而言，多数业务会设立唯一的目标。当然如果优化功能都比较小，也不排除建立几个目标的可能。目标往往是团队的负责人做的定义，但是对于远景的优化角度不同，思考方式不同，小步目标也不一定是单一的。在互联网的实际环境中，目标也是可以被拿来讨论的。从这里开始，有兴趣的技术工程师，便可以参与深入的讨论。</p>\n</li>\n<li><p>指标，既然订立了目标，就一定要寻找可以进行衡量的目标完成好坏的指标。我们拿“让用户在上传病历过程中使用不会产生疑问”这个目标来说，这里可以建立的核心指标是“用户上传病历数增加”，这是核心业务指标。分解下来，可能还有一些其他指标，比如上传服务器反应时间在200ms以下，上传成功了70%以上等等。做任何一件事情，都应该是可衡量的，没有量化的工作，就是刷流氓好么:)。定义指标的工作，需要技术工程师的重度参与，这里包括订立那些指标，以及如何将指标反馈给BI系统或者其他数据报告系统。这一点非常重要。也是后期业务、技术团队跟踪做事好坏的核心。</p>\n</li>\n<li><p>做事，在了解远景，目标达成一致，以及订立好合适的指标后，大家就要开始基于指标工作了。那么指标从某种角度上讲，就是质量的衡量方法，这个一般说来，叫做质量底线标准。任何的速度，在底线标准前，都必须服从。而做事，就是考虑，如何在保证质量的基础上，尽量的提升速度。这里就是Tech Lead们的专业技能体现了。具体的方法有很多，比如XP，Scrum等的工程实践，比如新技术的引用提升研发效率，再比如团队的文化气氛等。</p>\n</li>\n</ul>\n<p>总结一下，技术工程师，Tech Lead都很难真的完全深入业务细节去思考逻辑关系和用户使用习惯。但是经过以上的四个步骤，可以大体梳理每一个小步的背后的逻辑，并通过一系列指标进行有效的质量监控，用一系列工程方法指导做事，提升速度。</p>\n<h3 id=\"针对工程的技术原则\"><a href=\"#针对工程的技术原则\" class=\"headerlink\" title=\"针对工程的技术原则\"></a>针对工程的技术原则</h3><p>上面谈到了做事，谈到了比如工程方法、新技术的引用、团队文化等，那么针对技术工程，是不是可以衡量呢？答案当然是肯定的。任何工作都应该是可以衡量的，只是指标不同。这里我总结下来还是四个步骤：监控－报警－处理－优化</p>\n<p><strong>为什么要衡量？</strong></p>\n<p>在解释具体工程上的四步方法之前，先解释一个问题。可能又不少人会问，为什么你总是说衡量。看看我说的做事顺序，其中最后一步叫做“优化”。人类进行工程实践可能有几千年的历史了，很难说哪些事情是没有干过的。但是，问题的关键在于，对于每一次“质量”和“速度”博弈的实验中，如何进行优化，才是人类和动物最大的分别。我们在不断积累经验，寻找更好的方法。这就是实验的本质，也是任何事物前进的源头。“做事”的方法许多，根据团队、实践、节奏、个人知识能力等等不同，有太多的优秀实践可以应用，但是哪种是最好的，只有在指标定一下进行有效优化，才能得到越来越好的效果。所以下面说的四步，不仅仅适用于业务开发，也适用于技术工程人员对一切问题的抽象解决方案中。</p>\n<ul>\n<li><p>监控，在针对业务的技术原则中，我们说到了指标。我们说技术人员要深度参与指标的定义，并做好指标的数据向BI或其他数据报告系统反馈工作，这就是监控的一种，主要应用在业务的指标监控中。其实，针对技术本身也有一系列的可以做的监控指标，比如线上Bug的数量、Crash率、服务器Apex指标，应用可用性等等。但是这里需要需要特别注意的是，在我看来，监控不是目的，是为了解决问题而存在的，特别是解决上面说的业务目标。比如有段时间业务目标是满足双11的客流访问量，那么Apex、网络带宽、负载均衡的资源使用这些就是重要的监控，可能需要秒级别的信息收集。那么这个时段，其他的比如CPU数量、客户端Crash率、网页兼容性，在这个目标面前，就不需要被严密监控，可能是天级别监控就可以了。所以监控是分级别，也是分时段的。<br>人曾经问我，我是一个Android起家的Tech Lead，对于iOS、Java服务端，我不是很了解，要怎么才能做到很好对团队工作进行有效的带领和优化。这里我想说，通过指标监控，就可以达到目的。与相关的Tech Lead讨论针对特殊目的而细化的监控指标。</p>\n</li>\n<li><p>报警，许多人谈论监控，更多的是一套漂亮的报表，线图、饼图、柱图、箱图各种上。这是我以前常犯的错误，以为报表意味着监控。其实，在过去一年，我学会的最重要的知识就是，展现的目的更多是为了给别人看的直观，用以表明优化的成果，是一种回朔和预测型的工具。但是做事的时候，却不应该作为工具，回到计算机或者生物的本质上，0和1才是我们认识事件最直观的方法。因此，报警在我看来是工程监控最有效的手段（工具）。这种被动式相应能够大大加强响应效率和工作效率，根据响应的情况再去寻找相应的报表或者内容数据进行分析，从而得出有效的处理方案。<br>设立报警的过程本身并不难，最难的是适当报警阀值的斟酌。太高起不到监控的效果，太低虚报误报使得报警无法有效应用。这个时候需要对技术有比较深入理解，对不同阶段对应指标有比较深入理解的技术人员（这里就不是一个纯工程问题，而上升到技术本身能力上），对报警阀值进行准确的把控，并持续优化。</p>\n</li>\n<li><p>处理，这块儿没太多可说的，这是每一个工程师的老本行，根据报警问题，使用数据综合分析，数据不全可能需要进行场景复现，如果是紧急问题可能还需要采取紧急方案、备份方案、临时方案等进行解决。总之是确保报警状态恢复到正常的水平。这个时间会根据不同业务和级别的不同，处理时间也会不同。</p>\n</li>\n<li><p>优化，最后来到了前面说的，工程技术原则的核心，就是优化。前面说到互联网的最大特点就是实验，不断的、快速的实验，了解和提升用户的体验，帮助用户解决问题。所以，无论是对业务指标的优化，还是技术指标，或者别的什么指标的优化，总之优化是技术工程师做事的终点，也是新的一轮工作的起点。研发工程由此不断迭代向前，帮助解决一个又一个业务目标。</p>\n</li>\n</ul>\n<p>最后做个总结，在过去的一年里，对于上面方法的理解和应用真正改变了我将近十年的认识。过去的我，一直在学习，学习各种技术、学习各种最佳时间、学习各种方法论来解决一个又一个的研发问题。记得刚进微软那会儿，我的最大收获是如何通过互联网快速解决问题，把Google用的炉火纯青。在英国的4年里，我明白了从0到1的建设过程和研究方法。ThoughtWorks的3年让我懂得什么叫做研发体系和最佳实践。如今，在杏树林，很高兴能遇到一群志同道合的人，至少现在吧，感觉自己的理解又上升了一个新的高度，实践因什么而存在，要解决哪些问题，如何才算真正解决，解决问题的为了什么。这是我2015-16年度，最有收获的部分，共勉。</p>\n"},{"title":"重新认识数据驱动","date":"2016-04-10T15:10:58.000Z","_content":"这两周干的最有意思的一件事，莫过于搞清楚数据录入时效性这个衡量指标了。先讲讲故事吧：\n\n故事起源于数据录入时效性的一个分析，可以看下面的图。以前我们认为通过所谓每天完成百分数这个数据可以实现对时效性的有效评估。说白了就是，当时的一个假设是，如果1天的当天数据完成100%，那么就意味着数据可以在一天内完成，处理时效是24小时。\n\n{% asset_img efficency48h.png \"录入数据时效性\" %}\n\n但是，真实的用于工业生产，你会发现，其实每天的数据不可能达到理想的100%，而是会有各种差异，比如图里面的70%，98%等等。于是乎问题就来了，看了这个数据，我知道我们并没有完成我给用户承诺的48小时返回的指标完成情况这件事情，但是我距离这个目标差距有多远呢？于是乎我们使用了下面一个图的数据指标。\n\n{% asset_img efficency24n48.png \"新时效计算法\" %}\n\n看了17%，50%，作为24小时和48小时的处理时效。刚开始的时候这个数字吓到我了，于是想各种办法改进，当然效果是有的，但是改来改去还是回到问题的原点，我到底和我当初的预期差距有多远？就着现有的数据，我只能模糊的感觉到很远，但是我要如何解释用户那一端获得的真实感受呢？每天一大堆同事抱怨数据录入时效太慢，到底这个感受是否真实呢？真实到什么程度呢？\n\n就着这个问题，我和公司的数据分析老大做了半个下午的深入探讨。得到了如下的一番答案。首先，我得到的答案是，要解决什么问题，就要重新回到问题的原始场景，那就是我所有的问题，其实是来源于我关心用户的真实使用感受。那么我关心的所谓时效性，是返回到用户那里的时间感受，而不是作业机器和车间里的完成百分率。按照这个理论，我们重新画了一下正常作业的数据处理方法，大体是满足泊松分布的。\n\n{% asset_img poisson_distribution.png \"泊松分布图\" %}\n\n根据这个分布，大部分的数据会在前面的相对比较短的时间内快速完成，后面的数据将呈现比较明显的长尾效应。于是根据这个图，如果真的想得用户的感知，其实需要的不是计算某个时段处理的比例数，而是应该反过来，看某个固定比例数所处的时间段是多少。比如某个月最后算出来是80个小时，那么就意味着我实际返回给用户的感知是在80个小时以内。这样就很明确的界定了用户对于数据录入返回时间的感受。也解释了为什么用户反馈增加的原因。于是我们后来做了一些从后端到前端的改进方案。\n\n那么好了，说到这里大家可能觉得这个故事的结尾的确实现了数据驱动，但故事的主体，那个翻来覆去，改了三遍的指标到底意味着什么呢？这就是这两周，我特别兴奋于学会的东西，即数据背后的那个建模过程。这才是所谓数据的Insight能够产生的核心原因，也是数据驱动的基础。\n\n往往过去我们对数据驱动可能是这样认识的：\n\n1.\t我们要看数据，数据是重要的交流沟通工具，是一个可以形成共识的语言\n2.\t我们要了解各种数据，以帮组我们检验所做工作的好坏\n3.\t数据可以为决策提供有效的参考工具\n\n这在我现在看来依然是非常重要的底层基础。但是随着真实使用的场景，我渐渐发现，仅有这些是不够的。随着信息爆炸，我们每个人身边的数据都可以说是海量大，可以说各种各样的数据充斥在我们耳边。尤其是大数据的兴起，更让一群人对这方面趋之若鹜，争相希望把更多的数据展示出来。然而，随着实际工作中的经验我发现，仅仅展现数据是远远不够的。公司的统计系统里面，有超过几十个报表，成百的数据项目在平台上进行统计，可是每当我问道业务部门你们为什么不看数据时候，得到的答案往往是，我无法从数据里看出问题。\n\n这个事情曾经让我百思不得其解，直到我做完上面的故事，我才豁然开朗。原来数据驱动不是简简单单拿着数据去指导下一步的事情，而是数据是为了解决问题而产生的，它的源头应该是解决问题驱动。这就对了，因为事实上，这也是一个公司，一个团队存在的意义，就是为了解决一个又一个的问题。那么这些问题，需要经过合理的，有效的抽象，最终形成一套完整的数据指标。数据的诞生是为了提供证据，而正确的寻找证据，就能破案，能解决问题。但是如果只是数据的罗列，或者不去追踪数据的合理性，和是否能够为问题服务的特性，数据驱动其实是无法发挥价值的。\n\n所以，在数据驱动里，除了收集数据，展示数据之外，还有重要的一个部分叫做数据建模，正是通过这个部分的工作，我们把原始的问题进行剖析，寻找对问题相应的证据，这就是数据驱动的真正意义，也是解决问题的根本。","source":"_posts/datamodel.md","raw":"title: 重新认识数据驱动\ndate: 2016-04-10 23:10:58\ncategories:\n- Management\ntags:\n- book\n- mgnt\n---\n这两周干的最有意思的一件事，莫过于搞清楚数据录入时效性这个衡量指标了。先讲讲故事吧：\n\n故事起源于数据录入时效性的一个分析，可以看下面的图。以前我们认为通过所谓每天完成百分数这个数据可以实现对时效性的有效评估。说白了就是，当时的一个假设是，如果1天的当天数据完成100%，那么就意味着数据可以在一天内完成，处理时效是24小时。\n\n{% asset_img efficency48h.png \"录入数据时效性\" %}\n\n但是，真实的用于工业生产，你会发现，其实每天的数据不可能达到理想的100%，而是会有各种差异，比如图里面的70%，98%等等。于是乎问题就来了，看了这个数据，我知道我们并没有完成我给用户承诺的48小时返回的指标完成情况这件事情，但是我距离这个目标差距有多远呢？于是乎我们使用了下面一个图的数据指标。\n\n{% asset_img efficency24n48.png \"新时效计算法\" %}\n\n看了17%，50%，作为24小时和48小时的处理时效。刚开始的时候这个数字吓到我了，于是想各种办法改进，当然效果是有的，但是改来改去还是回到问题的原点，我到底和我当初的预期差距有多远？就着现有的数据，我只能模糊的感觉到很远，但是我要如何解释用户那一端获得的真实感受呢？每天一大堆同事抱怨数据录入时效太慢，到底这个感受是否真实呢？真实到什么程度呢？\n\n就着这个问题，我和公司的数据分析老大做了半个下午的深入探讨。得到了如下的一番答案。首先，我得到的答案是，要解决什么问题，就要重新回到问题的原始场景，那就是我所有的问题，其实是来源于我关心用户的真实使用感受。那么我关心的所谓时效性，是返回到用户那里的时间感受，而不是作业机器和车间里的完成百分率。按照这个理论，我们重新画了一下正常作业的数据处理方法，大体是满足泊松分布的。\n\n{% asset_img poisson_distribution.png \"泊松分布图\" %}\n\n根据这个分布，大部分的数据会在前面的相对比较短的时间内快速完成，后面的数据将呈现比较明显的长尾效应。于是根据这个图，如果真的想得用户的感知，其实需要的不是计算某个时段处理的比例数，而是应该反过来，看某个固定比例数所处的时间段是多少。比如某个月最后算出来是80个小时，那么就意味着我实际返回给用户的感知是在80个小时以内。这样就很明确的界定了用户对于数据录入返回时间的感受。也解释了为什么用户反馈增加的原因。于是我们后来做了一些从后端到前端的改进方案。\n\n那么好了，说到这里大家可能觉得这个故事的结尾的确实现了数据驱动，但故事的主体，那个翻来覆去，改了三遍的指标到底意味着什么呢？这就是这两周，我特别兴奋于学会的东西，即数据背后的那个建模过程。这才是所谓数据的Insight能够产生的核心原因，也是数据驱动的基础。\n\n往往过去我们对数据驱动可能是这样认识的：\n\n1.\t我们要看数据，数据是重要的交流沟通工具，是一个可以形成共识的语言\n2.\t我们要了解各种数据，以帮组我们检验所做工作的好坏\n3.\t数据可以为决策提供有效的参考工具\n\n这在我现在看来依然是非常重要的底层基础。但是随着真实使用的场景，我渐渐发现，仅有这些是不够的。随着信息爆炸，我们每个人身边的数据都可以说是海量大，可以说各种各样的数据充斥在我们耳边。尤其是大数据的兴起，更让一群人对这方面趋之若鹜，争相希望把更多的数据展示出来。然而，随着实际工作中的经验我发现，仅仅展现数据是远远不够的。公司的统计系统里面，有超过几十个报表，成百的数据项目在平台上进行统计，可是每当我问道业务部门你们为什么不看数据时候，得到的答案往往是，我无法从数据里看出问题。\n\n这个事情曾经让我百思不得其解，直到我做完上面的故事，我才豁然开朗。原来数据驱动不是简简单单拿着数据去指导下一步的事情，而是数据是为了解决问题而产生的，它的源头应该是解决问题驱动。这就对了，因为事实上，这也是一个公司，一个团队存在的意义，就是为了解决一个又一个的问题。那么这些问题，需要经过合理的，有效的抽象，最终形成一套完整的数据指标。数据的诞生是为了提供证据，而正确的寻找证据，就能破案，能解决问题。但是如果只是数据的罗列，或者不去追踪数据的合理性，和是否能够为问题服务的特性，数据驱动其实是无法发挥价值的。\n\n所以，在数据驱动里，除了收集数据，展示数据之外，还有重要的一个部分叫做数据建模，正是通过这个部分的工作，我们把原始的问题进行剖析，寻找对问题相应的证据，这就是数据驱动的真正意义，也是解决问题的根本。","slug":"datamodel","published":1,"updated":"2018-04-16T09:59:37.080Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offco60005rlfyxds2anvi","content":"<p>这两周干的最有意思的一件事，莫过于搞清楚数据录入时效性这个衡量指标了。先讲讲故事吧：</p>\n<p>故事起源于数据录入时效性的一个分析，可以看下面的图。以前我们认为通过所谓每天完成百分数这个数据可以实现对时效性的有效评估。说白了就是，当时的一个假设是，如果1天的当天数据完成100%，那么就意味着数据可以在一天内完成，处理时效是24小时。</p>\n<img src=\"/2016/04/10/datamodel/efficency48h.png\" title=\"录入数据时效性\">\n<p>但是，真实的用于工业生产，你会发现，其实每天的数据不可能达到理想的100%，而是会有各种差异，比如图里面的70%，98%等等。于是乎问题就来了，看了这个数据，我知道我们并没有完成我给用户承诺的48小时返回的指标完成情况这件事情，但是我距离这个目标差距有多远呢？于是乎我们使用了下面一个图的数据指标。</p>\n<img src=\"/2016/04/10/datamodel/efficency24n48.png\" title=\"新时效计算法\">\n<p>看了17%，50%，作为24小时和48小时的处理时效。刚开始的时候这个数字吓到我了，于是想各种办法改进，当然效果是有的，但是改来改去还是回到问题的原点，我到底和我当初的预期差距有多远？就着现有的数据，我只能模糊的感觉到很远，但是我要如何解释用户那一端获得的真实感受呢？每天一大堆同事抱怨数据录入时效太慢，到底这个感受是否真实呢？真实到什么程度呢？</p>\n<p>就着这个问题，我和公司的数据分析老大做了半个下午的深入探讨。得到了如下的一番答案。首先，我得到的答案是，要解决什么问题，就要重新回到问题的原始场景，那就是我所有的问题，其实是来源于我关心用户的真实使用感受。那么我关心的所谓时效性，是返回到用户那里的时间感受，而不是作业机器和车间里的完成百分率。按照这个理论，我们重新画了一下正常作业的数据处理方法，大体是满足泊松分布的。</p>\n<img src=\"/2016/04/10/datamodel/poisson_distribution.png\" title=\"泊松分布图\">\n<p>根据这个分布，大部分的数据会在前面的相对比较短的时间内快速完成，后面的数据将呈现比较明显的长尾效应。于是根据这个图，如果真的想得用户的感知，其实需要的不是计算某个时段处理的比例数，而是应该反过来，看某个固定比例数所处的时间段是多少。比如某个月最后算出来是80个小时，那么就意味着我实际返回给用户的感知是在80个小时以内。这样就很明确的界定了用户对于数据录入返回时间的感受。也解释了为什么用户反馈增加的原因。于是我们后来做了一些从后端到前端的改进方案。</p>\n<p>那么好了，说到这里大家可能觉得这个故事的结尾的确实现了数据驱动，但故事的主体，那个翻来覆去，改了三遍的指标到底意味着什么呢？这就是这两周，我特别兴奋于学会的东西，即数据背后的那个建模过程。这才是所谓数据的Insight能够产生的核心原因，也是数据驱动的基础。</p>\n<p>往往过去我们对数据驱动可能是这样认识的：</p>\n<ol>\n<li>我们要看数据，数据是重要的交流沟通工具，是一个可以形成共识的语言</li>\n<li>我们要了解各种数据，以帮组我们检验所做工作的好坏</li>\n<li>数据可以为决策提供有效的参考工具</li>\n</ol>\n<p>这在我现在看来依然是非常重要的底层基础。但是随着真实使用的场景，我渐渐发现，仅有这些是不够的。随着信息爆炸，我们每个人身边的数据都可以说是海量大，可以说各种各样的数据充斥在我们耳边。尤其是大数据的兴起，更让一群人对这方面趋之若鹜，争相希望把更多的数据展示出来。然而，随着实际工作中的经验我发现，仅仅展现数据是远远不够的。公司的统计系统里面，有超过几十个报表，成百的数据项目在平台上进行统计，可是每当我问道业务部门你们为什么不看数据时候，得到的答案往往是，我无法从数据里看出问题。</p>\n<p>这个事情曾经让我百思不得其解，直到我做完上面的故事，我才豁然开朗。原来数据驱动不是简简单单拿着数据去指导下一步的事情，而是数据是为了解决问题而产生的，它的源头应该是解决问题驱动。这就对了，因为事实上，这也是一个公司，一个团队存在的意义，就是为了解决一个又一个的问题。那么这些问题，需要经过合理的，有效的抽象，最终形成一套完整的数据指标。数据的诞生是为了提供证据，而正确的寻找证据，就能破案，能解决问题。但是如果只是数据的罗列，或者不去追踪数据的合理性，和是否能够为问题服务的特性，数据驱动其实是无法发挥价值的。</p>\n<p>所以，在数据驱动里，除了收集数据，展示数据之外，还有重要的一个部分叫做数据建模，正是通过这个部分的工作，我们把原始的问题进行剖析，寻找对问题相应的证据，这就是数据驱动的真正意义，也是解决问题的根本。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>这两周干的最有意思的一件事，莫过于搞清楚数据录入时效性这个衡量指标了。先讲讲故事吧：</p>\n<p>故事起源于数据录入时效性的一个分析，可以看下面的图。以前我们认为通过所谓每天完成百分数这个数据可以实现对时效性的有效评估。说白了就是，当时的一个假设是，如果1天的当天数据完成100%，那么就意味着数据可以在一天内完成，处理时效是24小时。</p>\n<img src=\"/2016/04/10/datamodel/efficency48h.png\" title=\"录入数据时效性\">\n<p>但是，真实的用于工业生产，你会发现，其实每天的数据不可能达到理想的100%，而是会有各种差异，比如图里面的70%，98%等等。于是乎问题就来了，看了这个数据，我知道我们并没有完成我给用户承诺的48小时返回的指标完成情况这件事情，但是我距离这个目标差距有多远呢？于是乎我们使用了下面一个图的数据指标。</p>\n<img src=\"/2016/04/10/datamodel/efficency24n48.png\" title=\"新时效计算法\">\n<p>看了17%，50%，作为24小时和48小时的处理时效。刚开始的时候这个数字吓到我了，于是想各种办法改进，当然效果是有的，但是改来改去还是回到问题的原点，我到底和我当初的预期差距有多远？就着现有的数据，我只能模糊的感觉到很远，但是我要如何解释用户那一端获得的真实感受呢？每天一大堆同事抱怨数据录入时效太慢，到底这个感受是否真实呢？真实到什么程度呢？</p>\n<p>就着这个问题，我和公司的数据分析老大做了半个下午的深入探讨。得到了如下的一番答案。首先，我得到的答案是，要解决什么问题，就要重新回到问题的原始场景，那就是我所有的问题，其实是来源于我关心用户的真实使用感受。那么我关心的所谓时效性，是返回到用户那里的时间感受，而不是作业机器和车间里的完成百分率。按照这个理论，我们重新画了一下正常作业的数据处理方法，大体是满足泊松分布的。</p>\n<img src=\"/2016/04/10/datamodel/poisson_distribution.png\" title=\"泊松分布图\">\n<p>根据这个分布，大部分的数据会在前面的相对比较短的时间内快速完成，后面的数据将呈现比较明显的长尾效应。于是根据这个图，如果真的想得用户的感知，其实需要的不是计算某个时段处理的比例数，而是应该反过来，看某个固定比例数所处的时间段是多少。比如某个月最后算出来是80个小时，那么就意味着我实际返回给用户的感知是在80个小时以内。这样就很明确的界定了用户对于数据录入返回时间的感受。也解释了为什么用户反馈增加的原因。于是我们后来做了一些从后端到前端的改进方案。</p>\n<p>那么好了，说到这里大家可能觉得这个故事的结尾的确实现了数据驱动，但故事的主体，那个翻来覆去，改了三遍的指标到底意味着什么呢？这就是这两周，我特别兴奋于学会的东西，即数据背后的那个建模过程。这才是所谓数据的Insight能够产生的核心原因，也是数据驱动的基础。</p>\n<p>往往过去我们对数据驱动可能是这样认识的：</p>\n<ol>\n<li>我们要看数据，数据是重要的交流沟通工具，是一个可以形成共识的语言</li>\n<li>我们要了解各种数据，以帮组我们检验所做工作的好坏</li>\n<li>数据可以为决策提供有效的参考工具</li>\n</ol>\n<p>这在我现在看来依然是非常重要的底层基础。但是随着真实使用的场景，我渐渐发现，仅有这些是不够的。随着信息爆炸，我们每个人身边的数据都可以说是海量大，可以说各种各样的数据充斥在我们耳边。尤其是大数据的兴起，更让一群人对这方面趋之若鹜，争相希望把更多的数据展示出来。然而，随着实际工作中的经验我发现，仅仅展现数据是远远不够的。公司的统计系统里面，有超过几十个报表，成百的数据项目在平台上进行统计，可是每当我问道业务部门你们为什么不看数据时候，得到的答案往往是，我无法从数据里看出问题。</p>\n<p>这个事情曾经让我百思不得其解，直到我做完上面的故事，我才豁然开朗。原来数据驱动不是简简单单拿着数据去指导下一步的事情，而是数据是为了解决问题而产生的，它的源头应该是解决问题驱动。这就对了，因为事实上，这也是一个公司，一个团队存在的意义，就是为了解决一个又一个的问题。那么这些问题，需要经过合理的，有效的抽象，最终形成一套完整的数据指标。数据的诞生是为了提供证据，而正确的寻找证据，就能破案，能解决问题。但是如果只是数据的罗列，或者不去追踪数据的合理性，和是否能够为问题服务的特性，数据驱动其实是无法发挥价值的。</p>\n<p>所以，在数据驱动里，除了收集数据，展示数据之外，还有重要的一个部分叫做数据建模，正是通过这个部分的工作，我们把原始的问题进行剖析，寻找对问题相应的证据，这就是数据驱动的真正意义，也是解决问题的根本。</p>\n"},{"title":"深度学习概论","date":"2018-04-24T13:43:20.000Z","mathjax":true,"_content":"\n在Andy Ng所讲的深度学习课程里，第一周的课相对比较简单，基本上以通识介绍为主。课程分为了一下几个部分：\n\n*\t什么是神经网络\n*\t监督学习的神经网络是什么\n*\t为什么深度学习开始受到重视\n*\t三巨头中的Geoffrey Hinton访谈\n\n####\t第一部分，什么是神经网络（Neural Network)\n看一下关于房价是如何预测的，以及什么是Hidden Unit\n{% asset_img nerual_network.png [nerual network] %}\n\n####\t第二部分，监督学习的神经网络是什么（Supervised Learning)\n\n在神经网络中的监督学习是这个样子的\n{% asset_img supervised_learning.png [supervised learning] %}\n\n针对与结构化和非结构化，还是有一定区别的\n{% asset_img stuctured_unstructured.png [stuctured unstructured] %}\n\n####\t第三部分，为什么深度学习开始受到重视（Why Neural Network take-off)\n一张图片说明一切\n{% asset_img why_takeoff.png [why takeoff] %}\n\n####\t第四部分，Geoffrey Hinton访谈\n这部分是一个访谈，不算是学习内容。讲述了Hinton个人的成长，学习等等吧。两个有意思的Highlight\n*\tHinton本人大学学生物物理，研究生学习心理学，博士开始研修AI，这也是为什么他在神经网络能做出重大贡献的主要原因吧\n*\t另一个关键是对GAN的学习，生成对抗网络方面的了解。如何在unsupervised的情况下进行学习","source":"_posts/deep-learning-ai-1.md","raw":"title: 深度学习概论\ndate: 2018-04-24 21:43:20\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\nmathjax: true\n---\n\n在Andy Ng所讲的深度学习课程里，第一周的课相对比较简单，基本上以通识介绍为主。课程分为了一下几个部分：\n\n*\t什么是神经网络\n*\t监督学习的神经网络是什么\n*\t为什么深度学习开始受到重视\n*\t三巨头中的Geoffrey Hinton访谈\n\n####\t第一部分，什么是神经网络（Neural Network)\n看一下关于房价是如何预测的，以及什么是Hidden Unit\n{% asset_img nerual_network.png [nerual network] %}\n\n####\t第二部分，监督学习的神经网络是什么（Supervised Learning)\n\n在神经网络中的监督学习是这个样子的\n{% asset_img supervised_learning.png [supervised learning] %}\n\n针对与结构化和非结构化，还是有一定区别的\n{% asset_img stuctured_unstructured.png [stuctured unstructured] %}\n\n####\t第三部分，为什么深度学习开始受到重视（Why Neural Network take-off)\n一张图片说明一切\n{% asset_img why_takeoff.png [why takeoff] %}\n\n####\t第四部分，Geoffrey Hinton访谈\n这部分是一个访谈，不算是学习内容。讲述了Hinton个人的成长，学习等等吧。两个有意思的Highlight\n*\tHinton本人大学学生物物理，研究生学习心理学，博士开始研修AI，这也是为什么他在神经网络能做出重大贡献的主要原因吧\n*\t另一个关键是对GAN的学习，生成对抗网络方面的了解。如何在unsupervised的情况下进行学习","slug":"deep-learning-ai-1","published":1,"updated":"2018-05-03T11:02:07.200Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offco70006rlfylf0ka009","content":"<p>在Andy Ng所讲的深度学习课程里，第一周的课相对比较简单，基本上以通识介绍为主。课程分为了一下几个部分：</p>\n<ul>\n<li>什么是神经网络</li>\n<li>监督学习的神经网络是什么</li>\n<li>为什么深度学习开始受到重视</li>\n<li>三巨头中的Geoffrey Hinton访谈</li>\n</ul>\n<h4 id=\"第一部分，什么是神经网络（Neural-Network\"><a href=\"#第一部分，什么是神经网络（Neural-Network\" class=\"headerlink\" title=\"第一部分，什么是神经网络（Neural Network)\"></a>第一部分，什么是神经网络（Neural Network)</h4><p>看一下关于房价是如何预测的，以及什么是Hidden Unit<br><img src=\"/2018/04/24/deep-learning-ai-1/nerual_network.png\" title=\"[nerual network]\"></p>\n<h4 id=\"第二部分，监督学习的神经网络是什么（Supervised-Learning\"><a href=\"#第二部分，监督学习的神经网络是什么（Supervised-Learning\" class=\"headerlink\" title=\"第二部分，监督学习的神经网络是什么（Supervised Learning)\"></a>第二部分，监督学习的神经网络是什么（Supervised Learning)</h4><p>在神经网络中的监督学习是这个样子的<br><img src=\"/2018/04/24/deep-learning-ai-1/supervised_learning.png\" title=\"[supervised learning]\"></p>\n<p>针对与结构化和非结构化，还是有一定区别的<br><img src=\"/2018/04/24/deep-learning-ai-1/stuctured_unstructured.png\" title=\"[stuctured unstructured]\"></p>\n<h4 id=\"第三部分，为什么深度学习开始受到重视（Why-Neural-Network-take-off\"><a href=\"#第三部分，为什么深度学习开始受到重视（Why-Neural-Network-take-off\" class=\"headerlink\" title=\"第三部分，为什么深度学习开始受到重视（Why Neural Network take-off)\"></a>第三部分，为什么深度学习开始受到重视（Why Neural Network take-off)</h4><p>一张图片说明一切<br><img src=\"/2018/04/24/deep-learning-ai-1/why_takeoff.png\" title=\"[why takeoff]\"></p>\n<h4 id=\"第四部分，Geoffrey-Hinton访谈\"><a href=\"#第四部分，Geoffrey-Hinton访谈\" class=\"headerlink\" title=\"第四部分，Geoffrey Hinton访谈\"></a>第四部分，Geoffrey Hinton访谈</h4><p>这部分是一个访谈，不算是学习内容。讲述了Hinton个人的成长，学习等等吧。两个有意思的Highlight</p>\n<ul>\n<li>Hinton本人大学学生物物理，研究生学习心理学，博士开始研修AI，这也是为什么他在神经网络能做出重大贡献的主要原因吧</li>\n<li>另一个关键是对GAN的学习，生成对抗网络方面的了解。如何在unsupervised的情况下进行学习</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>在Andy Ng所讲的深度学习课程里，第一周的课相对比较简单，基本上以通识介绍为主。课程分为了一下几个部分：</p>\n<ul>\n<li>什么是神经网络</li>\n<li>监督学习的神经网络是什么</li>\n<li>为什么深度学习开始受到重视</li>\n<li>三巨头中的Geoffrey Hinton访谈</li>\n</ul>\n<h4 id=\"第一部分，什么是神经网络（Neural-Network\"><a href=\"#第一部分，什么是神经网络（Neural-Network\" class=\"headerlink\" title=\"第一部分，什么是神经网络（Neural Network)\"></a>第一部分，什么是神经网络（Neural Network)</h4><p>看一下关于房价是如何预测的，以及什么是Hidden Unit<br><img src=\"/2018/04/24/deep-learning-ai-1/nerual_network.png\" title=\"[nerual network]\"></p>\n<h4 id=\"第二部分，监督学习的神经网络是什么（Supervised-Learning\"><a href=\"#第二部分，监督学习的神经网络是什么（Supervised-Learning\" class=\"headerlink\" title=\"第二部分，监督学习的神经网络是什么（Supervised Learning)\"></a>第二部分，监督学习的神经网络是什么（Supervised Learning)</h4><p>在神经网络中的监督学习是这个样子的<br><img src=\"/2018/04/24/deep-learning-ai-1/supervised_learning.png\" title=\"[supervised learning]\"></p>\n<p>针对与结构化和非结构化，还是有一定区别的<br><img src=\"/2018/04/24/deep-learning-ai-1/stuctured_unstructured.png\" title=\"[stuctured unstructured]\"></p>\n<h4 id=\"第三部分，为什么深度学习开始受到重视（Why-Neural-Network-take-off\"><a href=\"#第三部分，为什么深度学习开始受到重视（Why-Neural-Network-take-off\" class=\"headerlink\" title=\"第三部分，为什么深度学习开始受到重视（Why Neural Network take-off)\"></a>第三部分，为什么深度学习开始受到重视（Why Neural Network take-off)</h4><p>一张图片说明一切<br><img src=\"/2018/04/24/deep-learning-ai-1/why_takeoff.png\" title=\"[why takeoff]\"></p>\n<h4 id=\"第四部分，Geoffrey-Hinton访谈\"><a href=\"#第四部分，Geoffrey-Hinton访谈\" class=\"headerlink\" title=\"第四部分，Geoffrey Hinton访谈\"></a>第四部分，Geoffrey Hinton访谈</h4><p>这部分是一个访谈，不算是学习内容。讲述了Hinton个人的成长，学习等等吧。两个有意思的Highlight</p>\n<ul>\n<li>Hinton本人大学学生物物理，研究生学习心理学，博士开始研修AI，这也是为什么他在神经网络能做出重大贡献的主要原因吧</li>\n<li>另一个关键是对GAN的学习，生成对抗网络方面的了解。如何在unsupervised的情况下进行学习</li>\n</ul>\n"},{"title":"关于Collection和Set","date":"2015-08-28T23:39:44.000Z","_content":"早上看MongoDB的文档，忽然看到Collection，疑惑了一下为什么起这个名字，于是上网搜寻了一下Collection和Set的区别，主要是看了[Stackexchange的一篇文章](http://math.stackexchange.com/questions/172966/difference-between-class-set-family-and-collection)。\n\n也就是认为Collection代表一个集合，可以放任何东西，是一种总集合。数学上，在提及一个概念的时候，通常正式表带为notion。然后用公立（axioms）来描述这个概念（notion）。如果这个公立被认为不是自相矛盾的，那么就开始根据公立来定义一系列定理（definition）。所谓一个Collection，就是那些适用公立存在的东西（a collection is a notion of something that we can talk about, like a mystery bag）。\n\n那么说Set有什么不同呢？Native Belief，我们本能可以相信Collection就是Set，尤其是对于非数学人士。这里的例外是Collection包含有哪些悖论的存在，而Set是不可以有的。如果是一个Set，那么通过反证也可以推导出是Set的结论。但是悖论不行，如Russell's Paradox。\n\n这里补充知识，Russell's Paradox（罗素悖论）也叫理发师悖论\n在某个城市中有一位理发师，他的广告词是这样写的：“本人的理发技艺十分高超，誉满全城。我将为本城所有不给自己刮脸的人刮脸，我也只给这些人刮脸。我对各位表示热诚欢迎！”来找他刮脸的人络绎不绝，自然都是那些不给自己刮脸的人。可是，有一天，这位理发师从镜子里看见自己的胡子长了，他本能地抓起了剃刀，你们看他能不能给他自己刮脸呢？如果他不给自己刮脸，他就属于“不给自己刮脸的人”，他就要给自己刮脸，而如果他给自己刮脸呢？他又属于“给自己刮脸的人”，他就不该给自己刮脸。于是产生矛盾。\n\nCollection公式如下\n{% asset_img russell_paradox.png \"russell_paradox\" %}\n\n<!-- ![russell_paradox](russell_paradox.png) -->\n\n这个就不应该叫做Set了，但是依然可以称之为Collection。本来悖论这个事情，造成了第三次数学危机。不过内涵公里，进行了证明。\n","source":"_posts/collection-n-set.md","raw":"title: 关于Collection和Set\ndate: 2015-08-29 07:39:44\ncategories:\n- Technology\ntags:\n- math\n---\n早上看MongoDB的文档，忽然看到Collection，疑惑了一下为什么起这个名字，于是上网搜寻了一下Collection和Set的区别，主要是看了[Stackexchange的一篇文章](http://math.stackexchange.com/questions/172966/difference-between-class-set-family-and-collection)。\n\n也就是认为Collection代表一个集合，可以放任何东西，是一种总集合。数学上，在提及一个概念的时候，通常正式表带为notion。然后用公立（axioms）来描述这个概念（notion）。如果这个公立被认为不是自相矛盾的，那么就开始根据公立来定义一系列定理（definition）。所谓一个Collection，就是那些适用公立存在的东西（a collection is a notion of something that we can talk about, like a mystery bag）。\n\n那么说Set有什么不同呢？Native Belief，我们本能可以相信Collection就是Set，尤其是对于非数学人士。这里的例外是Collection包含有哪些悖论的存在，而Set是不可以有的。如果是一个Set，那么通过反证也可以推导出是Set的结论。但是悖论不行，如Russell's Paradox。\n\n这里补充知识，Russell's Paradox（罗素悖论）也叫理发师悖论\n在某个城市中有一位理发师，他的广告词是这样写的：“本人的理发技艺十分高超，誉满全城。我将为本城所有不给自己刮脸的人刮脸，我也只给这些人刮脸。我对各位表示热诚欢迎！”来找他刮脸的人络绎不绝，自然都是那些不给自己刮脸的人。可是，有一天，这位理发师从镜子里看见自己的胡子长了，他本能地抓起了剃刀，你们看他能不能给他自己刮脸呢？如果他不给自己刮脸，他就属于“不给自己刮脸的人”，他就要给自己刮脸，而如果他给自己刮脸呢？他又属于“给自己刮脸的人”，他就不该给自己刮脸。于是产生矛盾。\n\nCollection公式如下\n{% asset_img russell_paradox.png \"russell_paradox\" %}\n\n<!-- ![russell_paradox](russell_paradox.png) -->\n\n这个就不应该叫做Set了，但是依然可以称之为Collection。本来悖论这个事情，造成了第三次数学危机。不过内涵公里，进行了证明。\n","slug":"collection-n-set","published":1,"updated":"2018-04-16T09:59:37.055Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offco80007rlfyaew1b5tz","content":"<p>早上看MongoDB的文档，忽然看到Collection，疑惑了一下为什么起这个名字，于是上网搜寻了一下Collection和Set的区别，主要是看了<a href=\"http://math.stackexchange.com/questions/172966/difference-between-class-set-family-and-collection\" target=\"_blank\" rel=\"noopener\">Stackexchange的一篇文章</a>。</p>\n<p>也就是认为Collection代表一个集合，可以放任何东西，是一种总集合。数学上，在提及一个概念的时候，通常正式表带为notion。然后用公立（axioms）来描述这个概念（notion）。如果这个公立被认为不是自相矛盾的，那么就开始根据公立来定义一系列定理（definition）。所谓一个Collection，就是那些适用公立存在的东西（a collection is a notion of something that we can talk about, like a mystery bag）。</p>\n<p>那么说Set有什么不同呢？Native Belief，我们本能可以相信Collection就是Set，尤其是对于非数学人士。这里的例外是Collection包含有哪些悖论的存在，而Set是不可以有的。如果是一个Set，那么通过反证也可以推导出是Set的结论。但是悖论不行，如Russell’s Paradox。</p>\n<p>这里补充知识，Russell’s Paradox（罗素悖论）也叫理发师悖论<br>在某个城市中有一位理发师，他的广告词是这样写的：“本人的理发技艺十分高超，誉满全城。我将为本城所有不给自己刮脸的人刮脸，我也只给这些人刮脸。我对各位表示热诚欢迎！”来找他刮脸的人络绎不绝，自然都是那些不给自己刮脸的人。可是，有一天，这位理发师从镜子里看见自己的胡子长了，他本能地抓起了剃刀，你们看他能不能给他自己刮脸呢？如果他不给自己刮脸，他就属于“不给自己刮脸的人”，他就要给自己刮脸，而如果他给自己刮脸呢？他又属于“给自己刮脸的人”，他就不该给自己刮脸。于是产生矛盾。</p>\n<p>Collection公式如下<br><img src=\"/2015/08/29/collection-n-set/russell_paradox.png\" title=\"russell_paradox\"></p>\n<!-- ![russell_paradox](russell_paradox.png) -->\n<p>这个就不应该叫做Set了，但是依然可以称之为Collection。本来悖论这个事情，造成了第三次数学危机。不过内涵公里，进行了证明。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>早上看MongoDB的文档，忽然看到Collection，疑惑了一下为什么起这个名字，于是上网搜寻了一下Collection和Set的区别，主要是看了<a href=\"http://math.stackexchange.com/questions/172966/difference-between-class-set-family-and-collection\" target=\"_blank\" rel=\"noopener\">Stackexchange的一篇文章</a>。</p>\n<p>也就是认为Collection代表一个集合，可以放任何东西，是一种总集合。数学上，在提及一个概念的时候，通常正式表带为notion。然后用公立（axioms）来描述这个概念（notion）。如果这个公立被认为不是自相矛盾的，那么就开始根据公立来定义一系列定理（definition）。所谓一个Collection，就是那些适用公立存在的东西（a collection is a notion of something that we can talk about, like a mystery bag）。</p>\n<p>那么说Set有什么不同呢？Native Belief，我们本能可以相信Collection就是Set，尤其是对于非数学人士。这里的例外是Collection包含有哪些悖论的存在，而Set是不可以有的。如果是一个Set，那么通过反证也可以推导出是Set的结论。但是悖论不行，如Russell’s Paradox。</p>\n<p>这里补充知识，Russell’s Paradox（罗素悖论）也叫理发师悖论<br>在某个城市中有一位理发师，他的广告词是这样写的：“本人的理发技艺十分高超，誉满全城。我将为本城所有不给自己刮脸的人刮脸，我也只给这些人刮脸。我对各位表示热诚欢迎！”来找他刮脸的人络绎不绝，自然都是那些不给自己刮脸的人。可是，有一天，这位理发师从镜子里看见自己的胡子长了，他本能地抓起了剃刀，你们看他能不能给他自己刮脸呢？如果他不给自己刮脸，他就属于“不给自己刮脸的人”，他就要给自己刮脸，而如果他给自己刮脸呢？他又属于“给自己刮脸的人”，他就不该给自己刮脸。于是产生矛盾。</p>\n<p>Collection公式如下<br><img src=\"/2015/08/29/collection-n-set/russell_paradox.png\" title=\"russell_paradox\"></p>\n<!-- ![russell_paradox](russell_paradox.png) -->\n<p>这个就不应该叫做Set了，但是依然可以称之为Collection。本来悖论这个事情，造成了第三次数学危机。不过内涵公里，进行了证明。</p>\n"},{"title":"深度学习第二周课程（下）","date":"2018-04-25T13:22:38.000Z","_content":"\n上一章节提到的梯度下降（Gradient Decent）过程需要多层嵌套For-Loop循环。这种循环非常耗费计算资源。为了降低计算资源消耗，提升计算效率，本章节引入向量计算（Vectorization）的概念。本章的主要内容也是围绕着向量计算和使用Python中的numpy库来实现限量计算的过程。\n\n本章课程内容目录（与本文无关）：\n*\t向量计算（Vectorization）\n*\t使用向量计算逻辑回归（Vectorizing Logistic Regression）\n*\t使用向量计算逻辑回归中的梯度下降（Vectorizing Logistic Regression's Gradient）\n*\tPython的广播（Broadcasting in Python）\n*\tPython/numpy向量的介绍\n*\t逻辑回归里的Cost Function解释\n\n### 安装一下jupyter\n\n本章开始需要进行练习，Python是必须要装的，强烈建议Python3，课程使用Jupyter，这里也提示了一下安装，不过后来发现好像用处也不大。这个工具主要是可以把Python的程序脚本和文字进行混排，方便与演示。如果从来没有接触过Python的话，可以考虑用一下，毕竟这个是课程也在用的环境。如果有一点基础的话，Python有自己的IDE工具，PyCharm，很好用直接下载就行。\n\n安装Python&Jupyter。因为一直写Python所以这个一直有没什么大问题，但是Jupyt这个倒是头一次见，好像是一种基于Browser的IDE，挺有意思的。具体可以访问以下几个地址：\n\n*\t安装Python，我一直用Brew install就好了\n*\t安装Pip，不过因为买了这个新电脑以后就没怎么写代码，所以竟然没有Pip。这个倒也不难，随便上网搜索“install pip”，两步简单操作就搞定了\n*\t安装Jupyter，没有按照web上说的用pkg包的方式，只是担心会安装额外的Python3.所以选择了通过pip命令行形式进行安装。\n\nOK，三个安装结束，键入下面命令，直接启动Browser的界面\n\n```\njupyter notebook\n```\n\n下面是标准编辑页面（竟然Hexo的asset_img可以用，好激动，但是asset_link确实不行，估计是因为marked里面escape的问题）\n\n{% asset_img jupyter.png [jupyter starter] %}\n\n首界面很简单，就是系统文件夹。右上角有新建功能，可以新建一个可运行的python文件。在课程中，Andrew主要介绍了 np.dot(i,j) 在程序中对比for-loop，超过300倍的优势。笔者后来自己查了一下原因，看来主要还是回到了Python作为解释性语言本身的问题。为了对初学者友好，Python作为解释性语言牺牲了许多性能上的东西。而np.dot之所以much faster，主要原因是一句Python语言，对应的是用C写成numpy库，这个库会将输入进来的数据进行编译，形成编译后的语言进行调用。这种方法，远好于Python一个字符一个字符的进行读取，并根据语法分析器进行描述，占据了大量的时间。当然其他原因也有许多，比如借助编译可以使用CPU或者GPU的SIMD指令集（Simple instuction multiple data）进行并行计算，大大提升效率。根据文章将，numpy的效率可以是原生python的2万倍。而据说选用cpython会达到20万倍之多。具体原理可以参看知乎上的这篇文章：\n\n[python的numpy向量化语句为什么会比for快？](https://www.zhihu.com/question/67652386)\n\n### 建立神经网络的主要过程\n\n####\t先来回顾一下基础算法：\nFor one example $x^{(i)}$:\n$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n\nThe cost is then computed by summing over all training examples:\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$\n\n####\t建模过程\n*\t定义模型结构（例如输入feature的数量）\n*\t初始化模型参数\n*\t升级参数（Gradient Descent）\n\n讲上述三个部分逐个建立并整合进一个叫做model()的 $function$ 里。几个简单的 $function$ 会包含的 $sigmoid$ , initialize_with_zeros() , propagate() \n\n特别说明，关于 propagate() 的算法回顾如下：\nForward Propagation:\n- You get X\n- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\nHere are the two formulas you will be using: \n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n\n下面的Code里面用到了一些“内积”、“外积”、“General Dot”的概念。在课程中有相关的联系材料。具体参见这个解释可能会更加实在\n\n[使用numpy进行行列式乘积的计算](https://hk.saowen.com/a/c2cbbdb3dc43d41a717517faca384dc6228a9d2cbab31b59eca3f468c59e33b4)\n\n\n```Python\t\n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    A = sigmoid(np.dot(X.T, w) + b).reshape(1, -1) # compute activation\n    cost = - (1/m) * np.sum(Y*np.log(A) + (1-Y) * np.log(1-A))  # compute cost\n    ### END CODE HERE ###\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    dw = 1/m * np.dot(X, (A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost\n```\n\n总结一下，这里建立的初始模型包括\n$sigmoid$, $initialize$, $propagate$\n\n####\t优化模型参数（Optimization）\n*\t初始化参数\n*\t计算cost function \n*\t通过梯级下降的方式进行参数更新并计算w和b的结果()\n\n最近基本的梯级下降依据如下：\n$$ \\theta = \\theta - \\alpha \\text{ } d\\theta \\tag{9}$$ \nwhere $\\alpha$ is the learning rate\n\n{% asset_img closing.jpg [gradient descent] %}\n\n总结一下，这里建立的初始模型包括\n$initialize$, $optimize$, $predict$\n\n####\t整合模型\n\n合并模型建立$model$，使用plot建立拟合线\n\n","source":"_posts/deep-learning-ai-2-1.md","raw":"title: 深度学习第二周课程（下）\ndate: 2018-04-25 21:22:38\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\n---\n\n上一章节提到的梯度下降（Gradient Decent）过程需要多层嵌套For-Loop循环。这种循环非常耗费计算资源。为了降低计算资源消耗，提升计算效率，本章节引入向量计算（Vectorization）的概念。本章的主要内容也是围绕着向量计算和使用Python中的numpy库来实现限量计算的过程。\n\n本章课程内容目录（与本文无关）：\n*\t向量计算（Vectorization）\n*\t使用向量计算逻辑回归（Vectorizing Logistic Regression）\n*\t使用向量计算逻辑回归中的梯度下降（Vectorizing Logistic Regression's Gradient）\n*\tPython的广播（Broadcasting in Python）\n*\tPython/numpy向量的介绍\n*\t逻辑回归里的Cost Function解释\n\n### 安装一下jupyter\n\n本章开始需要进行练习，Python是必须要装的，强烈建议Python3，课程使用Jupyter，这里也提示了一下安装，不过后来发现好像用处也不大。这个工具主要是可以把Python的程序脚本和文字进行混排，方便与演示。如果从来没有接触过Python的话，可以考虑用一下，毕竟这个是课程也在用的环境。如果有一点基础的话，Python有自己的IDE工具，PyCharm，很好用直接下载就行。\n\n安装Python&Jupyter。因为一直写Python所以这个一直有没什么大问题，但是Jupyt这个倒是头一次见，好像是一种基于Browser的IDE，挺有意思的。具体可以访问以下几个地址：\n\n*\t安装Python，我一直用Brew install就好了\n*\t安装Pip，不过因为买了这个新电脑以后就没怎么写代码，所以竟然没有Pip。这个倒也不难，随便上网搜索“install pip”，两步简单操作就搞定了\n*\t安装Jupyter，没有按照web上说的用pkg包的方式，只是担心会安装额外的Python3.所以选择了通过pip命令行形式进行安装。\n\nOK，三个安装结束，键入下面命令，直接启动Browser的界面\n\n```\njupyter notebook\n```\n\n下面是标准编辑页面（竟然Hexo的asset_img可以用，好激动，但是asset_link确实不行，估计是因为marked里面escape的问题）\n\n{% asset_img jupyter.png [jupyter starter] %}\n\n首界面很简单，就是系统文件夹。右上角有新建功能，可以新建一个可运行的python文件。在课程中，Andrew主要介绍了 np.dot(i,j) 在程序中对比for-loop，超过300倍的优势。笔者后来自己查了一下原因，看来主要还是回到了Python作为解释性语言本身的问题。为了对初学者友好，Python作为解释性语言牺牲了许多性能上的东西。而np.dot之所以much faster，主要原因是一句Python语言，对应的是用C写成numpy库，这个库会将输入进来的数据进行编译，形成编译后的语言进行调用。这种方法，远好于Python一个字符一个字符的进行读取，并根据语法分析器进行描述，占据了大量的时间。当然其他原因也有许多，比如借助编译可以使用CPU或者GPU的SIMD指令集（Simple instuction multiple data）进行并行计算，大大提升效率。根据文章将，numpy的效率可以是原生python的2万倍。而据说选用cpython会达到20万倍之多。具体原理可以参看知乎上的这篇文章：\n\n[python的numpy向量化语句为什么会比for快？](https://www.zhihu.com/question/67652386)\n\n### 建立神经网络的主要过程\n\n####\t先来回顾一下基础算法：\nFor one example $x^{(i)}$:\n$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n\nThe cost is then computed by summing over all training examples:\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$\n\n####\t建模过程\n*\t定义模型结构（例如输入feature的数量）\n*\t初始化模型参数\n*\t升级参数（Gradient Descent）\n\n讲上述三个部分逐个建立并整合进一个叫做model()的 $function$ 里。几个简单的 $function$ 会包含的 $sigmoid$ , initialize_with_zeros() , propagate() \n\n特别说明，关于 propagate() 的算法回顾如下：\nForward Propagation:\n- You get X\n- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\nHere are the two formulas you will be using: \n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n\n下面的Code里面用到了一些“内积”、“外积”、“General Dot”的概念。在课程中有相关的联系材料。具体参见这个解释可能会更加实在\n\n[使用numpy进行行列式乘积的计算](https://hk.saowen.com/a/c2cbbdb3dc43d41a717517faca384dc6228a9d2cbab31b59eca3f468c59e33b4)\n\n\n```Python\t\n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    A = sigmoid(np.dot(X.T, w) + b).reshape(1, -1) # compute activation\n    cost = - (1/m) * np.sum(Y*np.log(A) + (1-Y) * np.log(1-A))  # compute cost\n    ### END CODE HERE ###\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    dw = 1/m * np.dot(X, (A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost\n```\n\n总结一下，这里建立的初始模型包括\n$sigmoid$, $initialize$, $propagate$\n\n####\t优化模型参数（Optimization）\n*\t初始化参数\n*\t计算cost function \n*\t通过梯级下降的方式进行参数更新并计算w和b的结果()\n\n最近基本的梯级下降依据如下：\n$$ \\theta = \\theta - \\alpha \\text{ } d\\theta \\tag{9}$$ \nwhere $\\alpha$ is the learning rate\n\n{% asset_img closing.jpg [gradient descent] %}\n\n总结一下，这里建立的初始模型包括\n$initialize$, $optimize$, $predict$\n\n####\t整合模型\n\n合并模型建立$model$，使用plot建立拟合线\n\n","slug":"deep-learning-ai-2-1","published":1,"updated":"2018-05-03T11:14:05.130Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offco90008rlfy8wrn1jx0","content":"<p>上一章节提到的梯度下降（Gradient Decent）过程需要多层嵌套For-Loop循环。这种循环非常耗费计算资源。为了降低计算资源消耗，提升计算效率，本章节引入向量计算（Vectorization）的概念。本章的主要内容也是围绕着向量计算和使用Python中的numpy库来实现限量计算的过程。</p>\n<p>本章课程内容目录（与本文无关）：</p>\n<ul>\n<li>向量计算（Vectorization）</li>\n<li>使用向量计算逻辑回归（Vectorizing Logistic Regression）</li>\n<li>使用向量计算逻辑回归中的梯度下降（Vectorizing Logistic Regression’s Gradient）</li>\n<li>Python的广播（Broadcasting in Python）</li>\n<li>Python/numpy向量的介绍</li>\n<li>逻辑回归里的Cost Function解释</li>\n</ul>\n<h3 id=\"安装一下jupyter\"><a href=\"#安装一下jupyter\" class=\"headerlink\" title=\"安装一下jupyter\"></a>安装一下jupyter</h3><p>本章开始需要进行练习，Python是必须要装的，强烈建议Python3，课程使用Jupyter，这里也提示了一下安装，不过后来发现好像用处也不大。这个工具主要是可以把Python的程序脚本和文字进行混排，方便与演示。如果从来没有接触过Python的话，可以考虑用一下，毕竟这个是课程也在用的环境。如果有一点基础的话，Python有自己的IDE工具，PyCharm，很好用直接下载就行。</p>\n<p>安装Python&amp;Jupyter。因为一直写Python所以这个一直有没什么大问题，但是Jupyt这个倒是头一次见，好像是一种基于Browser的IDE，挺有意思的。具体可以访问以下几个地址：</p>\n<ul>\n<li>安装Python，我一直用Brew install就好了</li>\n<li>安装Pip，不过因为买了这个新电脑以后就没怎么写代码，所以竟然没有Pip。这个倒也不难，随便上网搜索“install pip”，两步简单操作就搞定了</li>\n<li>安装Jupyter，没有按照web上说的用pkg包的方式，只是担心会安装额外的Python3.所以选择了通过pip命令行形式进行安装。</li>\n</ul>\n<p>OK，三个安装结束，键入下面命令，直接启动Browser的界面</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook</span><br></pre></td></tr></table></figure>\n<p>下面是标准编辑页面（竟然Hexo的asset_img可以用，好激动，但是asset_link确实不行，估计是因为marked里面escape的问题）</p>\n<img src=\"/2018/04/25/deep-learning-ai-2-1/jupyter.png\" title=\"[jupyter starter]\">\n<p>首界面很简单，就是系统文件夹。右上角有新建功能，可以新建一个可运行的python文件。在课程中，Andrew主要介绍了 np.dot(i,j) 在程序中对比for-loop，超过300倍的优势。笔者后来自己查了一下原因，看来主要还是回到了Python作为解释性语言本身的问题。为了对初学者友好，Python作为解释性语言牺牲了许多性能上的东西。而np.dot之所以much faster，主要原因是一句Python语言，对应的是用C写成numpy库，这个库会将输入进来的数据进行编译，形成编译后的语言进行调用。这种方法，远好于Python一个字符一个字符的进行读取，并根据语法分析器进行描述，占据了大量的时间。当然其他原因也有许多，比如借助编译可以使用CPU或者GPU的SIMD指令集（Simple instuction multiple data）进行并行计算，大大提升效率。根据文章将，numpy的效率可以是原生python的2万倍。而据说选用cpython会达到20万倍之多。具体原理可以参看知乎上的这篇文章：</p>\n<p><a href=\"https://www.zhihu.com/question/67652386\" target=\"_blank\" rel=\"noopener\">python的numpy向量化语句为什么会比for快？</a></p>\n<h3 id=\"建立神经网络的主要过程\"><a href=\"#建立神经网络的主要过程\" class=\"headerlink\" title=\"建立神经网络的主要过程\"></a>建立神经网络的主要过程</h3><h4 id=\"先来回顾一下基础算法：\"><a href=\"#先来回顾一下基础算法：\" class=\"headerlink\" title=\"先来回顾一下基础算法：\"></a>先来回顾一下基础算法：</h4><p>For one example $x^{(i)}$:</p>\n<script type=\"math/tex; mode=display\">z^{(i)} = w^T x^{(i)} + b \\tag{1}</script><script type=\"math/tex; mode=display\">\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}</script><script type=\"math/tex; mode=display\">\\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}</script><p>The cost is then computed by summing over all training examples:</p>\n<script type=\"math/tex; mode=display\">J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}</script><h4 id=\"建模过程\"><a href=\"#建模过程\" class=\"headerlink\" title=\"建模过程\"></a>建模过程</h4><ul>\n<li>定义模型结构（例如输入feature的数量）</li>\n<li>初始化模型参数</li>\n<li>升级参数（Gradient Descent）</li>\n</ul>\n<p>讲上述三个部分逐个建立并整合进一个叫做model()的 $function$ 里。几个简单的 $function$ 会包含的 $sigmoid$ , initialize_with_zeros() , propagate() </p>\n<p>特别说明，关于 propagate() 的算法回顾如下：<br>Forward Propagation:</p>\n<ul>\n<li>You get X</li>\n<li>You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, …, a^{(m-1)}, a^{(m)})$</li>\n<li>You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$</li>\n</ul>\n<p>Here are the two formulas you will be using: </p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}</script><script type=\"math/tex; mode=display\">\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}</script><p>下面的Code里面用到了一些“内积”、“外积”、“General Dot”的概念。在课程中有相关的联系材料。具体参见这个解释可能会更加实在</p>\n<p><a href=\"https://hk.saowen.com/a/c2cbbdb3dc43d41a717517faca384dc6228a9d2cbab31b59eca3f468c59e33b4\" target=\"_blank\" rel=\"noopener\">使用numpy进行行列式乘积的计算</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">m = X.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class=\"line\"><span class=\"comment\">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class=\"line\">A = sigmoid(np.dot(X.T, w) + b).reshape(<span class=\"number\">1</span>, <span class=\"number\">-1</span>) <span class=\"comment\"># compute activation</span></span><br><span class=\"line\">cost = - (<span class=\"number\">1</span>/m) * np.sum(Y*np.log(A) + (<span class=\"number\">1</span>-Y) * np.log(<span class=\"number\">1</span>-A))  <span class=\"comment\"># compute cost</span></span><br><span class=\"line\"><span class=\"comment\">### END CODE HERE ###</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class=\"line\"><span class=\"comment\">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class=\"line\">dw = <span class=\"number\">1</span>/m * np.dot(X, (A-Y).T)</span><br><span class=\"line\">db = <span class=\"number\">1</span>/m * np.sum(A-Y)</span><br><span class=\"line\"><span class=\"comment\">### END CODE HERE ###</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">assert</span>(dw.shape == w.shape)</span><br><span class=\"line\"><span class=\"keyword\">assert</span>(db.dtype == float)</span><br><span class=\"line\">cost = np.squeeze(cost)</span><br><span class=\"line\"><span class=\"keyword\">assert</span>(cost.shape == ())</span><br><span class=\"line\"></span><br><span class=\"line\">grads = &#123;<span class=\"string\">\"dw\"</span>: dw,</span><br><span class=\"line\">         <span class=\"string\">\"db\"</span>: db&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">return</span> grads, cost</span><br></pre></td></tr></table></figure>\n<p>总结一下，这里建立的初始模型包括<br>$sigmoid$, $initialize$, $propagate$</p>\n<h4 id=\"优化模型参数（Optimization）\"><a href=\"#优化模型参数（Optimization）\" class=\"headerlink\" title=\"优化模型参数（Optimization）\"></a>优化模型参数（Optimization）</h4><ul>\n<li>初始化参数</li>\n<li>计算cost function </li>\n<li>通过梯级下降的方式进行参数更新并计算w和b的结果()</li>\n</ul>\n<p>最近基本的梯级下降依据如下：</p>\n<script type=\"math/tex; mode=display\">\\theta = \\theta - \\alpha \\text{ } d\\theta \\tag{9}</script><p>where $\\alpha$ is the learning rate</p>\n<img src=\"/2018/04/25/deep-learning-ai-2-1/closing.jpg\" title=\"[gradient descent]\">\n<p>总结一下，这里建立的初始模型包括<br>$initialize$, $optimize$, $predict$</p>\n<h4 id=\"整合模型\"><a href=\"#整合模型\" class=\"headerlink\" title=\"整合模型\"></a>整合模型</h4><p>合并模型建立$model$，使用plot建立拟合线</p>\n","site":{"data":{}},"excerpt":"","more":"<p>上一章节提到的梯度下降（Gradient Decent）过程需要多层嵌套For-Loop循环。这种循环非常耗费计算资源。为了降低计算资源消耗，提升计算效率，本章节引入向量计算（Vectorization）的概念。本章的主要内容也是围绕着向量计算和使用Python中的numpy库来实现限量计算的过程。</p>\n<p>本章课程内容目录（与本文无关）：</p>\n<ul>\n<li>向量计算（Vectorization）</li>\n<li>使用向量计算逻辑回归（Vectorizing Logistic Regression）</li>\n<li>使用向量计算逻辑回归中的梯度下降（Vectorizing Logistic Regression’s Gradient）</li>\n<li>Python的广播（Broadcasting in Python）</li>\n<li>Python/numpy向量的介绍</li>\n<li>逻辑回归里的Cost Function解释</li>\n</ul>\n<h3 id=\"安装一下jupyter\"><a href=\"#安装一下jupyter\" class=\"headerlink\" title=\"安装一下jupyter\"></a>安装一下jupyter</h3><p>本章开始需要进行练习，Python是必须要装的，强烈建议Python3，课程使用Jupyter，这里也提示了一下安装，不过后来发现好像用处也不大。这个工具主要是可以把Python的程序脚本和文字进行混排，方便与演示。如果从来没有接触过Python的话，可以考虑用一下，毕竟这个是课程也在用的环境。如果有一点基础的话，Python有自己的IDE工具，PyCharm，很好用直接下载就行。</p>\n<p>安装Python&amp;Jupyter。因为一直写Python所以这个一直有没什么大问题，但是Jupyt这个倒是头一次见，好像是一种基于Browser的IDE，挺有意思的。具体可以访问以下几个地址：</p>\n<ul>\n<li>安装Python，我一直用Brew install就好了</li>\n<li>安装Pip，不过因为买了这个新电脑以后就没怎么写代码，所以竟然没有Pip。这个倒也不难，随便上网搜索“install pip”，两步简单操作就搞定了</li>\n<li>安装Jupyter，没有按照web上说的用pkg包的方式，只是担心会安装额外的Python3.所以选择了通过pip命令行形式进行安装。</li>\n</ul>\n<p>OK，三个安装结束，键入下面命令，直接启动Browser的界面</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook</span><br></pre></td></tr></table></figure>\n<p>下面是标准编辑页面（竟然Hexo的asset_img可以用，好激动，但是asset_link确实不行，估计是因为marked里面escape的问题）</p>\n<img src=\"/2018/04/25/deep-learning-ai-2-1/jupyter.png\" title=\"[jupyter starter]\">\n<p>首界面很简单，就是系统文件夹。右上角有新建功能，可以新建一个可运行的python文件。在课程中，Andrew主要介绍了 np.dot(i,j) 在程序中对比for-loop，超过300倍的优势。笔者后来自己查了一下原因，看来主要还是回到了Python作为解释性语言本身的问题。为了对初学者友好，Python作为解释性语言牺牲了许多性能上的东西。而np.dot之所以much faster，主要原因是一句Python语言，对应的是用C写成numpy库，这个库会将输入进来的数据进行编译，形成编译后的语言进行调用。这种方法，远好于Python一个字符一个字符的进行读取，并根据语法分析器进行描述，占据了大量的时间。当然其他原因也有许多，比如借助编译可以使用CPU或者GPU的SIMD指令集（Simple instuction multiple data）进行并行计算，大大提升效率。根据文章将，numpy的效率可以是原生python的2万倍。而据说选用cpython会达到20万倍之多。具体原理可以参看知乎上的这篇文章：</p>\n<p><a href=\"https://www.zhihu.com/question/67652386\" target=\"_blank\" rel=\"noopener\">python的numpy向量化语句为什么会比for快？</a></p>\n<h3 id=\"建立神经网络的主要过程\"><a href=\"#建立神经网络的主要过程\" class=\"headerlink\" title=\"建立神经网络的主要过程\"></a>建立神经网络的主要过程</h3><h4 id=\"先来回顾一下基础算法：\"><a href=\"#先来回顾一下基础算法：\" class=\"headerlink\" title=\"先来回顾一下基础算法：\"></a>先来回顾一下基础算法：</h4><p>For one example $x^{(i)}$:</p>\n<script type=\"math/tex; mode=display\">z^{(i)} = w^T x^{(i)} + b \\tag{1}</script><script type=\"math/tex; mode=display\">\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}</script><script type=\"math/tex; mode=display\">\\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}</script><p>The cost is then computed by summing over all training examples:</p>\n<script type=\"math/tex; mode=display\">J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}</script><h4 id=\"建模过程\"><a href=\"#建模过程\" class=\"headerlink\" title=\"建模过程\"></a>建模过程</h4><ul>\n<li>定义模型结构（例如输入feature的数量）</li>\n<li>初始化模型参数</li>\n<li>升级参数（Gradient Descent）</li>\n</ul>\n<p>讲上述三个部分逐个建立并整合进一个叫做model()的 $function$ 里。几个简单的 $function$ 会包含的 $sigmoid$ , initialize_with_zeros() , propagate() </p>\n<p>特别说明，关于 propagate() 的算法回顾如下：<br>Forward Propagation:</p>\n<ul>\n<li>You get X</li>\n<li>You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, …, a^{(m-1)}, a^{(m)})$</li>\n<li>You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$</li>\n</ul>\n<p>Here are the two formulas you will be using: </p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}</script><script type=\"math/tex; mode=display\">\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}</script><p>下面的Code里面用到了一些“内积”、“外积”、“General Dot”的概念。在课程中有相关的联系材料。具体参见这个解释可能会更加实在</p>\n<p><a href=\"https://hk.saowen.com/a/c2cbbdb3dc43d41a717517faca384dc6228a9d2cbab31b59eca3f468c59e33b4\" target=\"_blank\" rel=\"noopener\">使用numpy进行行列式乘积的计算</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">m = X.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class=\"line\"><span class=\"comment\">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class=\"line\">A = sigmoid(np.dot(X.T, w) + b).reshape(<span class=\"number\">1</span>, <span class=\"number\">-1</span>) <span class=\"comment\"># compute activation</span></span><br><span class=\"line\">cost = - (<span class=\"number\">1</span>/m) * np.sum(Y*np.log(A) + (<span class=\"number\">1</span>-Y) * np.log(<span class=\"number\">1</span>-A))  <span class=\"comment\"># compute cost</span></span><br><span class=\"line\"><span class=\"comment\">### END CODE HERE ###</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class=\"line\"><span class=\"comment\">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class=\"line\">dw = <span class=\"number\">1</span>/m * np.dot(X, (A-Y).T)</span><br><span class=\"line\">db = <span class=\"number\">1</span>/m * np.sum(A-Y)</span><br><span class=\"line\"><span class=\"comment\">### END CODE HERE ###</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">assert</span>(dw.shape == w.shape)</span><br><span class=\"line\"><span class=\"keyword\">assert</span>(db.dtype == float)</span><br><span class=\"line\">cost = np.squeeze(cost)</span><br><span class=\"line\"><span class=\"keyword\">assert</span>(cost.shape == ())</span><br><span class=\"line\"></span><br><span class=\"line\">grads = &#123;<span class=\"string\">\"dw\"</span>: dw,</span><br><span class=\"line\">         <span class=\"string\">\"db\"</span>: db&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">return</span> grads, cost</span><br></pre></td></tr></table></figure>\n<p>总结一下，这里建立的初始模型包括<br>$sigmoid$, $initialize$, $propagate$</p>\n<h4 id=\"优化模型参数（Optimization）\"><a href=\"#优化模型参数（Optimization）\" class=\"headerlink\" title=\"优化模型参数（Optimization）\"></a>优化模型参数（Optimization）</h4><ul>\n<li>初始化参数</li>\n<li>计算cost function </li>\n<li>通过梯级下降的方式进行参数更新并计算w和b的结果()</li>\n</ul>\n<p>最近基本的梯级下降依据如下：</p>\n<script type=\"math/tex; mode=display\">\\theta = \\theta - \\alpha \\text{ } d\\theta \\tag{9}</script><p>where $\\alpha$ is the learning rate</p>\n<img src=\"/2018/04/25/deep-learning-ai-2-1/closing.jpg\" title=\"[gradient descent]\">\n<p>总结一下，这里建立的初始模型包括<br>$initialize$, $optimize$, $predict$</p>\n<h4 id=\"整合模型\"><a href=\"#整合模型\" class=\"headerlink\" title=\"整合模型\"></a>整合模型</h4><p>合并模型建立$model$，使用plot建立拟合线</p>\n"},{"title":"深度学习第二周课程（上）","date":"2018-04-25T13:16:40.000Z","mathjax":true,"_content":"\n这是一个比较数学化的一章。本章课程主要分为两部分（数学基础 + Python编程实践）。\n\n让我们先来看看第一部分。该部分内容重点是基于神经网络的一个基础数学模型，其中包括：\n\n*\t二元分类（Binary Classification）\n*\t逻辑回归（Logistic Regression）\n*\t逻辑回归中的Cost Function（Logistic Regression Cost Function）\n*\t梯度下降（Gradient Decent）\n*\t导数（Derivatives / Derivatives Examples）\n*\t计算图（Computation Graph）\n*\t计算图求导（Derivatives with a Computation Graph）\n*\t梯度下架求逻辑回顾（Logistic Regression Gradient Decent）\n*\t对m样本的梯度下降（Gradient Decent on m Examples）\n\n下面我们分部分进行一些描述：\n","source":"_posts/deep-learning-ai-2.md","raw":"title: 深度学习第二周课程（上）\ndate: 2018-04-25 21:16:40\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\nmathjax: true\n---\n\n这是一个比较数学化的一章。本章课程主要分为两部分（数学基础 + Python编程实践）。\n\n让我们先来看看第一部分。该部分内容重点是基于神经网络的一个基础数学模型，其中包括：\n\n*\t二元分类（Binary Classification）\n*\t逻辑回归（Logistic Regression）\n*\t逻辑回归中的Cost Function（Logistic Regression Cost Function）\n*\t梯度下降（Gradient Decent）\n*\t导数（Derivatives / Derivatives Examples）\n*\t计算图（Computation Graph）\n*\t计算图求导（Derivatives with a Computation Graph）\n*\t梯度下架求逻辑回顾（Logistic Regression Gradient Decent）\n*\t对m样本的梯度下降（Gradient Decent on m Examples）\n\n下面我们分部分进行一些描述：\n","slug":"deep-learning-ai-2","published":1,"updated":"2018-05-03T10:47:37.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoa0009rlfyblfg9521","content":"<p>这是一个比较数学化的一章。本章课程主要分为两部分（数学基础 + Python编程实践）。</p>\n<p>让我们先来看看第一部分。该部分内容重点是基于神经网络的一个基础数学模型，其中包括：</p>\n<ul>\n<li>二元分类（Binary Classification）</li>\n<li>逻辑回归（Logistic Regression）</li>\n<li>逻辑回归中的Cost Function（Logistic Regression Cost Function）</li>\n<li>梯度下降（Gradient Decent）</li>\n<li>导数（Derivatives / Derivatives Examples）</li>\n<li>计算图（Computation Graph）</li>\n<li>计算图求导（Derivatives with a Computation Graph）</li>\n<li>梯度下架求逻辑回顾（Logistic Regression Gradient Decent）</li>\n<li>对m样本的梯度下降（Gradient Decent on m Examples）</li>\n</ul>\n<p>下面我们分部分进行一些描述：</p>\n","site":{"data":{}},"excerpt":"","more":"<p>这是一个比较数学化的一章。本章课程主要分为两部分（数学基础 + Python编程实践）。</p>\n<p>让我们先来看看第一部分。该部分内容重点是基于神经网络的一个基础数学模型，其中包括：</p>\n<ul>\n<li>二元分类（Binary Classification）</li>\n<li>逻辑回归（Logistic Regression）</li>\n<li>逻辑回归中的Cost Function（Logistic Regression Cost Function）</li>\n<li>梯度下降（Gradient Decent）</li>\n<li>导数（Derivatives / Derivatives Examples）</li>\n<li>计算图（Computation Graph）</li>\n<li>计算图求导（Derivatives with a Computation Graph）</li>\n<li>梯度下架求逻辑回顾（Logistic Regression Gradient Decent）</li>\n<li>对m样本的梯度下降（Gradient Decent on m Examples）</li>\n</ul>\n<p>下面我们分部分进行一些描述：</p>\n"},{"title":"深度学习顺序模型第三周","date":"2018-09-13T08:07:05.000Z","mathjax":true,"_content":"\n来到最后一周了，本周分为10课，两个大部分\n\n# Various sequence to sequence architectures\n\n这个部分是本周的重点，8个课都是围绕着展开的。这种模型被广泛应用在了文字翻译和语义识别领域。让我们来看看都讲述了些什么。\n\n##\tBasic Models\n\n```\nx: Jane visite I'Afrique en septembre\ny: Jane is visiting Africa in September\n```\n\n这里用了两套模型进行了组合，\n\n+\t作为encoding network：$x^{<1>}, x^{<2>}, x^{<3>}, x^{<4>} ... x^{ < T_x > }$，可以是一个RNN模型（GRU或者LSTM）\n+\t作为decoding network：$y^{<1>}, y^{<2>}, y^{<3>}, y^{<4>} ... y^{ < T_y > }$，同时后面继续跟随了\n\n这样一种算法，同样对于Image Captioning这样的应用有效。应用是我们的输入是一张图片，应用的输出会把图片转化成一个语义\n\n$$ \\begin {matrix}\n     y^{<1>} & y^{<2>} & y^{<3>} & y^{<4>} & y^{<5>} & y^{<6>} \\\\\n     A & Cat & sitting & on & a & chair \\\\\n\\end {matrix} $$\n\n在这样一个应用场景下，我们可以用CNN模型作为encoding network，把这个的输出，feed到RNN的顺序模型里面，进行训练。这个的输出，就是一个对Image的描述了。\n\n<PlaceHolder for image>\n\n## Picking the most likely sentence\n\n### Machine translation as building a conditional language model\n\n一个正常的language model，是一个从$a^{0}$ 到 $x^{<1>}$ 再到 $\\hat y^{<1>} $ 再进行混合的演进的顺序模型。同时在这个模型里，会有 $ P(y^{<1>}, ..., y^{< T_x >})$ 的数值，来代表每一个过程中产生的y的可能性。这个一般是用来生成普通的句子\n\nMachine Translation Model对应的是两个部分，encoding model和decoding model。可以看到decoding model和language model是很相似的。做为encoding model的输出，就是decoding model的输入 $ a_{<0>}$\n\n这也是为什么我们把Machine translation model也叫做conditional language model。在形象一点的说法是，一段被翻译后的句子（比如Jane is visiting Africa）本身，是由前一个被翻译前句子的作为条件形成的。这个说法还挺有意思的，让我认清了翻译这件事情的本质。的确是这样。\n\n### Finding the most likely translation （如何找到最好翻译）\n\n```\nx: Jane visite I'Afrique en septembre\n```\n\n于是乎，我们就有了 $ P(y^{<1>}, ..., y^{< T_x >} |x) $ 这样一个可能性的表示。这里的x就是上面说的那段法语句子。所以整个翻译模型的结果就是寻找P的最大值，找到最有可能的translation结果\n\n### Why not Greedy Search\n\nGreedy Search是一种算法，大概的意思就是，因为有了x(翻译前句子的输入)，我们接下来寻找每一个 $ P(y^{<1>} |x)$, 然后是$ P(y^{<2>} |x y^{<1>})$，一次类推，就是每次都寻找那个最合适的。而不是 $ P(y|x)$。我觉得其实这里不用多解释，这就是一个局部优化和整体优化的问题。如果计算量允许，肯定尽量采用整体优化，这样才能取得好结果。\n\n既然Greedy Search不好用，那么如何从$ 10000^{10} $ (假设vocabulary有10000个，句子有10个单次) 这么多种可能性里进行选取呢？当然第一方法，就是使用x作为输入，寻找较大可能性。那么然后呢？还有什么方法？于是这里就用到了更进一步的Search Algorithm。这就又回到了这一课的主题，就是寻找最好的翻译，而不是随机寻找翻译结果（这里作者列举的例子是going和visiting，前者更加常见，但是后者更加适合Jane的语境）\n\n## Beam Search Algorith\n\n关于Beam Search算法，第一步是吧10000 words的vacabulary放进array里面。\n\n$$ \\begin {pmatrix}\n     a\\\\\n     \\vdots\\\\\n     in\\\\\n     \\vdots\\\\\n     jane\\\\\n     \\vdots\\\\\n     september\\\\\n     \\vdots\\\\\n     zulu\\\\\n\\end {pmatrix} $$\n\n所以这里的Step1，第一步就是 $P(y^{<1>}|x)$ 对应的单词。这里插入一个概念B，叫做Beam width, 例如B=3.这里给出的Beam的数值，就是为了系统记录下B个最有可能的单词。看起来，其实意思就是说，Greedy Search不是每次只找一个最好的么，这个Beam Search是根据Beam的数值，找B个最好的。\n\n接下来进行Step2第二步。就很简单了，其实就是因为前一个假设是in了，那想标准的language model一样，会有一个$P(y^{<2>}|x”in“)$这样的表达式，来表示下一个最大可能的单词。因此下面这个公式还挺重要的，就是：\n\n$$ P(y^{<1>}, y^{<2>} |x) = P(y^{<1>}|x) \\times P(y^{<2>}|x”in“) $$\n\nAgain, 这里因为Beam Width为3，所以我们还是，选择最大可能的三个，不过这回就是三个pair了 $P(y^{<1>}, y^{<2>} |x) $. 如此循环进行下一步的运算。你看，这里B=1的话，那就真的是Greedy Search了\n\n## Refinements to Beam Search\n\n### Length normalization\n\n这个就是Beam Search里面的一个部分。他的用法是这样的。\n\n如同前文讲的，Beam Search的核心是寻找到最大的B个可能性，并进行模型推演。所以\n\n$$ P(y|x) = avg max \\prod_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} ) $$\n\n但是这里有一个问题，就是若干个百分之几十的数字相乘，会让这个结果趋近于非常小，不方便计算。于是这里引入了log，作为计算函数。这里P的数值越大，\n\n$$ \\log^{P(y|x)} = avg max \\sum_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} ) $$\n\n$$ \\frac{1}{Ty^\\alpha} avg max \\sum_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} ) $$\n\n$ \\alpha $ 一般是从0到1的一个中间值，比如0.7。这个数用来对于模型进行校正和调整是比较管用的\n\n## Error Analysis on Beam Search\n\n关于整个翻译过程实际上是用到了Beam Search算法和两个RNN模型，对于结果而言，如何评价到底是哪里不好导致的问题。Error Analysis提供了一些方法\n\n{% asset_img error_analysis.png [Error Analysis] %}\n\nCase1: $ P(y_{*}|x) > P( \\hat y|x) $\n在这种情况下，因为Beam Search的作用是用来进行选取$\\hat y$，那么既然命名$P(y_{*}|x)$会更好，但是Beam Search却选的不对。说明Beam Search有问题\n\nCase2：$ P(y_{*}|x) < P( \\hat y|x) $\nRNN负责预测 $\\hat y$ 而他错误的将$ P( \\hat y|x) $ 生成了一个更大的P数值，这说明生成有问题。是RNN需要被调整\n\n## Attention Module intuition\n\n对比于简单使用输入RNN和输出RNN，Attention Module更好的处理翻译过程中的长句子问题。我们注意到在日常翻译中，MT在处理10个左右单词的句子时候，表现效果会比较好。但是如果再长，效果就会下降的非常明显。因此我们需要更好的模型来改进这一点。\n\n首先分析一下原因，主要是单词的记忆导致的。事实上，人类在翻译长句子的时候，采用的是一部分一部分的翻译方式，并不会把他们都记录下来，因为记忆的原因。这个事情同样出现在机器上面，因为计算资源有限，所以我们也无法给计算机留出那么大量的选项进行综合运算。\n\n+\tAttention Weight，标注了对于一个生成的词来讲，哪些input word是应该被关注的，以及关注Weight是多少\n\n+\t$ \\alpha^{ <1,2> } $ 这个表示第一个翻译后词汇，需要对翻译前句子中的第二个词的关注度有多高\n\n## Atention Module\n\n$$ \\sum_{t} \\alpha^{ <1, t> } = 1 $$ \n$$ C^{ <1> } （context）=  \\sum_{t} \\alpha^{ <1, t> } a^{ <t> }$$\n\n$$ bug_{a} =  \\alpha * sprint_{a} + \\beta * sprint_{a-1} + bias $$\n\n\n# 领域应用\n\n## 语音识别 （Speech Recorgnition）\n\nAudio Clip x，translate to ，Transcript y ","source":"_posts/deep-learning-sequence-models-w3.md","raw":"title: 深度学习顺序模型第三周\ndate: 2018-09-13 16:07:05\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\nmathjax: true\n---\n\n来到最后一周了，本周分为10课，两个大部分\n\n# Various sequence to sequence architectures\n\n这个部分是本周的重点，8个课都是围绕着展开的。这种模型被广泛应用在了文字翻译和语义识别领域。让我们来看看都讲述了些什么。\n\n##\tBasic Models\n\n```\nx: Jane visite I'Afrique en septembre\ny: Jane is visiting Africa in September\n```\n\n这里用了两套模型进行了组合，\n\n+\t作为encoding network：$x^{<1>}, x^{<2>}, x^{<3>}, x^{<4>} ... x^{ < T_x > }$，可以是一个RNN模型（GRU或者LSTM）\n+\t作为decoding network：$y^{<1>}, y^{<2>}, y^{<3>}, y^{<4>} ... y^{ < T_y > }$，同时后面继续跟随了\n\n这样一种算法，同样对于Image Captioning这样的应用有效。应用是我们的输入是一张图片，应用的输出会把图片转化成一个语义\n\n$$ \\begin {matrix}\n     y^{<1>} & y^{<2>} & y^{<3>} & y^{<4>} & y^{<5>} & y^{<6>} \\\\\n     A & Cat & sitting & on & a & chair \\\\\n\\end {matrix} $$\n\n在这样一个应用场景下，我们可以用CNN模型作为encoding network，把这个的输出，feed到RNN的顺序模型里面，进行训练。这个的输出，就是一个对Image的描述了。\n\n<PlaceHolder for image>\n\n## Picking the most likely sentence\n\n### Machine translation as building a conditional language model\n\n一个正常的language model，是一个从$a^{0}$ 到 $x^{<1>}$ 再到 $\\hat y^{<1>} $ 再进行混合的演进的顺序模型。同时在这个模型里，会有 $ P(y^{<1>}, ..., y^{< T_x >})$ 的数值，来代表每一个过程中产生的y的可能性。这个一般是用来生成普通的句子\n\nMachine Translation Model对应的是两个部分，encoding model和decoding model。可以看到decoding model和language model是很相似的。做为encoding model的输出，就是decoding model的输入 $ a_{<0>}$\n\n这也是为什么我们把Machine translation model也叫做conditional language model。在形象一点的说法是，一段被翻译后的句子（比如Jane is visiting Africa）本身，是由前一个被翻译前句子的作为条件形成的。这个说法还挺有意思的，让我认清了翻译这件事情的本质。的确是这样。\n\n### Finding the most likely translation （如何找到最好翻译）\n\n```\nx: Jane visite I'Afrique en septembre\n```\n\n于是乎，我们就有了 $ P(y^{<1>}, ..., y^{< T_x >} |x) $ 这样一个可能性的表示。这里的x就是上面说的那段法语句子。所以整个翻译模型的结果就是寻找P的最大值，找到最有可能的translation结果\n\n### Why not Greedy Search\n\nGreedy Search是一种算法，大概的意思就是，因为有了x(翻译前句子的输入)，我们接下来寻找每一个 $ P(y^{<1>} |x)$, 然后是$ P(y^{<2>} |x y^{<1>})$，一次类推，就是每次都寻找那个最合适的。而不是 $ P(y|x)$。我觉得其实这里不用多解释，这就是一个局部优化和整体优化的问题。如果计算量允许，肯定尽量采用整体优化，这样才能取得好结果。\n\n既然Greedy Search不好用，那么如何从$ 10000^{10} $ (假设vocabulary有10000个，句子有10个单次) 这么多种可能性里进行选取呢？当然第一方法，就是使用x作为输入，寻找较大可能性。那么然后呢？还有什么方法？于是这里就用到了更进一步的Search Algorithm。这就又回到了这一课的主题，就是寻找最好的翻译，而不是随机寻找翻译结果（这里作者列举的例子是going和visiting，前者更加常见，但是后者更加适合Jane的语境）\n\n## Beam Search Algorith\n\n关于Beam Search算法，第一步是吧10000 words的vacabulary放进array里面。\n\n$$ \\begin {pmatrix}\n     a\\\\\n     \\vdots\\\\\n     in\\\\\n     \\vdots\\\\\n     jane\\\\\n     \\vdots\\\\\n     september\\\\\n     \\vdots\\\\\n     zulu\\\\\n\\end {pmatrix} $$\n\n所以这里的Step1，第一步就是 $P(y^{<1>}|x)$ 对应的单词。这里插入一个概念B，叫做Beam width, 例如B=3.这里给出的Beam的数值，就是为了系统记录下B个最有可能的单词。看起来，其实意思就是说，Greedy Search不是每次只找一个最好的么，这个Beam Search是根据Beam的数值，找B个最好的。\n\n接下来进行Step2第二步。就很简单了，其实就是因为前一个假设是in了，那想标准的language model一样，会有一个$P(y^{<2>}|x”in“)$这样的表达式，来表示下一个最大可能的单词。因此下面这个公式还挺重要的，就是：\n\n$$ P(y^{<1>}, y^{<2>} |x) = P(y^{<1>}|x) \\times P(y^{<2>}|x”in“) $$\n\nAgain, 这里因为Beam Width为3，所以我们还是，选择最大可能的三个，不过这回就是三个pair了 $P(y^{<1>}, y^{<2>} |x) $. 如此循环进行下一步的运算。你看，这里B=1的话，那就真的是Greedy Search了\n\n## Refinements to Beam Search\n\n### Length normalization\n\n这个就是Beam Search里面的一个部分。他的用法是这样的。\n\n如同前文讲的，Beam Search的核心是寻找到最大的B个可能性，并进行模型推演。所以\n\n$$ P(y|x) = avg max \\prod_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} ) $$\n\n但是这里有一个问题，就是若干个百分之几十的数字相乘，会让这个结果趋近于非常小，不方便计算。于是这里引入了log，作为计算函数。这里P的数值越大，\n\n$$ \\log^{P(y|x)} = avg max \\sum_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} ) $$\n\n$$ \\frac{1}{Ty^\\alpha} avg max \\sum_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} ) $$\n\n$ \\alpha $ 一般是从0到1的一个中间值，比如0.7。这个数用来对于模型进行校正和调整是比较管用的\n\n## Error Analysis on Beam Search\n\n关于整个翻译过程实际上是用到了Beam Search算法和两个RNN模型，对于结果而言，如何评价到底是哪里不好导致的问题。Error Analysis提供了一些方法\n\n{% asset_img error_analysis.png [Error Analysis] %}\n\nCase1: $ P(y_{*}|x) > P( \\hat y|x) $\n在这种情况下，因为Beam Search的作用是用来进行选取$\\hat y$，那么既然命名$P(y_{*}|x)$会更好，但是Beam Search却选的不对。说明Beam Search有问题\n\nCase2：$ P(y_{*}|x) < P( \\hat y|x) $\nRNN负责预测 $\\hat y$ 而他错误的将$ P( \\hat y|x) $ 生成了一个更大的P数值，这说明生成有问题。是RNN需要被调整\n\n## Attention Module intuition\n\n对比于简单使用输入RNN和输出RNN，Attention Module更好的处理翻译过程中的长句子问题。我们注意到在日常翻译中，MT在处理10个左右单词的句子时候，表现效果会比较好。但是如果再长，效果就会下降的非常明显。因此我们需要更好的模型来改进这一点。\n\n首先分析一下原因，主要是单词的记忆导致的。事实上，人类在翻译长句子的时候，采用的是一部分一部分的翻译方式，并不会把他们都记录下来，因为记忆的原因。这个事情同样出现在机器上面，因为计算资源有限，所以我们也无法给计算机留出那么大量的选项进行综合运算。\n\n+\tAttention Weight，标注了对于一个生成的词来讲，哪些input word是应该被关注的，以及关注Weight是多少\n\n+\t$ \\alpha^{ <1,2> } $ 这个表示第一个翻译后词汇，需要对翻译前句子中的第二个词的关注度有多高\n\n## Atention Module\n\n$$ \\sum_{t} \\alpha^{ <1, t> } = 1 $$ \n$$ C^{ <1> } （context）=  \\sum_{t} \\alpha^{ <1, t> } a^{ <t> }$$\n\n$$ bug_{a} =  \\alpha * sprint_{a} + \\beta * sprint_{a-1} + bias $$\n\n\n# 领域应用\n\n## 语音识别 （Speech Recorgnition）\n\nAudio Clip x，translate to ，Transcript y ","slug":"deep-learning-sequence-models-w3","published":1,"updated":"2018-11-09T02:39:11.071Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoc000arlfyh5p4aan2","content":"<p>来到最后一周了，本周分为10课，两个大部分</p>\n<h1 id=\"Various-sequence-to-sequence-architectures\"><a href=\"#Various-sequence-to-sequence-architectures\" class=\"headerlink\" title=\"Various sequence to sequence architectures\"></a>Various sequence to sequence architectures</h1><p>这个部分是本周的重点，8个课都是围绕着展开的。这种模型被广泛应用在了文字翻译和语义识别领域。让我们来看看都讲述了些什么。</p>\n<h2 id=\"Basic-Models\"><a href=\"#Basic-Models\" class=\"headerlink\" title=\"Basic Models\"></a>Basic Models</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x: Jane visite I&apos;Afrique en septembre</span><br><span class=\"line\">y: Jane is visiting Africa in September</span><br></pre></td></tr></table></figure>\n<p>这里用了两套模型进行了组合，</p>\n<ul>\n<li>作为encoding network：$x^{<1>}, x^{<2>}, x^{<3>}, x^{<4>} … x^{ &lt; T_x &gt; }$，可以是一个RNN模型（GRU或者LSTM）</4></3></2></1></li>\n<li>作为decoding network：$y^{<1>}, y^{<2>}, y^{<3>}, y^{<4>} … y^{ &lt; T_y &gt; }$，同时后面继续跟随了</4></3></2></1></li>\n</ul>\n<p>这样一种算法，同样对于Image Captioning这样的应用有效。应用是我们的输入是一张图片，应用的输出会把图片转化成一个语义</p>\n<script type=\"math/tex; mode=display\">\\begin {matrix}\n     y^{<1>} & y^{<2>} & y^{<3>} & y^{<4>} & y^{<5>} & y^{<6>} \\\\\n     A & Cat & sitting & on & a & chair \\\\\n\\end {matrix}</script><p>在这样一个应用场景下，我们可以用CNN模型作为encoding network，把这个的输出，feed到RNN的顺序模型里面，进行训练。这个的输出，就是一个对Image的描述了。</p>\n<placeholder for=\"\" image=\"\">\n\n<h2 id=\"Picking-the-most-likely-sentence\"><a href=\"#Picking-the-most-likely-sentence\" class=\"headerlink\" title=\"Picking the most likely sentence\"></a>Picking the most likely sentence</h2><h3 id=\"Machine-translation-as-building-a-conditional-language-model\"><a href=\"#Machine-translation-as-building-a-conditional-language-model\" class=\"headerlink\" title=\"Machine translation as building a conditional language model\"></a>Machine translation as building a conditional language model</h3><p>一个正常的language model，是一个从$a^{0}$ 到 $x^{<1>}$ 再到 $\\hat y^{<1>} $ 再进行混合的演进的顺序模型。同时在这个模型里，会有 $ P(y^{<1>}, …, y^{&lt; T_x &gt;})$ 的数值，来代表每一个过程中产生的y的可能性。这个一般是用来生成普通的句子</1></1></1></p>\n<p>Machine Translation Model对应的是两个部分，encoding model和decoding model。可以看到decoding model和language model是很相似的。做为encoding model的输出，就是decoding model的输入 $ a_{<0>}$</0></p>\n<p>这也是为什么我们把Machine translation model也叫做conditional language model。在形象一点的说法是，一段被翻译后的句子（比如Jane is visiting Africa）本身，是由前一个被翻译前句子的作为条件形成的。这个说法还挺有意思的，让我认清了翻译这件事情的本质。的确是这样。</p>\n<h3 id=\"Finding-the-most-likely-translation-（如何找到最好翻译）\"><a href=\"#Finding-the-most-likely-translation-（如何找到最好翻译）\" class=\"headerlink\" title=\"Finding the most likely translation （如何找到最好翻译）\"></a>Finding the most likely translation （如何找到最好翻译）</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x: Jane visite I&apos;Afrique en septembre</span><br></pre></td></tr></table></figure>\n<p>于是乎，我们就有了 $ P(y^{<1>}, …, y^{&lt; T_x &gt;} |x) $ 这样一个可能性的表示。这里的x就是上面说的那段法语句子。所以整个翻译模型的结果就是寻找P的最大值，找到最有可能的translation结果</1></p>\n<h3 id=\"Why-not-Greedy-Search\"><a href=\"#Why-not-Greedy-Search\" class=\"headerlink\" title=\"Why not Greedy Search\"></a>Why not Greedy Search</h3><p>Greedy Search是一种算法，大概的意思就是，因为有了x(翻译前句子的输入)，我们接下来寻找每一个 $ P(y^{<1>} |x)$, 然后是$ P(y^{<2>} |x y^{<1>})$，一次类推，就是每次都寻找那个最合适的。而不是 $ P(y|x)$。我觉得其实这里不用多解释，这就是一个局部优化和整体优化的问题。如果计算量允许，肯定尽量采用整体优化，这样才能取得好结果。</1></2></1></p>\n<p>既然Greedy Search不好用，那么如何从$ 10000^{10} $ (假设vocabulary有10000个，句子有10个单次) 这么多种可能性里进行选取呢？当然第一方法，就是使用x作为输入，寻找较大可能性。那么然后呢？还有什么方法？于是这里就用到了更进一步的Search Algorithm。这就又回到了这一课的主题，就是寻找最好的翻译，而不是随机寻找翻译结果（这里作者列举的例子是going和visiting，前者更加常见，但是后者更加适合Jane的语境）</p>\n<h2 id=\"Beam-Search-Algorith\"><a href=\"#Beam-Search-Algorith\" class=\"headerlink\" title=\"Beam Search Algorith\"></a>Beam Search Algorith</h2><p>关于Beam Search算法，第一步是吧10000 words的vacabulary放进array里面。</p>\n<script type=\"math/tex; mode=display\">\\begin {pmatrix}\n     a\\\\\n     \\vdots\\\\\n     in\\\\\n     \\vdots\\\\\n     jane\\\\\n     \\vdots\\\\\n     september\\\\\n     \\vdots\\\\\n     zulu\\\\\n\\end {pmatrix}</script><p>所以这里的Step1，第一步就是 $P(y^{<1>}|x)$ 对应的单词。这里插入一个概念B，叫做Beam width, 例如B=3.这里给出的Beam的数值，就是为了系统记录下B个最有可能的单词。看起来，其实意思就是说，Greedy Search不是每次只找一个最好的么，这个Beam Search是根据Beam的数值，找B个最好的。</1></p>\n<p>接下来进行Step2第二步。就很简单了，其实就是因为前一个假设是in了，那想标准的language model一样，会有一个$P(y^{<2>}|x”in“)$这样的表达式，来表示下一个最大可能的单词。因此下面这个公式还挺重要的，就是：</2></p>\n<script type=\"math/tex; mode=display\">P(y^{<1>}, y^{<2>} |x) = P(y^{<1>}|x) \\times P(y^{<2>}|x”in“)</script><p>Again, 这里因为Beam Width为3，所以我们还是，选择最大可能的三个，不过这回就是三个pair了 $P(y^{<1>}, y^{<2>} |x) $. 如此循环进行下一步的运算。你看，这里B=1的话，那就真的是Greedy Search了</2></1></p>\n<h2 id=\"Refinements-to-Beam-Search\"><a href=\"#Refinements-to-Beam-Search\" class=\"headerlink\" title=\"Refinements to Beam Search\"></a>Refinements to Beam Search</h2><h3 id=\"Length-normalization\"><a href=\"#Length-normalization\" class=\"headerlink\" title=\"Length normalization\"></a>Length normalization</h3><p>这个就是Beam Search里面的一个部分。他的用法是这样的。</p>\n<p>如同前文讲的，Beam Search的核心是寻找到最大的B个可能性，并进行模型推演。所以</p>\n<script type=\"math/tex; mode=display\">P(y|x) = avg max \\prod_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} )</script><p>但是这里有一个问题，就是若干个百分之几十的数字相乘，会让这个结果趋近于非常小，不方便计算。于是这里引入了log，作为计算函数。这里P的数值越大，</p>\n<script type=\"math/tex; mode=display\">\\log^{P(y|x)} = avg max \\sum_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} )</script><script type=\"math/tex; mode=display\">\\frac{1}{Ty^\\alpha} avg max \\sum_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} )</script><p>$ \\alpha $ 一般是从0到1的一个中间值，比如0.7。这个数用来对于模型进行校正和调整是比较管用的</p>\n<h2 id=\"Error-Analysis-on-Beam-Search\"><a href=\"#Error-Analysis-on-Beam-Search\" class=\"headerlink\" title=\"Error Analysis on Beam Search\"></a>Error Analysis on Beam Search</h2><p>关于整个翻译过程实际上是用到了Beam Search算法和两个RNN模型，对于结果而言，如何评价到底是哪里不好导致的问题。Error Analysis提供了一些方法</p>\n<img src=\"/2018/09/13/deep-learning-sequence-models-w3/error_analysis.png\" title=\"[Error Analysis]\">\n<p>Case1: $ P(y_{<em>}|x) &gt; P( \\hat y|x) $<br>在这种情况下，因为Beam Search的作用是用来进行选取$\\hat y$，那么既然命名$P(y_{</em>}|x)$会更好，但是Beam Search却选的不对。说明Beam Search有问题</p>\n<p>Case2：$ P(y_{*}|x) &lt; P( \\hat y|x) $<br>RNN负责预测 $\\hat y$ 而他错误的将$ P( \\hat y|x) $ 生成了一个更大的P数值，这说明生成有问题。是RNN需要被调整</p>\n<h2 id=\"Attention-Module-intuition\"><a href=\"#Attention-Module-intuition\" class=\"headerlink\" title=\"Attention Module intuition\"></a>Attention Module intuition</h2><p>对比于简单使用输入RNN和输出RNN，Attention Module更好的处理翻译过程中的长句子问题。我们注意到在日常翻译中，MT在处理10个左右单词的句子时候，表现效果会比较好。但是如果再长，效果就会下降的非常明显。因此我们需要更好的模型来改进这一点。</p>\n<p>首先分析一下原因，主要是单词的记忆导致的。事实上，人类在翻译长句子的时候，采用的是一部分一部分的翻译方式，并不会把他们都记录下来，因为记忆的原因。这个事情同样出现在机器上面，因为计算资源有限，所以我们也无法给计算机留出那么大量的选项进行综合运算。</p>\n<ul>\n<li><p>Attention Weight，标注了对于一个生成的词来讲，哪些input word是应该被关注的，以及关注Weight是多少</p>\n</li>\n<li><p>$ \\alpha^{ <1,2> } $ 这个表示第一个翻译后词汇，需要对翻译前句子中的第二个词的关注度有多高</1,2></p>\n</li>\n</ul>\n<h2 id=\"Atention-Module\"><a href=\"#Atention-Module\" class=\"headerlink\" title=\"Atention Module\"></a>Atention Module</h2><script type=\"math/tex; mode=display\">\\sum_{t} \\alpha^{ <1, t> } = 1</script><script type=\"math/tex; mode=display\">C^{ <1> } （context）=  \\sum_{t} \\alpha^{ <1, t> } a^{ <t> }</script><script type=\"math/tex; mode=display\">bug_{a} =  \\alpha * sprint_{a} + \\beta * sprint_{a-1} + bias</script><h1 id=\"领域应用\"><a href=\"#领域应用\" class=\"headerlink\" title=\"领域应用\"></a>领域应用</h1><h2 id=\"语音识别-（Speech-Recorgnition）\"><a href=\"#语音识别-（Speech-Recorgnition）\" class=\"headerlink\" title=\"语音识别 （Speech Recorgnition）\"></a>语音识别 （Speech Recorgnition）</h2><p>Audio Clip x，translate to ，Transcript y </p>\n</placeholder>","site":{"data":{}},"excerpt":"","more":"<p>来到最后一周了，本周分为10课，两个大部分</p>\n<h1 id=\"Various-sequence-to-sequence-architectures\"><a href=\"#Various-sequence-to-sequence-architectures\" class=\"headerlink\" title=\"Various sequence to sequence architectures\"></a>Various sequence to sequence architectures</h1><p>这个部分是本周的重点，8个课都是围绕着展开的。这种模型被广泛应用在了文字翻译和语义识别领域。让我们来看看都讲述了些什么。</p>\n<h2 id=\"Basic-Models\"><a href=\"#Basic-Models\" class=\"headerlink\" title=\"Basic Models\"></a>Basic Models</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x: Jane visite I&apos;Afrique en septembre</span><br><span class=\"line\">y: Jane is visiting Africa in September</span><br></pre></td></tr></table></figure>\n<p>这里用了两套模型进行了组合，</p>\n<ul>\n<li>作为encoding network：$x^{<1>}, x^{<2>}, x^{<3>}, x^{<4>} … x^{ &lt; T_x &gt; }$，可以是一个RNN模型（GRU或者LSTM）</4></3></2></1></li>\n<li>作为decoding network：$y^{<1>}, y^{<2>}, y^{<3>}, y^{<4>} … y^{ &lt; T_y &gt; }$，同时后面继续跟随了</4></3></2></1></li>\n</ul>\n<p>这样一种算法，同样对于Image Captioning这样的应用有效。应用是我们的输入是一张图片，应用的输出会把图片转化成一个语义</p>\n<script type=\"math/tex; mode=display\">\\begin {matrix}\n     y^{<1>} & y^{<2>} & y^{<3>} & y^{<4>} & y^{<5>} & y^{<6>} \\\\\n     A & Cat & sitting & on & a & chair \\\\\n\\end {matrix}</script><p>在这样一个应用场景下，我们可以用CNN模型作为encoding network，把这个的输出，feed到RNN的顺序模型里面，进行训练。这个的输出，就是一个对Image的描述了。</p>\n<placeholder for=\"\" image=\"\">\n\n<h2 id=\"Picking-the-most-likely-sentence\"><a href=\"#Picking-the-most-likely-sentence\" class=\"headerlink\" title=\"Picking the most likely sentence\"></a>Picking the most likely sentence</h2><h3 id=\"Machine-translation-as-building-a-conditional-language-model\"><a href=\"#Machine-translation-as-building-a-conditional-language-model\" class=\"headerlink\" title=\"Machine translation as building a conditional language model\"></a>Machine translation as building a conditional language model</h3><p>一个正常的language model，是一个从$a^{0}$ 到 $x^{<1>}$ 再到 $\\hat y^{<1>} $ 再进行混合的演进的顺序模型。同时在这个模型里，会有 $ P(y^{<1>}, …, y^{&lt; T_x &gt;})$ 的数值，来代表每一个过程中产生的y的可能性。这个一般是用来生成普通的句子</1></1></1></p>\n<p>Machine Translation Model对应的是两个部分，encoding model和decoding model。可以看到decoding model和language model是很相似的。做为encoding model的输出，就是decoding model的输入 $ a_{<0>}$</0></p>\n<p>这也是为什么我们把Machine translation model也叫做conditional language model。在形象一点的说法是，一段被翻译后的句子（比如Jane is visiting Africa）本身，是由前一个被翻译前句子的作为条件形成的。这个说法还挺有意思的，让我认清了翻译这件事情的本质。的确是这样。</p>\n<h3 id=\"Finding-the-most-likely-translation-（如何找到最好翻译）\"><a href=\"#Finding-the-most-likely-translation-（如何找到最好翻译）\" class=\"headerlink\" title=\"Finding the most likely translation （如何找到最好翻译）\"></a>Finding the most likely translation （如何找到最好翻译）</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x: Jane visite I&apos;Afrique en septembre</span><br></pre></td></tr></table></figure>\n<p>于是乎，我们就有了 $ P(y^{<1>}, …, y^{&lt; T_x &gt;} |x) $ 这样一个可能性的表示。这里的x就是上面说的那段法语句子。所以整个翻译模型的结果就是寻找P的最大值，找到最有可能的translation结果</1></p>\n<h3 id=\"Why-not-Greedy-Search\"><a href=\"#Why-not-Greedy-Search\" class=\"headerlink\" title=\"Why not Greedy Search\"></a>Why not Greedy Search</h3><p>Greedy Search是一种算法，大概的意思就是，因为有了x(翻译前句子的输入)，我们接下来寻找每一个 $ P(y^{<1>} |x)$, 然后是$ P(y^{<2>} |x y^{<1>})$，一次类推，就是每次都寻找那个最合适的。而不是 $ P(y|x)$。我觉得其实这里不用多解释，这就是一个局部优化和整体优化的问题。如果计算量允许，肯定尽量采用整体优化，这样才能取得好结果。</1></2></1></p>\n<p>既然Greedy Search不好用，那么如何从$ 10000^{10} $ (假设vocabulary有10000个，句子有10个单次) 这么多种可能性里进行选取呢？当然第一方法，就是使用x作为输入，寻找较大可能性。那么然后呢？还有什么方法？于是这里就用到了更进一步的Search Algorithm。这就又回到了这一课的主题，就是寻找最好的翻译，而不是随机寻找翻译结果（这里作者列举的例子是going和visiting，前者更加常见，但是后者更加适合Jane的语境）</p>\n<h2 id=\"Beam-Search-Algorith\"><a href=\"#Beam-Search-Algorith\" class=\"headerlink\" title=\"Beam Search Algorith\"></a>Beam Search Algorith</h2><p>关于Beam Search算法，第一步是吧10000 words的vacabulary放进array里面。</p>\n<script type=\"math/tex; mode=display\">\\begin {pmatrix}\n     a\\\\\n     \\vdots\\\\\n     in\\\\\n     \\vdots\\\\\n     jane\\\\\n     \\vdots\\\\\n     september\\\\\n     \\vdots\\\\\n     zulu\\\\\n\\end {pmatrix}</script><p>所以这里的Step1，第一步就是 $P(y^{<1>}|x)$ 对应的单词。这里插入一个概念B，叫做Beam width, 例如B=3.这里给出的Beam的数值，就是为了系统记录下B个最有可能的单词。看起来，其实意思就是说，Greedy Search不是每次只找一个最好的么，这个Beam Search是根据Beam的数值，找B个最好的。</1></p>\n<p>接下来进行Step2第二步。就很简单了，其实就是因为前一个假设是in了，那想标准的language model一样，会有一个$P(y^{<2>}|x”in“)$这样的表达式，来表示下一个最大可能的单词。因此下面这个公式还挺重要的，就是：</2></p>\n<script type=\"math/tex; mode=display\">P(y^{<1>}, y^{<2>} |x) = P(y^{<1>}|x) \\times P(y^{<2>}|x”in“)</script><p>Again, 这里因为Beam Width为3，所以我们还是，选择最大可能的三个，不过这回就是三个pair了 $P(y^{<1>}, y^{<2>} |x) $. 如此循环进行下一步的运算。你看，这里B=1的话，那就真的是Greedy Search了</2></1></p>\n<h2 id=\"Refinements-to-Beam-Search\"><a href=\"#Refinements-to-Beam-Search\" class=\"headerlink\" title=\"Refinements to Beam Search\"></a>Refinements to Beam Search</h2><h3 id=\"Length-normalization\"><a href=\"#Length-normalization\" class=\"headerlink\" title=\"Length normalization\"></a>Length normalization</h3><p>这个就是Beam Search里面的一个部分。他的用法是这样的。</p>\n<p>如同前文讲的，Beam Search的核心是寻找到最大的B个可能性，并进行模型推演。所以</p>\n<script type=\"math/tex; mode=display\">P(y|x) = avg max \\prod_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} )</script><p>但是这里有一个问题，就是若干个百分之几十的数字相乘，会让这个结果趋近于非常小，不方便计算。于是这里引入了log，作为计算函数。这里P的数值越大，</p>\n<script type=\"math/tex; mode=display\">\\log^{P(y|x)} = avg max \\sum_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} )</script><script type=\"math/tex; mode=display\">\\frac{1}{Ty^\\alpha} avg max \\sum_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} )</script><p>$ \\alpha $ 一般是从0到1的一个中间值，比如0.7。这个数用来对于模型进行校正和调整是比较管用的</p>\n<h2 id=\"Error-Analysis-on-Beam-Search\"><a href=\"#Error-Analysis-on-Beam-Search\" class=\"headerlink\" title=\"Error Analysis on Beam Search\"></a>Error Analysis on Beam Search</h2><p>关于整个翻译过程实际上是用到了Beam Search算法和两个RNN模型，对于结果而言，如何评价到底是哪里不好导致的问题。Error Analysis提供了一些方法</p>\n<img src=\"/2018/09/13/deep-learning-sequence-models-w3/error_analysis.png\" title=\"[Error Analysis]\">\n<p>Case1: $ P(y_{<em>}|x) &gt; P( \\hat y|x) $<br>在这种情况下，因为Beam Search的作用是用来进行选取$\\hat y$，那么既然命名$P(y_{</em>}|x)$会更好，但是Beam Search却选的不对。说明Beam Search有问题</p>\n<p>Case2：$ P(y_{*}|x) &lt; P( \\hat y|x) $<br>RNN负责预测 $\\hat y$ 而他错误的将$ P( \\hat y|x) $ 生成了一个更大的P数值，这说明生成有问题。是RNN需要被调整</p>\n<h2 id=\"Attention-Module-intuition\"><a href=\"#Attention-Module-intuition\" class=\"headerlink\" title=\"Attention Module intuition\"></a>Attention Module intuition</h2><p>对比于简单使用输入RNN和输出RNN，Attention Module更好的处理翻译过程中的长句子问题。我们注意到在日常翻译中，MT在处理10个左右单词的句子时候，表现效果会比较好。但是如果再长，效果就会下降的非常明显。因此我们需要更好的模型来改进这一点。</p>\n<p>首先分析一下原因，主要是单词的记忆导致的。事实上，人类在翻译长句子的时候，采用的是一部分一部分的翻译方式，并不会把他们都记录下来，因为记忆的原因。这个事情同样出现在机器上面，因为计算资源有限，所以我们也无法给计算机留出那么大量的选项进行综合运算。</p>\n<ul>\n<li><p>Attention Weight，标注了对于一个生成的词来讲，哪些input word是应该被关注的，以及关注Weight是多少</p>\n</li>\n<li><p>$ \\alpha^{ <1,2> } $ 这个表示第一个翻译后词汇，需要对翻译前句子中的第二个词的关注度有多高</1,2></p>\n</li>\n</ul>\n<h2 id=\"Atention-Module\"><a href=\"#Atention-Module\" class=\"headerlink\" title=\"Atention Module\"></a>Atention Module</h2><script type=\"math/tex; mode=display\">\\sum_{t} \\alpha^{ <1, t> } = 1</script><script type=\"math/tex; mode=display\">C^{ <1> } （context）=  \\sum_{t} \\alpha^{ <1, t> } a^{ <t> }</script><script type=\"math/tex; mode=display\">bug_{a} =  \\alpha * sprint_{a} + \\beta * sprint_{a-1} + bias</script><h1 id=\"领域应用\"><a href=\"#领域应用\" class=\"headerlink\" title=\"领域应用\"></a>领域应用</h1><h2 id=\"语音识别-（Speech-Recorgnition）\"><a href=\"#语音识别-（Speech-Recorgnition）\" class=\"headerlink\" title=\"语音识别 （Speech Recorgnition）\"></a>语音识别 （Speech Recorgnition）</h2><p>Audio Clip x，translate to ，Transcript y </p>\n</placeholder>"},{"title":"关于如何用Hexo书写数学符号(深度学习前哨贴)","date":"2018-04-23T13:36:17.000Z","mathjax":true,"_content":"\n翻了几个帖子，总算是搞定了。最主要的帖子是以下两个\n\n[在hexo博客中使用Mathjax写LaTex数学公式\"](https://blog.csdn.net/sherlockzoom/article/details/43835613)\n[如何在 hexo 中支持 Mathjax？](https://blog.csdn.net/u014630987/article/details/78670258)\n[描述在 hexo 中使用矩阵的方法](https://blog.csdn.net/Mage_EE/article/details/75317083)\n\n另外，按照Hexo文档上写的，理论上Hexo-math应该已经支持MathJax了，但是似乎用起来有点问题，不知道是hexo文档的错，还是我自己那个地方配置有错，以后找时间在研究吧。地址如下：\nhttps://github.com/hexojs/hexo-math\n\n还有最后要提醒一点，本次修改以后，不能用Hexo原生提供的assert方式来写作了，需要使用纯markdown模式。目前感觉良好，不知道后续会不会有什么坑。目前看这个改变不会影响之前的东西。\n\n[markdown的书写格式](https://help.ghost.org/article/4-markdown-guide)\n\n这是一个公式  $E=mc^2$\nSimple inline $a = b + c$.\n\n$$\\sum_{i=1}^n a_i=0$$ \n\n$$f(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2$$\n\n大功告成，接下来需要学习MathJax了\n[MathJax一些说明](https://blog.csdn.net/ethmery/article/details/50670297)\n[原文文档](https://docs.mathjax.org/en/latest/tex.html)","source":"_posts/deep-learning-study.md","raw":"title: 关于如何用Hexo书写数学符号(深度学习前哨贴)\ndate: 2018-04-23 21:36:17\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\nmathjax: true\n---\n\n翻了几个帖子，总算是搞定了。最主要的帖子是以下两个\n\n[在hexo博客中使用Mathjax写LaTex数学公式\"](https://blog.csdn.net/sherlockzoom/article/details/43835613)\n[如何在 hexo 中支持 Mathjax？](https://blog.csdn.net/u014630987/article/details/78670258)\n[描述在 hexo 中使用矩阵的方法](https://blog.csdn.net/Mage_EE/article/details/75317083)\n\n另外，按照Hexo文档上写的，理论上Hexo-math应该已经支持MathJax了，但是似乎用起来有点问题，不知道是hexo文档的错，还是我自己那个地方配置有错，以后找时间在研究吧。地址如下：\nhttps://github.com/hexojs/hexo-math\n\n还有最后要提醒一点，本次修改以后，不能用Hexo原生提供的assert方式来写作了，需要使用纯markdown模式。目前感觉良好，不知道后续会不会有什么坑。目前看这个改变不会影响之前的东西。\n\n[markdown的书写格式](https://help.ghost.org/article/4-markdown-guide)\n\n这是一个公式  $E=mc^2$\nSimple inline $a = b + c$.\n\n$$\\sum_{i=1}^n a_i=0$$ \n\n$$f(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2$$\n\n大功告成，接下来需要学习MathJax了\n[MathJax一些说明](https://blog.csdn.net/ethmery/article/details/50670297)\n[原文文档](https://docs.mathjax.org/en/latest/tex.html)","slug":"deep-learning-study","published":1,"updated":"2018-09-08T05:42:34.551Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcod000brlfyndh7s1mi","content":"<p>翻了几个帖子，总算是搞定了。最主要的帖子是以下两个</p>\n<p><a href=\"https://blog.csdn.net/sherlockzoom/article/details/43835613\" target=\"_blank\" rel=\"noopener\">在hexo博客中使用Mathjax写LaTex数学公式”</a><br><a href=\"https://blog.csdn.net/u014630987/article/details/78670258\" target=\"_blank\" rel=\"noopener\">如何在 hexo 中支持 Mathjax？</a><br><a href=\"https://blog.csdn.net/Mage_EE/article/details/75317083\" target=\"_blank\" rel=\"noopener\">描述在 hexo 中使用矩阵的方法</a></p>\n<p>另外，按照Hexo文档上写的，理论上Hexo-math应该已经支持MathJax了，但是似乎用起来有点问题，不知道是hexo文档的错，还是我自己那个地方配置有错，以后找时间在研究吧。地址如下：<br><a href=\"https://github.com/hexojs/hexo-math\" target=\"_blank\" rel=\"noopener\">https://github.com/hexojs/hexo-math</a></p>\n<p>还有最后要提醒一点，本次修改以后，不能用Hexo原生提供的assert方式来写作了，需要使用纯markdown模式。目前感觉良好，不知道后续会不会有什么坑。目前看这个改变不会影响之前的东西。</p>\n<p><a href=\"https://help.ghost.org/article/4-markdown-guide\" target=\"_blank\" rel=\"noopener\">markdown的书写格式</a></p>\n<p>这是一个公式  $E=mc^2$<br>Simple inline $a = b + c$.</p>\n<script type=\"math/tex; mode=display\">\\sum_{i=1}^n a_i=0</script><script type=\"math/tex; mode=display\">f(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2</script><p>大功告成，接下来需要学习MathJax了<br><a href=\"https://blog.csdn.net/ethmery/article/details/50670297\" target=\"_blank\" rel=\"noopener\">MathJax一些说明</a><br><a href=\"https://docs.mathjax.org/en/latest/tex.html\" target=\"_blank\" rel=\"noopener\">原文文档</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>翻了几个帖子，总算是搞定了。最主要的帖子是以下两个</p>\n<p><a href=\"https://blog.csdn.net/sherlockzoom/article/details/43835613\" target=\"_blank\" rel=\"noopener\">在hexo博客中使用Mathjax写LaTex数学公式”</a><br><a href=\"https://blog.csdn.net/u014630987/article/details/78670258\" target=\"_blank\" rel=\"noopener\">如何在 hexo 中支持 Mathjax？</a><br><a href=\"https://blog.csdn.net/Mage_EE/article/details/75317083\" target=\"_blank\" rel=\"noopener\">描述在 hexo 中使用矩阵的方法</a></p>\n<p>另外，按照Hexo文档上写的，理论上Hexo-math应该已经支持MathJax了，但是似乎用起来有点问题，不知道是hexo文档的错，还是我自己那个地方配置有错，以后找时间在研究吧。地址如下：<br><a href=\"https://github.com/hexojs/hexo-math\" target=\"_blank\" rel=\"noopener\">https://github.com/hexojs/hexo-math</a></p>\n<p>还有最后要提醒一点，本次修改以后，不能用Hexo原生提供的assert方式来写作了，需要使用纯markdown模式。目前感觉良好，不知道后续会不会有什么坑。目前看这个改变不会影响之前的东西。</p>\n<p><a href=\"https://help.ghost.org/article/4-markdown-guide\" target=\"_blank\" rel=\"noopener\">markdown的书写格式</a></p>\n<p>这是一个公式  $E=mc^2$<br>Simple inline $a = b + c$.</p>\n<script type=\"math/tex; mode=display\">\\sum_{i=1}^n a_i=0</script><script type=\"math/tex; mode=display\">f(x_1,x_2,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2</script><p>大功告成，接下来需要学习MathJax了<br><a href=\"https://blog.csdn.net/ethmery/article/details/50670297\" target=\"_blank\" rel=\"noopener\">MathJax一些说明</a><br><a href=\"https://docs.mathjax.org/en/latest/tex.html\" target=\"_blank\" rel=\"noopener\">原文文档</a></p>\n"},{"title":"深度学习第三周神经网络","date":"2018-05-04T07:45:00.000Z","_content":"\n上一周（章）主要学习的是如何构建模型，梯级下降和使用numpy进行向量计算。这一周开始进入神经网络的学习。同样的首先上大纲：\n\n*\t神经网络全景图（Neural Networks Overview）\n*\t神经网络表达（Neural Network Representation）\n*\t计算神经网络输出（Computing a Neural Network's Output）\n*\t向量化（Vectorizing across multiple examples）\n*\t向量实现（Expanation for Vectorized Implmetation）\n*\t激励函数（Activation functions）\n*\t为什么需要非线性模型（Why do you need non-linear ）\n\n这一周的学习说简单也简单，说难也难。我们来先说简单的：\n\n所谓简单，指的是这一周的课程主要是在上一周Logistics Regression的基础上加入了一个Hidden Layer的概念，即将所有的内容从两层输入输出结构，变成了三层，因此引入了$n^0, n^1, n^2$的三个层进行计算。这样做的好处是可以将模型变得拟合度更高，进一步提高Accuracy。而所有在Logistics Regression中用到的公式和方法基本沿用，所以数学本质上并不难，只是增加一个维度。\n\n说他难呢，基本上就是因为增加了一个维度，所以derivative的所有计算方面，确实需要更多的东西了。当然这里也有一些小改变，那就是引入了activation function的概念，将原来那个在逻辑回归中谈到的Sigmoid Functoin，作为一种activation function；从而进一步引入其他的activation function，比如“$tanh$”\n\n把这一周的内容集中在使用场景方面的总结：\n{% asset_img hl3_4.png [Hidden Layer 3] %}\n对于3到4层Hidden Layer的时候\n\n{% asset_img hl5_20.png [Hidden Layer 5] %}\n对于5到20层Hidden Layer的时候\n\n{% asset_img hl50.png [Hidden Layer 50] %}\n对于50层Hidden Layer的时候\n\n可以明显的看出当Hidden Layer提升了以后，预测拟合度就更高了，这就是神经网络带来的巨大意义\n","source":"_posts/deeplearning-3-1.md","raw":"title: 深度学习第三周神经网络\ndate: 2018-05-04 15:45:00\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\n---\n\n上一周（章）主要学习的是如何构建模型，梯级下降和使用numpy进行向量计算。这一周开始进入神经网络的学习。同样的首先上大纲：\n\n*\t神经网络全景图（Neural Networks Overview）\n*\t神经网络表达（Neural Network Representation）\n*\t计算神经网络输出（Computing a Neural Network's Output）\n*\t向量化（Vectorizing across multiple examples）\n*\t向量实现（Expanation for Vectorized Implmetation）\n*\t激励函数（Activation functions）\n*\t为什么需要非线性模型（Why do you need non-linear ）\n\n这一周的学习说简单也简单，说难也难。我们来先说简单的：\n\n所谓简单，指的是这一周的课程主要是在上一周Logistics Regression的基础上加入了一个Hidden Layer的概念，即将所有的内容从两层输入输出结构，变成了三层，因此引入了$n^0, n^1, n^2$的三个层进行计算。这样做的好处是可以将模型变得拟合度更高，进一步提高Accuracy。而所有在Logistics Regression中用到的公式和方法基本沿用，所以数学本质上并不难，只是增加一个维度。\n\n说他难呢，基本上就是因为增加了一个维度，所以derivative的所有计算方面，确实需要更多的东西了。当然这里也有一些小改变，那就是引入了activation function的概念，将原来那个在逻辑回归中谈到的Sigmoid Functoin，作为一种activation function；从而进一步引入其他的activation function，比如“$tanh$”\n\n把这一周的内容集中在使用场景方面的总结：\n{% asset_img hl3_4.png [Hidden Layer 3] %}\n对于3到4层Hidden Layer的时候\n\n{% asset_img hl5_20.png [Hidden Layer 5] %}\n对于5到20层Hidden Layer的时候\n\n{% asset_img hl50.png [Hidden Layer 50] %}\n对于50层Hidden Layer的时候\n\n可以明显的看出当Hidden Layer提升了以后，预测拟合度就更高了，这就是神经网络带来的巨大意义\n","slug":"deeplearning-3-1","published":1,"updated":"2018-05-19T03:37:05.298Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoe000crlfy2nyexxce","content":"<p>上一周（章）主要学习的是如何构建模型，梯级下降和使用numpy进行向量计算。这一周开始进入神经网络的学习。同样的首先上大纲：</p>\n<ul>\n<li>神经网络全景图（Neural Networks Overview）</li>\n<li>神经网络表达（Neural Network Representation）</li>\n<li>计算神经网络输出（Computing a Neural Network’s Output）</li>\n<li>向量化（Vectorizing across multiple examples）</li>\n<li>向量实现（Expanation for Vectorized Implmetation）</li>\n<li>激励函数（Activation functions）</li>\n<li>为什么需要非线性模型（Why do you need non-linear ）</li>\n</ul>\n<p>这一周的学习说简单也简单，说难也难。我们来先说简单的：</p>\n<p>所谓简单，指的是这一周的课程主要是在上一周Logistics Regression的基础上加入了一个Hidden Layer的概念，即将所有的内容从两层输入输出结构，变成了三层，因此引入了$n^0, n^1, n^2$的三个层进行计算。这样做的好处是可以将模型变得拟合度更高，进一步提高Accuracy。而所有在Logistics Regression中用到的公式和方法基本沿用，所以数学本质上并不难，只是增加一个维度。</p>\n<p>说他难呢，基本上就是因为增加了一个维度，所以derivative的所有计算方面，确实需要更多的东西了。当然这里也有一些小改变，那就是引入了activation function的概念，将原来那个在逻辑回归中谈到的Sigmoid Functoin，作为一种activation function；从而进一步引入其他的activation function，比如“$tanh$”</p>\n<p>把这一周的内容集中在使用场景方面的总结：<br><img src=\"/2018/05/04/deeplearning-3-1/hl3_4.png\" title=\"[Hidden Layer 3]\"><br>对于3到4层Hidden Layer的时候</p>\n<img src=\"/2018/05/04/deeplearning-3-1/hl5_20.png\" title=\"[Hidden Layer 5]\">\n<p>对于5到20层Hidden Layer的时候</p>\n<img src=\"/2018/05/04/deeplearning-3-1/hl50.png\" title=\"[Hidden Layer 50]\">\n<p>对于50层Hidden Layer的时候</p>\n<p>可以明显的看出当Hidden Layer提升了以后，预测拟合度就更高了，这就是神经网络带来的巨大意义</p>\n","site":{"data":{}},"excerpt":"","more":"<p>上一周（章）主要学习的是如何构建模型，梯级下降和使用numpy进行向量计算。这一周开始进入神经网络的学习。同样的首先上大纲：</p>\n<ul>\n<li>神经网络全景图（Neural Networks Overview）</li>\n<li>神经网络表达（Neural Network Representation）</li>\n<li>计算神经网络输出（Computing a Neural Network’s Output）</li>\n<li>向量化（Vectorizing across multiple examples）</li>\n<li>向量实现（Expanation for Vectorized Implmetation）</li>\n<li>激励函数（Activation functions）</li>\n<li>为什么需要非线性模型（Why do you need non-linear ）</li>\n</ul>\n<p>这一周的学习说简单也简单，说难也难。我们来先说简单的：</p>\n<p>所谓简单，指的是这一周的课程主要是在上一周Logistics Regression的基础上加入了一个Hidden Layer的概念，即将所有的内容从两层输入输出结构，变成了三层，因此引入了$n^0, n^1, n^2$的三个层进行计算。这样做的好处是可以将模型变得拟合度更高，进一步提高Accuracy。而所有在Logistics Regression中用到的公式和方法基本沿用，所以数学本质上并不难，只是增加一个维度。</p>\n<p>说他难呢，基本上就是因为增加了一个维度，所以derivative的所有计算方面，确实需要更多的东西了。当然这里也有一些小改变，那就是引入了activation function的概念，将原来那个在逻辑回归中谈到的Sigmoid Functoin，作为一种activation function；从而进一步引入其他的activation function，比如“$tanh$”</p>\n<p>把这一周的内容集中在使用场景方面的总结：<br><img src=\"/2018/05/04/deeplearning-3-1/hl3_4.png\" title=\"[Hidden Layer 3]\"><br>对于3到4层Hidden Layer的时候</p>\n<img src=\"/2018/05/04/deeplearning-3-1/hl5_20.png\" title=\"[Hidden Layer 5]\">\n<p>对于5到20层Hidden Layer的时候</p>\n<img src=\"/2018/05/04/deeplearning-3-1/hl50.png\" title=\"[Hidden Layer 50]\">\n<p>对于50层Hidden Layer的时候</p>\n<p>可以明显的看出当Hidden Layer提升了以后，预测拟合度就更高了，这就是神经网络带来的巨大意义</p>\n"},{"title":"指数型组织EXO读后感","date":"2016-02-26T13:26:01.000Z","_content":"摘要：\n\n+\t指数型组织是一系列信仰MTP和为之奋斗人组成的集合\n+\t指数型组织是以一套可复制的“内核”为基础而存在的\n+\t指数型组织一定是“不完整的”，甚至不一定是一个公司，可能只是一个部门\n\n\n今天来一起聊一本书。EXO（Exponential Organizations）指数型组织。书已经送人啦，所以完全靠着记忆来写。记录下我所理解这本书的精华部分。\n\n{% asset_img cover.jpeg \"指数型组织 EXO\" %}\n\n这本书最让我惊讶的部分是他描述了一个新时代下的公司需求模型和与之相配套对应的组织模型。书中认为，过去我们在理解商业需求，往往是基于稀缺化的理解。认为企业的目的就是占有稀缺资源，并以低廉的成本来销售稀缺资源从而获得收益。简言之，企业的目的就是“出售稀缺性”。\n\n+\t首先必须承认这是对的，毕竟在过去的生产工具情况下，任何一个单体的个人都很难靠自己去触及到所有生活必需的来源。那么能触达人越少，资源也就越稀缺。几个例子，就是钻石，其实钻石这个东西本身在自然界并不稀缺，甚至人类都能加工出钻石。而依照“出售稀缺性”的理论，事实上几家国际钻石企业垄断了全球所有的钻石矿藏，并不断通过政府关系，广告媒体，对外宣传纯自然钻石的难度和少有，造成了一种“人为稀缺性”的假象。所以现在的钻石才卖的如此之价格高昂。\n\n+\t其次必须承认这是错的，因为在现代互联网发展到今天，个体世界已经在出现变化。在这方面首当其冲的就是信息稀缺性在被日益打破，信息爆炸，如今在Google上已经可以找到几乎所有碰到问题的答案。在信息富足的基础上，所谓互联网＋，O2O在做的，就是通过信息的富足尝试激活更广泛意义上的富足资源。比如Uber，其方法是通过信息的快速传播，将满大街跑和停在停车场的汽车的剩余座位开放出来，让更多人使用。据说全世界每时每刻，有除去驾驶员之外的93%的汽车乘坐位是空的。\n\n因此，现代社会随着互联网的发展，企业从“出售稀缺性”开始进入到“出售富足性”。用两句话解释我的理解：\n\n*\t出售稀缺性：这个世界上只有我这里有，你来买吧\n*\t出售富足性：我告诉你这个世界还有哪里有，你去买吧\n\n也正是因为这种变化，于是有了新时代企业所谓互联网公司“独角兽”类型公司的发展方法。我不买产品，我买“算法”，这里的算法是一个广泛意义上的含义，可以是一种“流程”，可以是一种“自动化方法”，可以是一种真正的“算法”，也可以是一种“产品”。所以问题就来了，既然我是一个“告诉你哪里有”的内核，那我必然是要靠可大规模复制的“量”来取胜的，这和薄利多销有点像。不一样的是，新时代企业要做的是把销售的边际成本降到最低，甚至0的地步，来取胜。这就是EXO这本书告诉我们的一个新的商业逻辑。也是互联网的主要逻辑。\n\n说到我们的数据采集部分，对于数据采集，这是个很慢的工作传统的方法是尽量多的占有低端录入人员，销售大量的“稀缺”人员时间。但是，随着众测，机器自动化的发展，新型数据采集工作，销售的是广泛人的“富余时间”，同时通过算法，找到拥有“富余时间”的人，和降低这种购买“富余时间”所使用的成本。\n\n讲完商业思路的主旨，下面就是这本书让我非常欣赏的部分，就是将如果要实现“富足”销售，企业需要具备的几个基本要素。书中简化为了两个词，SCALE和IDEAS，这也是这本书非常值得推荐的原因。\n\n{% asset_img core.jpg \"SCALE和IDEAS\" %}\n\n简单讲几个，其他的大家可以看书。书中认为，SCALE是外部属性。\n\n+\t其中C，是Community&Crowd，社群和大众。书中认为，既然组织售卖的是富足性，那么这种富足性的来源一定是组织外部，因为组织的增长永远呈现的是线性模型，很难呈现大的指数型增长，这和招聘是相关的。因为优秀的人员是有限的，所以就需要广泛的通过外部合作来创造增长。传统意义上认为，外部合作是不稳定的，而如果读过《失控》的人都知道，一个完全有序的可控制体系是不存在的，有的就是一种混沌的平衡，而一个系统（企业或大自然）一定可以自我完善，最终形成一个稳定形态。对社群和大众的使用，主要就来源于这个思想。认为只有通过最大限度的网络和不多扩张社群资源，最终才可以实现组织利益的快速指数型发展。比如众包就是这样一种模式，通过平台调度算法，和各种企业运营手段，让一个广泛的人群使用这个平台，最终完成数据的采集工作。\n\n+\t关于A，是Algorithm，算法。这个其实挺明白的，举例说Uber之所以能够实现快速扩张，是因为算法本身是不变的，同样的算法是可以大规模扩展的。这里我想说的是自己的一个理解，那就是，算法这个东西可能许多人都会想象的比较抽象。但是如果说广义的“算法”其实就比较好理解了。因为一个写在纸上的流程，第一步、第二步、第三步做什么也可以称之为一种“算法”。这种算法帮助管理和执行变得简单，降低了反复沟通不标准所造成的成本浪费。产品模块，其实也是一种算法，因为从某种程度上，产品模块就是自动化了的“纸面流程”。讲大量人的工作变成了机器，通过自动化的方法进一步降低了成本。当然，狭义算法，比如对比，查询，搜索，推荐，学习，这些都是更进一步的算法部分。还拿数据采集做例子，最早的采集系统是\n\n说了两个外部属性，再来说说内部，在IDEAS部分，\n\n+\t关于D吧，是Dashboard，意思是说对于一个组织而言，既然数据的重要性不言而喻，那么接下来的问题就是如何表现，所以“仪表盘”就显得格外重要。它是数据的基础，也是工作赖以进行的保障。在后来和公司数据老大的了解中，*我更加能明白仪表盘本身还要分成几类*\n\n{% asset_img dashboard.png \"仪表盘数据系统的分类\" %}\n\n当然，书中还描述了许多别的东西，特别是如何执行和实施方面的东西。比如一个企业在关于实验要如何做，如何通过数据预期来规范实验的方向和效果。这里不打算一一陈述。想再多一些内容谈谈自己的感悟吧。\n\n+\t指数型组织是一系列信仰MTP和为之奋斗人组成的集合。他们在一起的原因不再仅仅是为了更好的更高效的进行生产，而是基于一个理想。比如SpaceX的一个让交通变的更容易。杏树林我理解之所以让我信仰，就是那句“让医生更轻松”。因为我总是喜欢加上后半句，如果“医生每天效率提升1%，那么全中国，我们的兄弟姐妹将有上千万人次能够享受到更好的医疗”。这才是杏树林我理解里的伟大。\n\n+\t指数型组织是以一套可复制的“内核”为基础而存在的。通过内核极大限度的降低旧有生产工程的边际成本，来实现高价值。所以新兴公司一定是“小的”，并且以类似“算法”和“产品”为中心而存在。所以，指数型组织一定要问自己一个问题，我的工作边际成本是不是足够低，远远低于同业。我的方法是否是极低成本复制的。如果不是，那么请审视一下自己你的工作。作为数据平台团队，我希望做到这一点，我们的每一个产品都可能量的极大降低已有工作的负荷，提升工作效率，并且如果可以，将它转化为商业产片，也就意味着，让他进行更加广泛意义上的复制。\n\n+\t指数型组织一定是“不完整的”，现代互联网通过连接联系万物，那么公司也一样，他不再是一个必须拥有所有部门而存在的结构体，而是为了一个MTP（书里说这是公司的目标也是）而努力的一群人。那么他的工作不是让整个xxx提升，而是让一个属于自己的MTP得到提升。每个人都无法解决链接那一头，但是可以解决的是，将许多东西进行链接，最终形成指数型成长。\n\n所以机遇这个，也是为什么我想把保险和数据整合。一个是HAVE，一个是READ，两者互相都在做这一点彼此的事情，但是又各有重点。只有互相合作，减少重复建设，才能真正把实现“杏树林数据价值化”这件事情走下去。这就是我想做的。我会相信，如果数据平台当成一个连接体，把公司内部作为一个客户，把公司外部做为其他平等的客户，这样，一个指数型组织才有可能真正成长起来。因为组织的内部所有工作，都将围绕着最大限度降低边际成本这件事情来完成，这样就一定能突出价值的最大化！加油吧！\n\n","source":"_posts/exo.md","raw":"title: 指数型组织EXO读后感\ndate: 2016-02-26 21:26:01\ncategories:\n- Management\ntags:\n- book\n- mgnt\n---\n摘要：\n\n+\t指数型组织是一系列信仰MTP和为之奋斗人组成的集合\n+\t指数型组织是以一套可复制的“内核”为基础而存在的\n+\t指数型组织一定是“不完整的”，甚至不一定是一个公司，可能只是一个部门\n\n\n今天来一起聊一本书。EXO（Exponential Organizations）指数型组织。书已经送人啦，所以完全靠着记忆来写。记录下我所理解这本书的精华部分。\n\n{% asset_img cover.jpeg \"指数型组织 EXO\" %}\n\n这本书最让我惊讶的部分是他描述了一个新时代下的公司需求模型和与之相配套对应的组织模型。书中认为，过去我们在理解商业需求，往往是基于稀缺化的理解。认为企业的目的就是占有稀缺资源，并以低廉的成本来销售稀缺资源从而获得收益。简言之，企业的目的就是“出售稀缺性”。\n\n+\t首先必须承认这是对的，毕竟在过去的生产工具情况下，任何一个单体的个人都很难靠自己去触及到所有生活必需的来源。那么能触达人越少，资源也就越稀缺。几个例子，就是钻石，其实钻石这个东西本身在自然界并不稀缺，甚至人类都能加工出钻石。而依照“出售稀缺性”的理论，事实上几家国际钻石企业垄断了全球所有的钻石矿藏，并不断通过政府关系，广告媒体，对外宣传纯自然钻石的难度和少有，造成了一种“人为稀缺性”的假象。所以现在的钻石才卖的如此之价格高昂。\n\n+\t其次必须承认这是错的，因为在现代互联网发展到今天，个体世界已经在出现变化。在这方面首当其冲的就是信息稀缺性在被日益打破，信息爆炸，如今在Google上已经可以找到几乎所有碰到问题的答案。在信息富足的基础上，所谓互联网＋，O2O在做的，就是通过信息的富足尝试激活更广泛意义上的富足资源。比如Uber，其方法是通过信息的快速传播，将满大街跑和停在停车场的汽车的剩余座位开放出来，让更多人使用。据说全世界每时每刻，有除去驾驶员之外的93%的汽车乘坐位是空的。\n\n因此，现代社会随着互联网的发展，企业从“出售稀缺性”开始进入到“出售富足性”。用两句话解释我的理解：\n\n*\t出售稀缺性：这个世界上只有我这里有，你来买吧\n*\t出售富足性：我告诉你这个世界还有哪里有，你去买吧\n\n也正是因为这种变化，于是有了新时代企业所谓互联网公司“独角兽”类型公司的发展方法。我不买产品，我买“算法”，这里的算法是一个广泛意义上的含义，可以是一种“流程”，可以是一种“自动化方法”，可以是一种真正的“算法”，也可以是一种“产品”。所以问题就来了，既然我是一个“告诉你哪里有”的内核，那我必然是要靠可大规模复制的“量”来取胜的，这和薄利多销有点像。不一样的是，新时代企业要做的是把销售的边际成本降到最低，甚至0的地步，来取胜。这就是EXO这本书告诉我们的一个新的商业逻辑。也是互联网的主要逻辑。\n\n说到我们的数据采集部分，对于数据采集，这是个很慢的工作传统的方法是尽量多的占有低端录入人员，销售大量的“稀缺”人员时间。但是，随着众测，机器自动化的发展，新型数据采集工作，销售的是广泛人的“富余时间”，同时通过算法，找到拥有“富余时间”的人，和降低这种购买“富余时间”所使用的成本。\n\n讲完商业思路的主旨，下面就是这本书让我非常欣赏的部分，就是将如果要实现“富足”销售，企业需要具备的几个基本要素。书中简化为了两个词，SCALE和IDEAS，这也是这本书非常值得推荐的原因。\n\n{% asset_img core.jpg \"SCALE和IDEAS\" %}\n\n简单讲几个，其他的大家可以看书。书中认为，SCALE是外部属性。\n\n+\t其中C，是Community&Crowd，社群和大众。书中认为，既然组织售卖的是富足性，那么这种富足性的来源一定是组织外部，因为组织的增长永远呈现的是线性模型，很难呈现大的指数型增长，这和招聘是相关的。因为优秀的人员是有限的，所以就需要广泛的通过外部合作来创造增长。传统意义上认为，外部合作是不稳定的，而如果读过《失控》的人都知道，一个完全有序的可控制体系是不存在的，有的就是一种混沌的平衡，而一个系统（企业或大自然）一定可以自我完善，最终形成一个稳定形态。对社群和大众的使用，主要就来源于这个思想。认为只有通过最大限度的网络和不多扩张社群资源，最终才可以实现组织利益的快速指数型发展。比如众包就是这样一种模式，通过平台调度算法，和各种企业运营手段，让一个广泛的人群使用这个平台，最终完成数据的采集工作。\n\n+\t关于A，是Algorithm，算法。这个其实挺明白的，举例说Uber之所以能够实现快速扩张，是因为算法本身是不变的，同样的算法是可以大规模扩展的。这里我想说的是自己的一个理解，那就是，算法这个东西可能许多人都会想象的比较抽象。但是如果说广义的“算法”其实就比较好理解了。因为一个写在纸上的流程，第一步、第二步、第三步做什么也可以称之为一种“算法”。这种算法帮助管理和执行变得简单，降低了反复沟通不标准所造成的成本浪费。产品模块，其实也是一种算法，因为从某种程度上，产品模块就是自动化了的“纸面流程”。讲大量人的工作变成了机器，通过自动化的方法进一步降低了成本。当然，狭义算法，比如对比，查询，搜索，推荐，学习，这些都是更进一步的算法部分。还拿数据采集做例子，最早的采集系统是\n\n说了两个外部属性，再来说说内部，在IDEAS部分，\n\n+\t关于D吧，是Dashboard，意思是说对于一个组织而言，既然数据的重要性不言而喻，那么接下来的问题就是如何表现，所以“仪表盘”就显得格外重要。它是数据的基础，也是工作赖以进行的保障。在后来和公司数据老大的了解中，*我更加能明白仪表盘本身还要分成几类*\n\n{% asset_img dashboard.png \"仪表盘数据系统的分类\" %}\n\n当然，书中还描述了许多别的东西，特别是如何执行和实施方面的东西。比如一个企业在关于实验要如何做，如何通过数据预期来规范实验的方向和效果。这里不打算一一陈述。想再多一些内容谈谈自己的感悟吧。\n\n+\t指数型组织是一系列信仰MTP和为之奋斗人组成的集合。他们在一起的原因不再仅仅是为了更好的更高效的进行生产，而是基于一个理想。比如SpaceX的一个让交通变的更容易。杏树林我理解之所以让我信仰，就是那句“让医生更轻松”。因为我总是喜欢加上后半句，如果“医生每天效率提升1%，那么全中国，我们的兄弟姐妹将有上千万人次能够享受到更好的医疗”。这才是杏树林我理解里的伟大。\n\n+\t指数型组织是以一套可复制的“内核”为基础而存在的。通过内核极大限度的降低旧有生产工程的边际成本，来实现高价值。所以新兴公司一定是“小的”，并且以类似“算法”和“产品”为中心而存在。所以，指数型组织一定要问自己一个问题，我的工作边际成本是不是足够低，远远低于同业。我的方法是否是极低成本复制的。如果不是，那么请审视一下自己你的工作。作为数据平台团队，我希望做到这一点，我们的每一个产品都可能量的极大降低已有工作的负荷，提升工作效率，并且如果可以，将它转化为商业产片，也就意味着，让他进行更加广泛意义上的复制。\n\n+\t指数型组织一定是“不完整的”，现代互联网通过连接联系万物，那么公司也一样，他不再是一个必须拥有所有部门而存在的结构体，而是为了一个MTP（书里说这是公司的目标也是）而努力的一群人。那么他的工作不是让整个xxx提升，而是让一个属于自己的MTP得到提升。每个人都无法解决链接那一头，但是可以解决的是，将许多东西进行链接，最终形成指数型成长。\n\n所以机遇这个，也是为什么我想把保险和数据整合。一个是HAVE，一个是READ，两者互相都在做这一点彼此的事情，但是又各有重点。只有互相合作，减少重复建设，才能真正把实现“杏树林数据价值化”这件事情走下去。这就是我想做的。我会相信，如果数据平台当成一个连接体，把公司内部作为一个客户，把公司外部做为其他平等的客户，这样，一个指数型组织才有可能真正成长起来。因为组织的内部所有工作，都将围绕着最大限度降低边际成本这件事情来完成，这样就一定能突出价值的最大化！加油吧！\n\n","slug":"exo","published":1,"updated":"2018-04-16T09:59:37.075Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcof000drlfyr6r1f9k4","content":"<p>摘要：</p>\n<ul>\n<li>指数型组织是一系列信仰MTP和为之奋斗人组成的集合</li>\n<li>指数型组织是以一套可复制的“内核”为基础而存在的</li>\n<li>指数型组织一定是“不完整的”，甚至不一定是一个公司，可能只是一个部门</li>\n</ul>\n<p>今天来一起聊一本书。EXO（Exponential Organizations）指数型组织。书已经送人啦，所以完全靠着记忆来写。记录下我所理解这本书的精华部分。</p>\n<img src=\"/2016/02/26/exo/cover.jpeg\" title=\"指数型组织 EXO\">\n<p>这本书最让我惊讶的部分是他描述了一个新时代下的公司需求模型和与之相配套对应的组织模型。书中认为，过去我们在理解商业需求，往往是基于稀缺化的理解。认为企业的目的就是占有稀缺资源，并以低廉的成本来销售稀缺资源从而获得收益。简言之，企业的目的就是“出售稀缺性”。</p>\n<ul>\n<li><p>首先必须承认这是对的，毕竟在过去的生产工具情况下，任何一个单体的个人都很难靠自己去触及到所有生活必需的来源。那么能触达人越少，资源也就越稀缺。几个例子，就是钻石，其实钻石这个东西本身在自然界并不稀缺，甚至人类都能加工出钻石。而依照“出售稀缺性”的理论，事实上几家国际钻石企业垄断了全球所有的钻石矿藏，并不断通过政府关系，广告媒体，对外宣传纯自然钻石的难度和少有，造成了一种“人为稀缺性”的假象。所以现在的钻石才卖的如此之价格高昂。</p>\n</li>\n<li><p>其次必须承认这是错的，因为在现代互联网发展到今天，个体世界已经在出现变化。在这方面首当其冲的就是信息稀缺性在被日益打破，信息爆炸，如今在Google上已经可以找到几乎所有碰到问题的答案。在信息富足的基础上，所谓互联网＋，O2O在做的，就是通过信息的富足尝试激活更广泛意义上的富足资源。比如Uber，其方法是通过信息的快速传播，将满大街跑和停在停车场的汽车的剩余座位开放出来，让更多人使用。据说全世界每时每刻，有除去驾驶员之外的93%的汽车乘坐位是空的。</p>\n</li>\n</ul>\n<p>因此，现代社会随着互联网的发展，企业从“出售稀缺性”开始进入到“出售富足性”。用两句话解释我的理解：</p>\n<ul>\n<li>出售稀缺性：这个世界上只有我这里有，你来买吧</li>\n<li>出售富足性：我告诉你这个世界还有哪里有，你去买吧</li>\n</ul>\n<p>也正是因为这种变化，于是有了新时代企业所谓互联网公司“独角兽”类型公司的发展方法。我不买产品，我买“算法”，这里的算法是一个广泛意义上的含义，可以是一种“流程”，可以是一种“自动化方法”，可以是一种真正的“算法”，也可以是一种“产品”。所以问题就来了，既然我是一个“告诉你哪里有”的内核，那我必然是要靠可大规模复制的“量”来取胜的，这和薄利多销有点像。不一样的是，新时代企业要做的是把销售的边际成本降到最低，甚至0的地步，来取胜。这就是EXO这本书告诉我们的一个新的商业逻辑。也是互联网的主要逻辑。</p>\n<p>说到我们的数据采集部分，对于数据采集，这是个很慢的工作传统的方法是尽量多的占有低端录入人员，销售大量的“稀缺”人员时间。但是，随着众测，机器自动化的发展，新型数据采集工作，销售的是广泛人的“富余时间”，同时通过算法，找到拥有“富余时间”的人，和降低这种购买“富余时间”所使用的成本。</p>\n<p>讲完商业思路的主旨，下面就是这本书让我非常欣赏的部分，就是将如果要实现“富足”销售，企业需要具备的几个基本要素。书中简化为了两个词，SCALE和IDEAS，这也是这本书非常值得推荐的原因。</p>\n<img src=\"/2016/02/26/exo/core.jpg\" title=\"SCALE和IDEAS\">\n<p>简单讲几个，其他的大家可以看书。书中认为，SCALE是外部属性。</p>\n<ul>\n<li><p>其中C，是Community&amp;Crowd，社群和大众。书中认为，既然组织售卖的是富足性，那么这种富足性的来源一定是组织外部，因为组织的增长永远呈现的是线性模型，很难呈现大的指数型增长，这和招聘是相关的。因为优秀的人员是有限的，所以就需要广泛的通过外部合作来创造增长。传统意义上认为，外部合作是不稳定的，而如果读过《失控》的人都知道，一个完全有序的可控制体系是不存在的，有的就是一种混沌的平衡，而一个系统（企业或大自然）一定可以自我完善，最终形成一个稳定形态。对社群和大众的使用，主要就来源于这个思想。认为只有通过最大限度的网络和不多扩张社群资源，最终才可以实现组织利益的快速指数型发展。比如众包就是这样一种模式，通过平台调度算法，和各种企业运营手段，让一个广泛的人群使用这个平台，最终完成数据的采集工作。</p>\n</li>\n<li><p>关于A，是Algorithm，算法。这个其实挺明白的，举例说Uber之所以能够实现快速扩张，是因为算法本身是不变的，同样的算法是可以大规模扩展的。这里我想说的是自己的一个理解，那就是，算法这个东西可能许多人都会想象的比较抽象。但是如果说广义的“算法”其实就比较好理解了。因为一个写在纸上的流程，第一步、第二步、第三步做什么也可以称之为一种“算法”。这种算法帮助管理和执行变得简单，降低了反复沟通不标准所造成的成本浪费。产品模块，其实也是一种算法，因为从某种程度上，产品模块就是自动化了的“纸面流程”。讲大量人的工作变成了机器，通过自动化的方法进一步降低了成本。当然，狭义算法，比如对比，查询，搜索，推荐，学习，这些都是更进一步的算法部分。还拿数据采集做例子，最早的采集系统是</p>\n</li>\n</ul>\n<p>说了两个外部属性，再来说说内部，在IDEAS部分，</p>\n<ul>\n<li>关于D吧，是Dashboard，意思是说对于一个组织而言，既然数据的重要性不言而喻，那么接下来的问题就是如何表现，所以“仪表盘”就显得格外重要。它是数据的基础，也是工作赖以进行的保障。在后来和公司数据老大的了解中，<em>我更加能明白仪表盘本身还要分成几类</em></li>\n</ul>\n<img src=\"/2016/02/26/exo/dashboard.png\" title=\"仪表盘数据系统的分类\">\n<p>当然，书中还描述了许多别的东西，特别是如何执行和实施方面的东西。比如一个企业在关于实验要如何做，如何通过数据预期来规范实验的方向和效果。这里不打算一一陈述。想再多一些内容谈谈自己的感悟吧。</p>\n<ul>\n<li><p>指数型组织是一系列信仰MTP和为之奋斗人组成的集合。他们在一起的原因不再仅仅是为了更好的更高效的进行生产，而是基于一个理想。比如SpaceX的一个让交通变的更容易。杏树林我理解之所以让我信仰，就是那句“让医生更轻松”。因为我总是喜欢加上后半句，如果“医生每天效率提升1%，那么全中国，我们的兄弟姐妹将有上千万人次能够享受到更好的医疗”。这才是杏树林我理解里的伟大。</p>\n</li>\n<li><p>指数型组织是以一套可复制的“内核”为基础而存在的。通过内核极大限度的降低旧有生产工程的边际成本，来实现高价值。所以新兴公司一定是“小的”，并且以类似“算法”和“产品”为中心而存在。所以，指数型组织一定要问自己一个问题，我的工作边际成本是不是足够低，远远低于同业。我的方法是否是极低成本复制的。如果不是，那么请审视一下自己你的工作。作为数据平台团队，我希望做到这一点，我们的每一个产品都可能量的极大降低已有工作的负荷，提升工作效率，并且如果可以，将它转化为商业产片，也就意味着，让他进行更加广泛意义上的复制。</p>\n</li>\n<li><p>指数型组织一定是“不完整的”，现代互联网通过连接联系万物，那么公司也一样，他不再是一个必须拥有所有部门而存在的结构体，而是为了一个MTP（书里说这是公司的目标也是）而努力的一群人。那么他的工作不是让整个xxx提升，而是让一个属于自己的MTP得到提升。每个人都无法解决链接那一头，但是可以解决的是，将许多东西进行链接，最终形成指数型成长。</p>\n</li>\n</ul>\n<p>所以机遇这个，也是为什么我想把保险和数据整合。一个是HAVE，一个是READ，两者互相都在做这一点彼此的事情，但是又各有重点。只有互相合作，减少重复建设，才能真正把实现“杏树林数据价值化”这件事情走下去。这就是我想做的。我会相信，如果数据平台当成一个连接体，把公司内部作为一个客户，把公司外部做为其他平等的客户，这样，一个指数型组织才有可能真正成长起来。因为组织的内部所有工作，都将围绕着最大限度降低边际成本这件事情来完成，这样就一定能突出价值的最大化！加油吧！</p>\n","site":{"data":{}},"excerpt":"","more":"<p>摘要：</p>\n<ul>\n<li>指数型组织是一系列信仰MTP和为之奋斗人组成的集合</li>\n<li>指数型组织是以一套可复制的“内核”为基础而存在的</li>\n<li>指数型组织一定是“不完整的”，甚至不一定是一个公司，可能只是一个部门</li>\n</ul>\n<p>今天来一起聊一本书。EXO（Exponential Organizations）指数型组织。书已经送人啦，所以完全靠着记忆来写。记录下我所理解这本书的精华部分。</p>\n<img src=\"/2016/02/26/exo/cover.jpeg\" title=\"指数型组织 EXO\">\n<p>这本书最让我惊讶的部分是他描述了一个新时代下的公司需求模型和与之相配套对应的组织模型。书中认为，过去我们在理解商业需求，往往是基于稀缺化的理解。认为企业的目的就是占有稀缺资源，并以低廉的成本来销售稀缺资源从而获得收益。简言之，企业的目的就是“出售稀缺性”。</p>\n<ul>\n<li><p>首先必须承认这是对的，毕竟在过去的生产工具情况下，任何一个单体的个人都很难靠自己去触及到所有生活必需的来源。那么能触达人越少，资源也就越稀缺。几个例子，就是钻石，其实钻石这个东西本身在自然界并不稀缺，甚至人类都能加工出钻石。而依照“出售稀缺性”的理论，事实上几家国际钻石企业垄断了全球所有的钻石矿藏，并不断通过政府关系，广告媒体，对外宣传纯自然钻石的难度和少有，造成了一种“人为稀缺性”的假象。所以现在的钻石才卖的如此之价格高昂。</p>\n</li>\n<li><p>其次必须承认这是错的，因为在现代互联网发展到今天，个体世界已经在出现变化。在这方面首当其冲的就是信息稀缺性在被日益打破，信息爆炸，如今在Google上已经可以找到几乎所有碰到问题的答案。在信息富足的基础上，所谓互联网＋，O2O在做的，就是通过信息的富足尝试激活更广泛意义上的富足资源。比如Uber，其方法是通过信息的快速传播，将满大街跑和停在停车场的汽车的剩余座位开放出来，让更多人使用。据说全世界每时每刻，有除去驾驶员之外的93%的汽车乘坐位是空的。</p>\n</li>\n</ul>\n<p>因此，现代社会随着互联网的发展，企业从“出售稀缺性”开始进入到“出售富足性”。用两句话解释我的理解：</p>\n<ul>\n<li>出售稀缺性：这个世界上只有我这里有，你来买吧</li>\n<li>出售富足性：我告诉你这个世界还有哪里有，你去买吧</li>\n</ul>\n<p>也正是因为这种变化，于是有了新时代企业所谓互联网公司“独角兽”类型公司的发展方法。我不买产品，我买“算法”，这里的算法是一个广泛意义上的含义，可以是一种“流程”，可以是一种“自动化方法”，可以是一种真正的“算法”，也可以是一种“产品”。所以问题就来了，既然我是一个“告诉你哪里有”的内核，那我必然是要靠可大规模复制的“量”来取胜的，这和薄利多销有点像。不一样的是，新时代企业要做的是把销售的边际成本降到最低，甚至0的地步，来取胜。这就是EXO这本书告诉我们的一个新的商业逻辑。也是互联网的主要逻辑。</p>\n<p>说到我们的数据采集部分，对于数据采集，这是个很慢的工作传统的方法是尽量多的占有低端录入人员，销售大量的“稀缺”人员时间。但是，随着众测，机器自动化的发展，新型数据采集工作，销售的是广泛人的“富余时间”，同时通过算法，找到拥有“富余时间”的人，和降低这种购买“富余时间”所使用的成本。</p>\n<p>讲完商业思路的主旨，下面就是这本书让我非常欣赏的部分，就是将如果要实现“富足”销售，企业需要具备的几个基本要素。书中简化为了两个词，SCALE和IDEAS，这也是这本书非常值得推荐的原因。</p>\n<img src=\"/2016/02/26/exo/core.jpg\" title=\"SCALE和IDEAS\">\n<p>简单讲几个，其他的大家可以看书。书中认为，SCALE是外部属性。</p>\n<ul>\n<li><p>其中C，是Community&amp;Crowd，社群和大众。书中认为，既然组织售卖的是富足性，那么这种富足性的来源一定是组织外部，因为组织的增长永远呈现的是线性模型，很难呈现大的指数型增长，这和招聘是相关的。因为优秀的人员是有限的，所以就需要广泛的通过外部合作来创造增长。传统意义上认为，外部合作是不稳定的，而如果读过《失控》的人都知道，一个完全有序的可控制体系是不存在的，有的就是一种混沌的平衡，而一个系统（企业或大自然）一定可以自我完善，最终形成一个稳定形态。对社群和大众的使用，主要就来源于这个思想。认为只有通过最大限度的网络和不多扩张社群资源，最终才可以实现组织利益的快速指数型发展。比如众包就是这样一种模式，通过平台调度算法，和各种企业运营手段，让一个广泛的人群使用这个平台，最终完成数据的采集工作。</p>\n</li>\n<li><p>关于A，是Algorithm，算法。这个其实挺明白的，举例说Uber之所以能够实现快速扩张，是因为算法本身是不变的，同样的算法是可以大规模扩展的。这里我想说的是自己的一个理解，那就是，算法这个东西可能许多人都会想象的比较抽象。但是如果说广义的“算法”其实就比较好理解了。因为一个写在纸上的流程，第一步、第二步、第三步做什么也可以称之为一种“算法”。这种算法帮助管理和执行变得简单，降低了反复沟通不标准所造成的成本浪费。产品模块，其实也是一种算法，因为从某种程度上，产品模块就是自动化了的“纸面流程”。讲大量人的工作变成了机器，通过自动化的方法进一步降低了成本。当然，狭义算法，比如对比，查询，搜索，推荐，学习，这些都是更进一步的算法部分。还拿数据采集做例子，最早的采集系统是</p>\n</li>\n</ul>\n<p>说了两个外部属性，再来说说内部，在IDEAS部分，</p>\n<ul>\n<li>关于D吧，是Dashboard，意思是说对于一个组织而言，既然数据的重要性不言而喻，那么接下来的问题就是如何表现，所以“仪表盘”就显得格外重要。它是数据的基础，也是工作赖以进行的保障。在后来和公司数据老大的了解中，<em>我更加能明白仪表盘本身还要分成几类</em></li>\n</ul>\n<img src=\"/2016/02/26/exo/dashboard.png\" title=\"仪表盘数据系统的分类\">\n<p>当然，书中还描述了许多别的东西，特别是如何执行和实施方面的东西。比如一个企业在关于实验要如何做，如何通过数据预期来规范实验的方向和效果。这里不打算一一陈述。想再多一些内容谈谈自己的感悟吧。</p>\n<ul>\n<li><p>指数型组织是一系列信仰MTP和为之奋斗人组成的集合。他们在一起的原因不再仅仅是为了更好的更高效的进行生产，而是基于一个理想。比如SpaceX的一个让交通变的更容易。杏树林我理解之所以让我信仰，就是那句“让医生更轻松”。因为我总是喜欢加上后半句，如果“医生每天效率提升1%，那么全中国，我们的兄弟姐妹将有上千万人次能够享受到更好的医疗”。这才是杏树林我理解里的伟大。</p>\n</li>\n<li><p>指数型组织是以一套可复制的“内核”为基础而存在的。通过内核极大限度的降低旧有生产工程的边际成本，来实现高价值。所以新兴公司一定是“小的”，并且以类似“算法”和“产品”为中心而存在。所以，指数型组织一定要问自己一个问题，我的工作边际成本是不是足够低，远远低于同业。我的方法是否是极低成本复制的。如果不是，那么请审视一下自己你的工作。作为数据平台团队，我希望做到这一点，我们的每一个产品都可能量的极大降低已有工作的负荷，提升工作效率，并且如果可以，将它转化为商业产片，也就意味着，让他进行更加广泛意义上的复制。</p>\n</li>\n<li><p>指数型组织一定是“不完整的”，现代互联网通过连接联系万物，那么公司也一样，他不再是一个必须拥有所有部门而存在的结构体，而是为了一个MTP（书里说这是公司的目标也是）而努力的一群人。那么他的工作不是让整个xxx提升，而是让一个属于自己的MTP得到提升。每个人都无法解决链接那一头，但是可以解决的是，将许多东西进行链接，最终形成指数型成长。</p>\n</li>\n</ul>\n<p>所以机遇这个，也是为什么我想把保险和数据整合。一个是HAVE，一个是READ，两者互相都在做这一点彼此的事情，但是又各有重点。只有互相合作，减少重复建设，才能真正把实现“杏树林数据价值化”这件事情走下去。这就是我想做的。我会相信，如果数据平台当成一个连接体，把公司内部作为一个客户，把公司外部做为其他平等的客户，这样，一个指数型组织才有可能真正成长起来。因为组织的内部所有工作，都将围绕着最大限度降低边际成本这件事情来完成，这样就一定能突出价值的最大化！加油吧！</p>\n"},{"title":"深度学习第四周神经网络","date":"2018-05-18T04:46:46.000Z","_content":"\n上一周（章）主要学习的是一个Hidden Layer的情况下，如何进行模型搭建。这一周开始进入多个Layer的学习。同样的首先上大纲：\n\n*\tL层深的神经网络 (Deep L-layer neural network)\n*\t深度网络中的向前扩展 (Forward Propagation in Deep Network)\n*\t正确获取和验证行列式 (Getting your matrix dimensions right)\n*\t深度代表什么 (Why deep representations)\n*\t组建深度神经网络的模块 (Building blocks of deep neural networks)\n*\t向前和向后扩展啊 (Forward and Backward Propagation)\n*\t参数和高度参数 (Parameters vs Hyperparameters)\n*\t计算机神经网络与大脑神经网络 (What does this have to do with the brain)\n\n\n一个基本的Linear到sigmoid的公式\n$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$\n\n\nIn general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with  n[l]=1n[l]=1  for every layer, and the network is no more powerful than a linear classifier such as logistic regression.","source":"_posts/deeplearning-4-1.md","raw":"title: 深度学习第四周神经网络\ndate: 2018-05-18 12:46:46\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\n---\n\n上一周（章）主要学习的是一个Hidden Layer的情况下，如何进行模型搭建。这一周开始进入多个Layer的学习。同样的首先上大纲：\n\n*\tL层深的神经网络 (Deep L-layer neural network)\n*\t深度网络中的向前扩展 (Forward Propagation in Deep Network)\n*\t正确获取和验证行列式 (Getting your matrix dimensions right)\n*\t深度代表什么 (Why deep representations)\n*\t组建深度神经网络的模块 (Building blocks of deep neural networks)\n*\t向前和向后扩展啊 (Forward and Backward Propagation)\n*\t参数和高度参数 (Parameters vs Hyperparameters)\n*\t计算机神经网络与大脑神经网络 (What does this have to do with the brain)\n\n\n一个基本的Linear到sigmoid的公式\n$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$\n\n\nIn general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with  n[l]=1n[l]=1  for every layer, and the network is no more powerful than a linear classifier such as logistic regression.","slug":"deeplearning-4-1","published":1,"updated":"2018-05-27T13:45:00.718Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcog000erlfy5zii1622","content":"<p>上一周（章）主要学习的是一个Hidden Layer的情况下，如何进行模型搭建。这一周开始进入多个Layer的学习。同样的首先上大纲：</p>\n<ul>\n<li>L层深的神经网络 (Deep L-layer neural network)</li>\n<li>深度网络中的向前扩展 (Forward Propagation in Deep Network)</li>\n<li>正确获取和验证行列式 (Getting your matrix dimensions right)</li>\n<li>深度代表什么 (Why deep representations)</li>\n<li>组建深度神经网络的模块 (Building blocks of deep neural networks)</li>\n<li>向前和向后扩展啊 (Forward and Backward Propagation)</li>\n<li>参数和高度参数 (Parameters vs Hyperparameters)</li>\n<li>计算机神经网络与大脑神经网络 (What does this have to do with the brain)</li>\n</ul>\n<p>一个基本的Linear到sigmoid的公式<br>$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$</p>\n<p>In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with  n[l]=1n[l]=1  for every layer, and the network is no more powerful than a linear classifier such as logistic regression.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>上一周（章）主要学习的是一个Hidden Layer的情况下，如何进行模型搭建。这一周开始进入多个Layer的学习。同样的首先上大纲：</p>\n<ul>\n<li>L层深的神经网络 (Deep L-layer neural network)</li>\n<li>深度网络中的向前扩展 (Forward Propagation in Deep Network)</li>\n<li>正确获取和验证行列式 (Getting your matrix dimensions right)</li>\n<li>深度代表什么 (Why deep representations)</li>\n<li>组建深度神经网络的模块 (Building blocks of deep neural networks)</li>\n<li>向前和向后扩展啊 (Forward and Backward Propagation)</li>\n<li>参数和高度参数 (Parameters vs Hyperparameters)</li>\n<li>计算机神经网络与大脑神经网络 (What does this have to do with the brain)</li>\n</ul>\n<p>一个基本的Linear到sigmoid的公式<br>$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$</p>\n<p>In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with  n[l]=1n[l]=1  for every layer, and the network is no more powerful than a linear classifier such as logistic regression.</p>\n"},{"title":"团队Lead需要关注的四件事","date":"2016-03-13T10:22:48.000Z","_content":"\n这个周发生了几件小事情。\n\n-\t首先和杨宇、张诚有过一次讨论，觉得事情推进困难，一个随访的小红点问题似乎N久也得不到解决。关于operation和admin等后台不知道谁管也不知道该怎么办。\n-\t在本周由于同事离职反馈了两点信息，公司创业气氛不够，总觉得推动不足，以及责任心的一场讨论。而团队的同事另一方面觉得，自己没人管，没有人安排工作和关心。\n-\t第三个事情是周末一早出现的一个产品事故，业务负责人很难受；另一方面其实我能感觉到，事情相关的技术同事也都是公司数一数二有责任心的人，怎么会出现这个结果呢？\n\n以上的事实，可能很多人都会归结成一句挺伤人心的话，叫做“责任感意识不强”。于是，这个似乎就理所应当成为了一个“人”的问题，更进一步说成了“别人”的问题。但说实话，在我心里，我挺认同文亮常说的那句话，“一个事情成功了找客观原因，失败了一定要找自己的问题”。这和我的为人风格也很像，说白了，任何事情，无论成败，都应该称为自己前进的阶梯，只有这样我们才能从中成长，不断颠覆打败那个曾经的自己。\n\n于是周末和同事的一场讨论，一场回顾会，让我好像悟出来点什么。话题开始与《格鲁夫给经理人的第一课》。这是一本像“德鲁克”系列的经典管理书。（虽然会不屑于一些陈旧的言论，但是经典总归是经典里面的宝藏，在不断的实践中会一点点露出来）\n\n{% asset_img mgnt.png \"格鲁夫给经理人的第一课\" %}\n\n书中有提到的一个观点。作者收集了自己工作中的时间和工作内容，概括起来大概分为四类，在这些事件中，时间的分配变少是最合理的。\n\n{% asset_img mgnt2.png \"传递\" %}\n\n在我们的讨论中，我发现，其实各个公司做得越好，越必须遵循这个原则。如果以这个原则我得到了一些新的想法：\n\n-\t从我们几个所谓技术管理的四个人的思想里，我们坚持认为，管理的目的是为了更好的帮助大家工作成长和提高，而不是为了“分配任务”，这一点我坚持认为是正确的，而且会一直坚持下去。所以，团队的人觉得没人管这个事情是错误的，而是让团队理解，他们不是要被管理，而是自我管理，做和公司方向一致的，并且自己认为争取的事情。有问题向周边人询问，这才是“杏树林”的文化。\n-\t既然是这样，我们是一批支持格鲁夫模型的人，那么问题出在哪儿了？在我看来，我们希望尽量少的做决策，多一点给大家建议，和引导大家的事情，而管理团队目前最缺乏的，反而是上面两个，那就是“传递”和“收集”\n-\t关于“收集”目前还没到这个高度，所以尚待完善，但是关于“传递”这个事情我们却真实应该做许多。其实我们总希望大家主动来问我们问题，但是如果我们没有传递足够强的信号给他们，如果我们传递信息不够多，大家在做事情的时候，就很难实现问问题这件事情，因为有太多他们所不了解的东西。所以你就可以理解，为什么ThoughtWorks，这么在乎每个月的monthly update，在乎buddy／sponser的1on1吃饭，在乎早上的站会，在乎物理墙，包括在乎老大不能有自己的座位，需要到处做，在乎小组一张大桌子进行随时随地的交流。这些都是信息充分传递的表现，只有信息传递了，大家才能拥有互动。（同理，关于OKR，我们也得出了类似的结论，那就是，每一次OKR指定的过程，恰恰是公司自定向下传递信息的过程）因此，信息的快速、健康、准确的传递，是企业管理者面临的重要责任。\n-\t于是乎我想到了关于7-10人管理边界的问题，《How Google Works》里面提到，Google认为管理边界应该不低于7个人，而不是像传统企业不超过7-10个人的上限。我以前的理解是，这是为了扁平组织的需要。但随着理解的深入，这原来是一个非常高的管理要求，说白了，传递效率是人类社会最大的障碍（推荐看三体，或者星际争霸，来了解哲学意交流的方法）。也正是因为此，人类单人传播能力大体在7-10个人之间。如果要实现更佳广泛的传播，就要借助各种“OKR”，“看板”，“monthly update”等等现代管理方法，从而最终实现管理扁平化。\n\n综上总结，管理扁平化，不是一个简单的规定或者想法。身为团队的Lead们，大家肩负着“收集”和“传递”信息的使命。在这方面，我们不应该把责任简单的归结到人不行，不能充分的提问，甚至没有责任心。自己的工作没有做好，才是这些事情的核心。\n\n因此，我自己打算给自己设定一个目标，要在未来的半年里，第一，变成一个大喇叭，不断传播公司各个级别的信息，学习更多的方法，让信息传播更有效率。第二，让周边的管理者们，尤其是技术总监们，管理团队们，更佳立即信息传递的重要性，让公司文化真正得到落实。我理解这也是所谓工程师文化必不可少的组成部分。\n\n\n","source":"_posts/four-things.md","raw":"title: 团队Lead需要关注的四件事\ndate: 2016-03-13 18:22:48\ncategories:\n- Management\ntags:\n- book\n- mgnt\n---\n\n这个周发生了几件小事情。\n\n-\t首先和杨宇、张诚有过一次讨论，觉得事情推进困难，一个随访的小红点问题似乎N久也得不到解决。关于operation和admin等后台不知道谁管也不知道该怎么办。\n-\t在本周由于同事离职反馈了两点信息，公司创业气氛不够，总觉得推动不足，以及责任心的一场讨论。而团队的同事另一方面觉得，自己没人管，没有人安排工作和关心。\n-\t第三个事情是周末一早出现的一个产品事故，业务负责人很难受；另一方面其实我能感觉到，事情相关的技术同事也都是公司数一数二有责任心的人，怎么会出现这个结果呢？\n\n以上的事实，可能很多人都会归结成一句挺伤人心的话，叫做“责任感意识不强”。于是，这个似乎就理所应当成为了一个“人”的问题，更进一步说成了“别人”的问题。但说实话，在我心里，我挺认同文亮常说的那句话，“一个事情成功了找客观原因，失败了一定要找自己的问题”。这和我的为人风格也很像，说白了，任何事情，无论成败，都应该称为自己前进的阶梯，只有这样我们才能从中成长，不断颠覆打败那个曾经的自己。\n\n于是周末和同事的一场讨论，一场回顾会，让我好像悟出来点什么。话题开始与《格鲁夫给经理人的第一课》。这是一本像“德鲁克”系列的经典管理书。（虽然会不屑于一些陈旧的言论，但是经典总归是经典里面的宝藏，在不断的实践中会一点点露出来）\n\n{% asset_img mgnt.png \"格鲁夫给经理人的第一课\" %}\n\n书中有提到的一个观点。作者收集了自己工作中的时间和工作内容，概括起来大概分为四类，在这些事件中，时间的分配变少是最合理的。\n\n{% asset_img mgnt2.png \"传递\" %}\n\n在我们的讨论中，我发现，其实各个公司做得越好，越必须遵循这个原则。如果以这个原则我得到了一些新的想法：\n\n-\t从我们几个所谓技术管理的四个人的思想里，我们坚持认为，管理的目的是为了更好的帮助大家工作成长和提高，而不是为了“分配任务”，这一点我坚持认为是正确的，而且会一直坚持下去。所以，团队的人觉得没人管这个事情是错误的，而是让团队理解，他们不是要被管理，而是自我管理，做和公司方向一致的，并且自己认为争取的事情。有问题向周边人询问，这才是“杏树林”的文化。\n-\t既然是这样，我们是一批支持格鲁夫模型的人，那么问题出在哪儿了？在我看来，我们希望尽量少的做决策，多一点给大家建议，和引导大家的事情，而管理团队目前最缺乏的，反而是上面两个，那就是“传递”和“收集”\n-\t关于“收集”目前还没到这个高度，所以尚待完善，但是关于“传递”这个事情我们却真实应该做许多。其实我们总希望大家主动来问我们问题，但是如果我们没有传递足够强的信号给他们，如果我们传递信息不够多，大家在做事情的时候，就很难实现问问题这件事情，因为有太多他们所不了解的东西。所以你就可以理解，为什么ThoughtWorks，这么在乎每个月的monthly update，在乎buddy／sponser的1on1吃饭，在乎早上的站会，在乎物理墙，包括在乎老大不能有自己的座位，需要到处做，在乎小组一张大桌子进行随时随地的交流。这些都是信息充分传递的表现，只有信息传递了，大家才能拥有互动。（同理，关于OKR，我们也得出了类似的结论，那就是，每一次OKR指定的过程，恰恰是公司自定向下传递信息的过程）因此，信息的快速、健康、准确的传递，是企业管理者面临的重要责任。\n-\t于是乎我想到了关于7-10人管理边界的问题，《How Google Works》里面提到，Google认为管理边界应该不低于7个人，而不是像传统企业不超过7-10个人的上限。我以前的理解是，这是为了扁平组织的需要。但随着理解的深入，这原来是一个非常高的管理要求，说白了，传递效率是人类社会最大的障碍（推荐看三体，或者星际争霸，来了解哲学意交流的方法）。也正是因为此，人类单人传播能力大体在7-10个人之间。如果要实现更佳广泛的传播，就要借助各种“OKR”，“看板”，“monthly update”等等现代管理方法，从而最终实现管理扁平化。\n\n综上总结，管理扁平化，不是一个简单的规定或者想法。身为团队的Lead们，大家肩负着“收集”和“传递”信息的使命。在这方面，我们不应该把责任简单的归结到人不行，不能充分的提问，甚至没有责任心。自己的工作没有做好，才是这些事情的核心。\n\n因此，我自己打算给自己设定一个目标，要在未来的半年里，第一，变成一个大喇叭，不断传播公司各个级别的信息，学习更多的方法，让信息传播更有效率。第二，让周边的管理者们，尤其是技术总监们，管理团队们，更佳立即信息传递的重要性，让公司文化真正得到落实。我理解这也是所谓工程师文化必不可少的组成部分。\n\n\n","slug":"four-things","published":1,"updated":"2018-04-16T09:59:37.083Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoh000frlfy5uagjyb9","content":"<p>这个周发生了几件小事情。</p>\n<ul>\n<li>首先和杨宇、张诚有过一次讨论，觉得事情推进困难，一个随访的小红点问题似乎N久也得不到解决。关于operation和admin等后台不知道谁管也不知道该怎么办。</li>\n<li>在本周由于同事离职反馈了两点信息，公司创业气氛不够，总觉得推动不足，以及责任心的一场讨论。而团队的同事另一方面觉得，自己没人管，没有人安排工作和关心。</li>\n<li>第三个事情是周末一早出现的一个产品事故，业务负责人很难受；另一方面其实我能感觉到，事情相关的技术同事也都是公司数一数二有责任心的人，怎么会出现这个结果呢？</li>\n</ul>\n<p>以上的事实，可能很多人都会归结成一句挺伤人心的话，叫做“责任感意识不强”。于是，这个似乎就理所应当成为了一个“人”的问题，更进一步说成了“别人”的问题。但说实话，在我心里，我挺认同文亮常说的那句话，“一个事情成功了找客观原因，失败了一定要找自己的问题”。这和我的为人风格也很像，说白了，任何事情，无论成败，都应该称为自己前进的阶梯，只有这样我们才能从中成长，不断颠覆打败那个曾经的自己。</p>\n<p>于是周末和同事的一场讨论，一场回顾会，让我好像悟出来点什么。话题开始与《格鲁夫给经理人的第一课》。这是一本像“德鲁克”系列的经典管理书。（虽然会不屑于一些陈旧的言论，但是经典总归是经典里面的宝藏，在不断的实践中会一点点露出来）</p>\n<img src=\"/2016/03/13/four-things/mgnt.png\" title=\"格鲁夫给经理人的第一课\">\n<p>书中有提到的一个观点。作者收集了自己工作中的时间和工作内容，概括起来大概分为四类，在这些事件中，时间的分配变少是最合理的。</p>\n<img src=\"/2016/03/13/four-things/mgnt2.png\" title=\"传递\">\n<p>在我们的讨论中，我发现，其实各个公司做得越好，越必须遵循这个原则。如果以这个原则我得到了一些新的想法：</p>\n<ul>\n<li>从我们几个所谓技术管理的四个人的思想里，我们坚持认为，管理的目的是为了更好的帮助大家工作成长和提高，而不是为了“分配任务”，这一点我坚持认为是正确的，而且会一直坚持下去。所以，团队的人觉得没人管这个事情是错误的，而是让团队理解，他们不是要被管理，而是自我管理，做和公司方向一致的，并且自己认为争取的事情。有问题向周边人询问，这才是“杏树林”的文化。</li>\n<li>既然是这样，我们是一批支持格鲁夫模型的人，那么问题出在哪儿了？在我看来，我们希望尽量少的做决策，多一点给大家建议，和引导大家的事情，而管理团队目前最缺乏的，反而是上面两个，那就是“传递”和“收集”</li>\n<li>关于“收集”目前还没到这个高度，所以尚待完善，但是关于“传递”这个事情我们却真实应该做许多。其实我们总希望大家主动来问我们问题，但是如果我们没有传递足够强的信号给他们，如果我们传递信息不够多，大家在做事情的时候，就很难实现问问题这件事情，因为有太多他们所不了解的东西。所以你就可以理解，为什么ThoughtWorks，这么在乎每个月的monthly update，在乎buddy／sponser的1on1吃饭，在乎早上的站会，在乎物理墙，包括在乎老大不能有自己的座位，需要到处做，在乎小组一张大桌子进行随时随地的交流。这些都是信息充分传递的表现，只有信息传递了，大家才能拥有互动。（同理，关于OKR，我们也得出了类似的结论，那就是，每一次OKR指定的过程，恰恰是公司自定向下传递信息的过程）因此，信息的快速、健康、准确的传递，是企业管理者面临的重要责任。</li>\n<li>于是乎我想到了关于7-10人管理边界的问题，《How Google Works》里面提到，Google认为管理边界应该不低于7个人，而不是像传统企业不超过7-10个人的上限。我以前的理解是，这是为了扁平组织的需要。但随着理解的深入，这原来是一个非常高的管理要求，说白了，传递效率是人类社会最大的障碍（推荐看三体，或者星际争霸，来了解哲学意交流的方法）。也正是因为此，人类单人传播能力大体在7-10个人之间。如果要实现更佳广泛的传播，就要借助各种“OKR”，“看板”，“monthly update”等等现代管理方法，从而最终实现管理扁平化。</li>\n</ul>\n<p>综上总结，管理扁平化，不是一个简单的规定或者想法。身为团队的Lead们，大家肩负着“收集”和“传递”信息的使命。在这方面，我们不应该把责任简单的归结到人不行，不能充分的提问，甚至没有责任心。自己的工作没有做好，才是这些事情的核心。</p>\n<p>因此，我自己打算给自己设定一个目标，要在未来的半年里，第一，变成一个大喇叭，不断传播公司各个级别的信息，学习更多的方法，让信息传播更有效率。第二，让周边的管理者们，尤其是技术总监们，管理团队们，更佳立即信息传递的重要性，让公司文化真正得到落实。我理解这也是所谓工程师文化必不可少的组成部分。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>这个周发生了几件小事情。</p>\n<ul>\n<li>首先和杨宇、张诚有过一次讨论，觉得事情推进困难，一个随访的小红点问题似乎N久也得不到解决。关于operation和admin等后台不知道谁管也不知道该怎么办。</li>\n<li>在本周由于同事离职反馈了两点信息，公司创业气氛不够，总觉得推动不足，以及责任心的一场讨论。而团队的同事另一方面觉得，自己没人管，没有人安排工作和关心。</li>\n<li>第三个事情是周末一早出现的一个产品事故，业务负责人很难受；另一方面其实我能感觉到，事情相关的技术同事也都是公司数一数二有责任心的人，怎么会出现这个结果呢？</li>\n</ul>\n<p>以上的事实，可能很多人都会归结成一句挺伤人心的话，叫做“责任感意识不强”。于是，这个似乎就理所应当成为了一个“人”的问题，更进一步说成了“别人”的问题。但说实话，在我心里，我挺认同文亮常说的那句话，“一个事情成功了找客观原因，失败了一定要找自己的问题”。这和我的为人风格也很像，说白了，任何事情，无论成败，都应该称为自己前进的阶梯，只有这样我们才能从中成长，不断颠覆打败那个曾经的自己。</p>\n<p>于是周末和同事的一场讨论，一场回顾会，让我好像悟出来点什么。话题开始与《格鲁夫给经理人的第一课》。这是一本像“德鲁克”系列的经典管理书。（虽然会不屑于一些陈旧的言论，但是经典总归是经典里面的宝藏，在不断的实践中会一点点露出来）</p>\n<img src=\"/2016/03/13/four-things/mgnt.png\" title=\"格鲁夫给经理人的第一课\">\n<p>书中有提到的一个观点。作者收集了自己工作中的时间和工作内容，概括起来大概分为四类，在这些事件中，时间的分配变少是最合理的。</p>\n<img src=\"/2016/03/13/four-things/mgnt2.png\" title=\"传递\">\n<p>在我们的讨论中，我发现，其实各个公司做得越好，越必须遵循这个原则。如果以这个原则我得到了一些新的想法：</p>\n<ul>\n<li>从我们几个所谓技术管理的四个人的思想里，我们坚持认为，管理的目的是为了更好的帮助大家工作成长和提高，而不是为了“分配任务”，这一点我坚持认为是正确的，而且会一直坚持下去。所以，团队的人觉得没人管这个事情是错误的，而是让团队理解，他们不是要被管理，而是自我管理，做和公司方向一致的，并且自己认为争取的事情。有问题向周边人询问，这才是“杏树林”的文化。</li>\n<li>既然是这样，我们是一批支持格鲁夫模型的人，那么问题出在哪儿了？在我看来，我们希望尽量少的做决策，多一点给大家建议，和引导大家的事情，而管理团队目前最缺乏的，反而是上面两个，那就是“传递”和“收集”</li>\n<li>关于“收集”目前还没到这个高度，所以尚待完善，但是关于“传递”这个事情我们却真实应该做许多。其实我们总希望大家主动来问我们问题，但是如果我们没有传递足够强的信号给他们，如果我们传递信息不够多，大家在做事情的时候，就很难实现问问题这件事情，因为有太多他们所不了解的东西。所以你就可以理解，为什么ThoughtWorks，这么在乎每个月的monthly update，在乎buddy／sponser的1on1吃饭，在乎早上的站会，在乎物理墙，包括在乎老大不能有自己的座位，需要到处做，在乎小组一张大桌子进行随时随地的交流。这些都是信息充分传递的表现，只有信息传递了，大家才能拥有互动。（同理，关于OKR，我们也得出了类似的结论，那就是，每一次OKR指定的过程，恰恰是公司自定向下传递信息的过程）因此，信息的快速、健康、准确的传递，是企业管理者面临的重要责任。</li>\n<li>于是乎我想到了关于7-10人管理边界的问题，《How Google Works》里面提到，Google认为管理边界应该不低于7个人，而不是像传统企业不超过7-10个人的上限。我以前的理解是，这是为了扁平组织的需要。但随着理解的深入，这原来是一个非常高的管理要求，说白了，传递效率是人类社会最大的障碍（推荐看三体，或者星际争霸，来了解哲学意交流的方法）。也正是因为此，人类单人传播能力大体在7-10个人之间。如果要实现更佳广泛的传播，就要借助各种“OKR”，“看板”，“monthly update”等等现代管理方法，从而最终实现管理扁平化。</li>\n</ul>\n<p>综上总结，管理扁平化，不是一个简单的规定或者想法。身为团队的Lead们，大家肩负着“收集”和“传递”信息的使命。在这方面，我们不应该把责任简单的归结到人不行，不能充分的提问，甚至没有责任心。自己的工作没有做好，才是这些事情的核心。</p>\n<p>因此，我自己打算给自己设定一个目标，要在未来的半年里，第一，变成一个大喇叭，不断传播公司各个级别的信息，学习更多的方法，让信息传播更有效率。第二，让周边的管理者们，尤其是技术总监们，管理团队们，更佳立即信息传递的重要性，让公司文化真正得到落实。我理解这也是所谓工程师文化必不可少的组成部分。</p>\n"},{"title":"深度学习第二课第二周算法优化","date":"2018-06-01T11:24:02.000Z","_content":"\n在算法优化这一周的课程里，大纲是这样的\n\n*\tMini-batch gradient descent\n*\tUnderstanding mini-batch gradient\n*\tExponentially weighted averages\n*\tUnderstanding exponentially weighted averages\n*\tBias correction in exponentially weighted averages\n*\tGradient descent with momentum\n*\tRMSprop\n*\tAdam optimization algorithm\n*\tLearning rate decay\n*\tThe problem of local optima\n\n这一周的课程非常连贯，10节课程一气呵成没有任何分段。我们来看一下他们的内在逻辑。先看几个图：\n\n{% asset_img gradient_descent.png [Gradient Descent] %}\n\n在一个标准的Gradient Descent中，下降是非常直接而且直线的，很完美。然而根据本章的描述，这种完美在大数据的情况下会出现一定的问题。那就是每一次迭代update parameter的过程，因为涉及到的样本的数量过于庞大，导致需要做完所有的样本才能实现一次下降。这导致计算效率过低。如何才能提高计算效率呢，那就是争取早一点出结果，让后面的计算早一点站在“前人”的肩膀上工作。于是有了基于mini-batch的算法。\n\n{% asset_img mini_batch_gradient_descent.png [Mini-batch Gradient Descent] %}\n\n这种算法的好处是，虽有下降不再是那么完美的直线，但是它能让数据计算快速产生结果，让参数的更新加快、\n\n在之后的学习中，Andew引入了Exponentially weighted averages概念。说白了，因为mini-batch的引入导致了比较剧烈的震荡，这会让下降的偏移度增加，导致下降到类似cost function指标需要的迭代明显增加了，于是引入了加权平均的概念，帮助缩小振幅，这让mini-batch算法既保留了快速应用前人结果进行下降的优势，又让下降的震荡幅度缩小。随后的momentum和RMSprop以及合体的Adam Optimization，大体就是以上逻辑的算法实现。\n\n最后，章节描述了梯级下降中Learning rate的调优方法以及解释了局部最优困扰为什么是不存在的。","source":"_posts/deeplearning-course2-2-1.md","raw":"title: 深度学习第二课第二周算法优化\ndate: 2018-06-01 19:24:02\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\n---\n\n在算法优化这一周的课程里，大纲是这样的\n\n*\tMini-batch gradient descent\n*\tUnderstanding mini-batch gradient\n*\tExponentially weighted averages\n*\tUnderstanding exponentially weighted averages\n*\tBias correction in exponentially weighted averages\n*\tGradient descent with momentum\n*\tRMSprop\n*\tAdam optimization algorithm\n*\tLearning rate decay\n*\tThe problem of local optima\n\n这一周的课程非常连贯，10节课程一气呵成没有任何分段。我们来看一下他们的内在逻辑。先看几个图：\n\n{% asset_img gradient_descent.png [Gradient Descent] %}\n\n在一个标准的Gradient Descent中，下降是非常直接而且直线的，很完美。然而根据本章的描述，这种完美在大数据的情况下会出现一定的问题。那就是每一次迭代update parameter的过程，因为涉及到的样本的数量过于庞大，导致需要做完所有的样本才能实现一次下降。这导致计算效率过低。如何才能提高计算效率呢，那就是争取早一点出结果，让后面的计算早一点站在“前人”的肩膀上工作。于是有了基于mini-batch的算法。\n\n{% asset_img mini_batch_gradient_descent.png [Mini-batch Gradient Descent] %}\n\n这种算法的好处是，虽有下降不再是那么完美的直线，但是它能让数据计算快速产生结果，让参数的更新加快、\n\n在之后的学习中，Andew引入了Exponentially weighted averages概念。说白了，因为mini-batch的引入导致了比较剧烈的震荡，这会让下降的偏移度增加，导致下降到类似cost function指标需要的迭代明显增加了，于是引入了加权平均的概念，帮助缩小振幅，这让mini-batch算法既保留了快速应用前人结果进行下降的优势，又让下降的震荡幅度缩小。随后的momentum和RMSprop以及合体的Adam Optimization，大体就是以上逻辑的算法实现。\n\n最后，章节描述了梯级下降中Learning rate的调优方法以及解释了局部最优困扰为什么是不存在的。","slug":"deeplearning-course2-2-1","published":1,"updated":"2018-06-02T02:43:12.242Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoh000grlfyinn51ze0","content":"<p>在算法优化这一周的课程里，大纲是这样的</p>\n<ul>\n<li>Mini-batch gradient descent</li>\n<li>Understanding mini-batch gradient</li>\n<li>Exponentially weighted averages</li>\n<li>Understanding exponentially weighted averages</li>\n<li>Bias correction in exponentially weighted averages</li>\n<li>Gradient descent with momentum</li>\n<li>RMSprop</li>\n<li>Adam optimization algorithm</li>\n<li>Learning rate decay</li>\n<li>The problem of local optima</li>\n</ul>\n<p>这一周的课程非常连贯，10节课程一气呵成没有任何分段。我们来看一下他们的内在逻辑。先看几个图：</p>\n<img src=\"/2018/06/01/deeplearning-course2-2-1/gradient_descent.png\" title=\"[Gradient Descent]\">\n<p>在一个标准的Gradient Descent中，下降是非常直接而且直线的，很完美。然而根据本章的描述，这种完美在大数据的情况下会出现一定的问题。那就是每一次迭代update parameter的过程，因为涉及到的样本的数量过于庞大，导致需要做完所有的样本才能实现一次下降。这导致计算效率过低。如何才能提高计算效率呢，那就是争取早一点出结果，让后面的计算早一点站在“前人”的肩膀上工作。于是有了基于mini-batch的算法。</p>\n<img src=\"/2018/06/01/deeplearning-course2-2-1/mini_batch_gradient_descent.png\" title=\"[Mini-batch Gradient Descent]\">\n<p>这种算法的好处是，虽有下降不再是那么完美的直线，但是它能让数据计算快速产生结果，让参数的更新加快、</p>\n<p>在之后的学习中，Andew引入了Exponentially weighted averages概念。说白了，因为mini-batch的引入导致了比较剧烈的震荡，这会让下降的偏移度增加，导致下降到类似cost function指标需要的迭代明显增加了，于是引入了加权平均的概念，帮助缩小振幅，这让mini-batch算法既保留了快速应用前人结果进行下降的优势，又让下降的震荡幅度缩小。随后的momentum和RMSprop以及合体的Adam Optimization，大体就是以上逻辑的算法实现。</p>\n<p>最后，章节描述了梯级下降中Learning rate的调优方法以及解释了局部最优困扰为什么是不存在的。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在算法优化这一周的课程里，大纲是这样的</p>\n<ul>\n<li>Mini-batch gradient descent</li>\n<li>Understanding mini-batch gradient</li>\n<li>Exponentially weighted averages</li>\n<li>Understanding exponentially weighted averages</li>\n<li>Bias correction in exponentially weighted averages</li>\n<li>Gradient descent with momentum</li>\n<li>RMSprop</li>\n<li>Adam optimization algorithm</li>\n<li>Learning rate decay</li>\n<li>The problem of local optima</li>\n</ul>\n<p>这一周的课程非常连贯，10节课程一气呵成没有任何分段。我们来看一下他们的内在逻辑。先看几个图：</p>\n<img src=\"/2018/06/01/deeplearning-course2-2-1/gradient_descent.png\" title=\"[Gradient Descent]\">\n<p>在一个标准的Gradient Descent中，下降是非常直接而且直线的，很完美。然而根据本章的描述，这种完美在大数据的情况下会出现一定的问题。那就是每一次迭代update parameter的过程，因为涉及到的样本的数量过于庞大，导致需要做完所有的样本才能实现一次下降。这导致计算效率过低。如何才能提高计算效率呢，那就是争取早一点出结果，让后面的计算早一点站在“前人”的肩膀上工作。于是有了基于mini-batch的算法。</p>\n<img src=\"/2018/06/01/deeplearning-course2-2-1/mini_batch_gradient_descent.png\" title=\"[Mini-batch Gradient Descent]\">\n<p>这种算法的好处是，虽有下降不再是那么完美的直线，但是它能让数据计算快速产生结果，让参数的更新加快、</p>\n<p>在之后的学习中，Andew引入了Exponentially weighted averages概念。说白了，因为mini-batch的引入导致了比较剧烈的震荡，这会让下降的偏移度增加，导致下降到类似cost function指标需要的迭代明显增加了，于是引入了加权平均的概念，帮助缩小振幅，这让mini-batch算法既保留了快速应用前人结果进行下降的优势，又让下降的震荡幅度缩小。随后的momentum和RMSprop以及合体的Adam Optimization，大体就是以上逻辑的算法实现。</p>\n<p>最后，章节描述了梯级下降中Learning rate的调优方法以及解释了局部最优困扰为什么是不存在的。</p>\n"},{"title":"Hello World","date":"2014-12-01T02:51:30.000Z","_content":"Welcome to [Hexo](http://hexo.io/)! This is your very first post. Check [documentation](http://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [trobuleshooting](http://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](http://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](http://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](http://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](http://hexo.io/docs/deployment.html)","source":"_posts/hello-world.md","raw":"title: Hello World\ndate: 2014-12-01 10:51:30\ncategories:\n- Technology\ntags:\n- tech\n---\nWelcome to [Hexo](http://hexo.io/)! This is your very first post. Check [documentation](http://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [trobuleshooting](http://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](http://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](http://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](http://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](http://hexo.io/docs/deployment.html)","slug":"hello-world","published":1,"updated":"2018-04-16T09:59:37.034Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoj000hrlfyo9pe8xvr","content":"<p>Welcome to <a href=\"http://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"http://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"http://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">trobuleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"http://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"http://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"http://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"http://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"http://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"http://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"http://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">trobuleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"http://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"http://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"http://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"http://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n"},{"title":"GCP搭建serverless","date":"2019-12-02T12:35:12.000Z","_content":"\n之前弄了阿里云的事情，最近因为做公司数据分析到AI的部分，所以必须把GCP彻底整理一遍。首先要澄清，从本身GCP的使用方面自己确实不是专家。但是感谢Google这一年多以来的合作，确实让我对high level的GCP以及相关产品，了解了许多。特别是全套的数据产品服务，也直接导致我来决策了GCP作为数据中心的定位，这也是为什么我需要开始把我的一些分析工具转移到GCP的原因。学习成本也是必须要付出的。\n\n这次转移的部分就是数据分析平台，从原来的RJMetrics转移到以GCP为基础的Data Studio和Tableau上面。既然公司业务已经全面转移了，技术的主要数据分析，我也想一并转进来，并且借这个机会学习一下GCP的几个主要产品，为我在这边这个平台上搭建AI的技术效率分析系统做铺垫，这应该是我最近最大的兴趣所在了。让我们一起动手，消灭我们手里那些反复消磨时间的无意义工作，也许分析业务产生算法，才是程序员，至少是下一代程序员的职责（之后的时代，虽然NLP的普及，也许连分析业务也会被取代，we will see）\n\n##GCP合理使用\n\nGoogle prefer to use Google Cloud SDK. so use gsutil in terminal is much easier to use.\n\n###关于serverless\n\n做这件事情的动因是因为做一个个人工作分析器，内容很简单，就是从wunderlist api里把数据拿出来，然后进行分析。这玩意儿明显就是个serverless。刚好之前在阿里云上研究的也就是serverless，索性用一下，应该蛮cool的。\n\ngcloud的serverless分为两种，code function和code run, 前者可以绑定若干种触发器，比如时间，event等等，后者主要绑定http触发。code function这里和阿里云的函数计算差不多，这里就不多说了。本次主要使用的是code run，一套基于container的方法进行的http serverless。我只能说太为程序员着想了。\n\n之前使用阿里云函数计算最大的问题就是，lib被阿里云绑死了，没法进行扩展。而且有些库，阿里云上就没有，导致必须为了兼容serverless该自己的程序。现在不用了，有了code run，完全是一套自己封闭的环境。requirements.txt随便写，系统帮你填上需要的lib和version。好用！好用！\n\n###Storage的选择\n\n在GCP上面，一共有五种storage，分别是\n+ SQL，这个基本上就是MySQL\n+ Datastore, 可扩展的NoSQL database\n+ Bigtable，这是一个结构化大数据库，是HBase的姊妹形态。如果有TB级别的结构化数据，存在大量写操作，高频low latency的读写要求，使用Bigtable是最合适的。Bigtable使用Hbase shell quickstart.sh来进行 \n+ Storage, 这个主要用于做object的存储，分为standard 99.9%，Durable Reduced Availability 99%, Nearline 99%, 可以通过gsutil上传或下载\n+ BigQuery\n\n[Youtube Video: Chose you storage and database on GCP](https://www.youtube.com/watch?v=mmjuMyRBPO4)\n\n###其他一些内容\n\nCompute Engine，就是VM，创建instances, 每一个instance都可以依据选择访问project下的所有资源。当使用terminal来访问资源的时候，还需要进行必要的配置工作。进入。首先是在SQL里面，给MySQL配置ip\n\n{% asset_img sql_network.png \"给SQL配置IP和访问权限\"\n\n安装gcloud terminal sdk，suprise，这是我非常少有的一次成功不带阻断的精力。认证直接通过sh脚本启动service，调用远程oauth，在网页端完成认证，这种方式很新颖，免除了以前需要.ssh交换秘钥的麻烦。一气呵成，很赞！\n","source":"_posts/gcp-study.md","raw":"title: GCP搭建serverless\ndate: 2019-12-02 20:35:12\ncategories:\n- Technology\ntags:\n- tech\n- cloud\n---\n\n之前弄了阿里云的事情，最近因为做公司数据分析到AI的部分，所以必须把GCP彻底整理一遍。首先要澄清，从本身GCP的使用方面自己确实不是专家。但是感谢Google这一年多以来的合作，确实让我对high level的GCP以及相关产品，了解了许多。特别是全套的数据产品服务，也直接导致我来决策了GCP作为数据中心的定位，这也是为什么我需要开始把我的一些分析工具转移到GCP的原因。学习成本也是必须要付出的。\n\n这次转移的部分就是数据分析平台，从原来的RJMetrics转移到以GCP为基础的Data Studio和Tableau上面。既然公司业务已经全面转移了，技术的主要数据分析，我也想一并转进来，并且借这个机会学习一下GCP的几个主要产品，为我在这边这个平台上搭建AI的技术效率分析系统做铺垫，这应该是我最近最大的兴趣所在了。让我们一起动手，消灭我们手里那些反复消磨时间的无意义工作，也许分析业务产生算法，才是程序员，至少是下一代程序员的职责（之后的时代，虽然NLP的普及，也许连分析业务也会被取代，we will see）\n\n##GCP合理使用\n\nGoogle prefer to use Google Cloud SDK. so use gsutil in terminal is much easier to use.\n\n###关于serverless\n\n做这件事情的动因是因为做一个个人工作分析器，内容很简单，就是从wunderlist api里把数据拿出来，然后进行分析。这玩意儿明显就是个serverless。刚好之前在阿里云上研究的也就是serverless，索性用一下，应该蛮cool的。\n\ngcloud的serverless分为两种，code function和code run, 前者可以绑定若干种触发器，比如时间，event等等，后者主要绑定http触发。code function这里和阿里云的函数计算差不多，这里就不多说了。本次主要使用的是code run，一套基于container的方法进行的http serverless。我只能说太为程序员着想了。\n\n之前使用阿里云函数计算最大的问题就是，lib被阿里云绑死了，没法进行扩展。而且有些库，阿里云上就没有，导致必须为了兼容serverless该自己的程序。现在不用了，有了code run，完全是一套自己封闭的环境。requirements.txt随便写，系统帮你填上需要的lib和version。好用！好用！\n\n###Storage的选择\n\n在GCP上面，一共有五种storage，分别是\n+ SQL，这个基本上就是MySQL\n+ Datastore, 可扩展的NoSQL database\n+ Bigtable，这是一个结构化大数据库，是HBase的姊妹形态。如果有TB级别的结构化数据，存在大量写操作，高频low latency的读写要求，使用Bigtable是最合适的。Bigtable使用Hbase shell quickstart.sh来进行 \n+ Storage, 这个主要用于做object的存储，分为standard 99.9%，Durable Reduced Availability 99%, Nearline 99%, 可以通过gsutil上传或下载\n+ BigQuery\n\n[Youtube Video: Chose you storage and database on GCP](https://www.youtube.com/watch?v=mmjuMyRBPO4)\n\n###其他一些内容\n\nCompute Engine，就是VM，创建instances, 每一个instance都可以依据选择访问project下的所有资源。当使用terminal来访问资源的时候，还需要进行必要的配置工作。进入。首先是在SQL里面，给MySQL配置ip\n\n{% asset_img sql_network.png \"给SQL配置IP和访问权限\"\n\n安装gcloud terminal sdk，suprise，这是我非常少有的一次成功不带阻断的精力。认证直接通过sh脚本启动service，调用远程oauth，在网页端完成认证，这种方式很新颖，免除了以前需要.ssh交换秘钥的麻烦。一气呵成，很赞！\n","slug":"gcp-study","published":1,"updated":"2019-12-05T08:49:09.265Z","_id":"ck3offcol000irlfy7ch67v00","comments":1,"layout":"post","photos":[],"link":"","content":"<p>之前弄了阿里云的事情，最近因为做公司数据分析到AI的部分，所以必须把GCP彻底整理一遍。首先要澄清，从本身GCP的使用方面自己确实不是专家。但是感谢Google这一年多以来的合作，确实让我对high level的GCP以及相关产品，了解了许多。特别是全套的数据产品服务，也直接导致我来决策了GCP作为数据中心的定位，这也是为什么我需要开始把我的一些分析工具转移到GCP的原因。学习成本也是必须要付出的。</p>\n<p>这次转移的部分就是数据分析平台，从原来的RJMetrics转移到以GCP为基础的Data Studio和Tableau上面。既然公司业务已经全面转移了，技术的主要数据分析，我也想一并转进来，并且借这个机会学习一下GCP的几个主要产品，为我在这边这个平台上搭建AI的技术效率分析系统做铺垫，这应该是我最近最大的兴趣所在了。让我们一起动手，消灭我们手里那些反复消磨时间的无意义工作，也许分析业务产生算法，才是程序员，至少是下一代程序员的职责（之后的时代，虽然NLP的普及，也许连分析业务也会被取代，we will see）</p>\n<h2 id=\"GCP合理使用\"><a href=\"#GCP合理使用\" class=\"headerlink\" title=\"GCP合理使用\"></a>GCP合理使用</h2><p>Google prefer to use Google Cloud SDK. so use gsutil in terminal is much easier to use.</p>\n<h3 id=\"关于serverless\"><a href=\"#关于serverless\" class=\"headerlink\" title=\"关于serverless\"></a>关于serverless</h3><p>做这件事情的动因是因为做一个个人工作分析器，内容很简单，就是从wunderlist api里把数据拿出来，然后进行分析。这玩意儿明显就是个serverless。刚好之前在阿里云上研究的也就是serverless，索性用一下，应该蛮cool的。</p>\n<p>gcloud的serverless分为两种，code function和code run, 前者可以绑定若干种触发器，比如时间，event等等，后者主要绑定http触发。code function这里和阿里云的函数计算差不多，这里就不多说了。本次主要使用的是code run，一套基于container的方法进行的http serverless。我只能说太为程序员着想了。</p>\n<p>之前使用阿里云函数计算最大的问题就是，lib被阿里云绑死了，没法进行扩展。而且有些库，阿里云上就没有，导致必须为了兼容serverless该自己的程序。现在不用了，有了code run，完全是一套自己封闭的环境。requirements.txt随便写，系统帮你填上需要的lib和version。好用！好用！</p>\n<h3 id=\"Storage的选择\"><a href=\"#Storage的选择\" class=\"headerlink\" title=\"Storage的选择\"></a>Storage的选择</h3><p>在GCP上面，一共有五种storage，分别是</p>\n<ul>\n<li>SQL，这个基本上就是MySQL</li>\n<li>Datastore, 可扩展的NoSQL database</li>\n<li>Bigtable，这是一个结构化大数据库，是HBase的姊妹形态。如果有TB级别的结构化数据，存在大量写操作，高频low latency的读写要求，使用Bigtable是最合适的。Bigtable使用Hbase shell quickstart.sh来进行 </li>\n<li>Storage, 这个主要用于做object的存储，分为standard 99.9%，Durable Reduced Availability 99%, Nearline 99%, 可以通过gsutil上传或下载</li>\n<li>BigQuery</li>\n</ul>\n<p><a href=\"https://www.youtube.com/watch?v=mmjuMyRBPO4\" target=\"_blank\" rel=\"noopener\">Youtube Video: Chose you storage and database on GCP</a></p>\n<h3 id=\"其他一些内容\"><a href=\"#其他一些内容\" class=\"headerlink\" title=\"其他一些内容\"></a>其他一些内容</h3><p>Compute Engine，就是VM，创建instances, 每一个instance都可以依据选择访问project下的所有资源。当使用terminal来访问资源的时候，还需要进行必要的配置工作。进入。首先是在SQL里面，给MySQL配置ip</p>\n<p><img src=\"/2019/12/02/gcp-study/sql_network.png\" title=\"“给SQL配置IP和访问权限”</p> <p>安装gcloud terminal sdk，suprise，这是我非常少有的一次成功不带阻断的精力。认证直接通过sh脚本启动service，调用远程oauth，在网页端完成认证，这种方式很新颖，免除了以前需要.ssh交换秘钥的麻烦。一气呵成，很赞！</p>\"></p>","site":{"data":{}},"excerpt":"","more":"<p>之前弄了阿里云的事情，最近因为做公司数据分析到AI的部分，所以必须把GCP彻底整理一遍。首先要澄清，从本身GCP的使用方面自己确实不是专家。但是感谢Google这一年多以来的合作，确实让我对high level的GCP以及相关产品，了解了许多。特别是全套的数据产品服务，也直接导致我来决策了GCP作为数据中心的定位，这也是为什么我需要开始把我的一些分析工具转移到GCP的原因。学习成本也是必须要付出的。</p>\n<p>这次转移的部分就是数据分析平台，从原来的RJMetrics转移到以GCP为基础的Data Studio和Tableau上面。既然公司业务已经全面转移了，技术的主要数据分析，我也想一并转进来，并且借这个机会学习一下GCP的几个主要产品，为我在这边这个平台上搭建AI的技术效率分析系统做铺垫，这应该是我最近最大的兴趣所在了。让我们一起动手，消灭我们手里那些反复消磨时间的无意义工作，也许分析业务产生算法，才是程序员，至少是下一代程序员的职责（之后的时代，虽然NLP的普及，也许连分析业务也会被取代，we will see）</p>\n<h2 id=\"GCP合理使用\"><a href=\"#GCP合理使用\" class=\"headerlink\" title=\"GCP合理使用\"></a>GCP合理使用</h2><p>Google prefer to use Google Cloud SDK. so use gsutil in terminal is much easier to use.</p>\n<h3 id=\"关于serverless\"><a href=\"#关于serverless\" class=\"headerlink\" title=\"关于serverless\"></a>关于serverless</h3><p>做这件事情的动因是因为做一个个人工作分析器，内容很简单，就是从wunderlist api里把数据拿出来，然后进行分析。这玩意儿明显就是个serverless。刚好之前在阿里云上研究的也就是serverless，索性用一下，应该蛮cool的。</p>\n<p>gcloud的serverless分为两种，code function和code run, 前者可以绑定若干种触发器，比如时间，event等等，后者主要绑定http触发。code function这里和阿里云的函数计算差不多，这里就不多说了。本次主要使用的是code run，一套基于container的方法进行的http serverless。我只能说太为程序员着想了。</p>\n<p>之前使用阿里云函数计算最大的问题就是，lib被阿里云绑死了，没法进行扩展。而且有些库，阿里云上就没有，导致必须为了兼容serverless该自己的程序。现在不用了，有了code run，完全是一套自己封闭的环境。requirements.txt随便写，系统帮你填上需要的lib和version。好用！好用！</p>\n<h3 id=\"Storage的选择\"><a href=\"#Storage的选择\" class=\"headerlink\" title=\"Storage的选择\"></a>Storage的选择</h3><p>在GCP上面，一共有五种storage，分别是</p>\n<ul>\n<li>SQL，这个基本上就是MySQL</li>\n<li>Datastore, 可扩展的NoSQL database</li>\n<li>Bigtable，这是一个结构化大数据库，是HBase的姊妹形态。如果有TB级别的结构化数据，存在大量写操作，高频low latency的读写要求，使用Bigtable是最合适的。Bigtable使用Hbase shell quickstart.sh来进行 </li>\n<li>Storage, 这个主要用于做object的存储，分为standard 99.9%，Durable Reduced Availability 99%, Nearline 99%, 可以通过gsutil上传或下载</li>\n<li>BigQuery</li>\n</ul>\n<p><a href=\"https://www.youtube.com/watch?v=mmjuMyRBPO4\" target=\"_blank\" rel=\"noopener\">Youtube Video: Chose you storage and database on GCP</a></p>\n<h3 id=\"其他一些内容\"><a href=\"#其他一些内容\" class=\"headerlink\" title=\"其他一些内容\"></a>其他一些内容</h3><p>Compute Engine，就是VM，创建instances, 每一个instance都可以依据选择访问project下的所有资源。当使用terminal来访问资源的时候，还需要进行必要的配置工作。进入。首先是在SQL里面，给MySQL配置ip</p>\n<p><img src=\"/2019/12/02/gcp-study/sql_network.png\" title=\"“给SQL配置IP和访问权限”</p> <p>安装gcloud terminal sdk，suprise，这是我非常少有的一次成功不带阻断的精力。认证直接通过sh脚本启动service，调用远程oauth，在网页端完成认证，这种方式很新颖，免除了以前需要.ssh交换秘钥的麻烦。一气呵成，很赞！</p>\"></p>"},{"title":"深度学习顺序模型第二周","date":"2018-09-08T05:15:35.000Z","mathjax":true,"_content":"\n第二周啦，这一周开始进入更深入的顺序模型的训练，这一周分为了三个部分，一共10课。\n\n# Introduct to word embedding\n\n第一部分是NLP and Word Embeddings，词语的嵌入\n\n## Word Representation 语言表示\n\n根据先前所学，每一个word被表现在一个Vocabulary的词典里面\n```Python\nV=[a, aaron, ..., zulu, <UNK>]  #假设 |V|=10,000\n```\n之前的表示，使用1-hot表达法。比如需要表示“Man”，假设这个词在词汇表的5391位置\n\n$$ Man： O_{5291} =\n\\begin {pmatrix}\n     0 \\\\\n     0 \\\\\n     \\vdots \\\\\n     1 \\\\\n     \\vdots \\\\\n     0 \\\\\n     0 \\\\\n\\end {pmatrix} \n$$\n\n在实际应用场景中，当我们有一个训练模型来预测下一个单次的时候，例如\n```\nI want a glass of orange ____ (juice)\nI want a glass of apple ____\n```\n按照常识，我们可以猜到下一个可能是juice，因为orange juice是比较流行常见的词汇。在训练中，我们当然也是这样做的。这样机器可以知道orange的下一个词可能是juice。然而，如果换成Apple呢？按理说，apple juice应该也是个很组合词汇。按照我们人类的推理，我们大约知道orange和apple是很相近的东西，所以既然有orange juice，大约也就有apple juice。但是根据以目前从训练模型的角度，因为在1-hot的词语表示下，每两个词之间相乘（product）得到的结果都是0。因此在这种情况下，我们说单词与单词之间是没有距离的。也就没有关联性可言，我们无法让机器从orange juice推演出apple后面是juice的预测结果。\n\n\n### Featurized representation: word embedding\n\n这里我们新学了一种方法叫做 word embedding\n{% asset_img featurized.png [featurized representation] %}\n\n每一个单词都会对应有一系列features，比如Man，对应Gender（性别），Royal（皇室），Age（年龄），Food（食物）。把这些feature和Man这个单词的关联关系进行数据化描述，得到一个数组用字母e来表示，比如Man，表示为$e_{5291}$。如图所示，我们可以通过对比$e_{456}$和$e_{4257}$，得到Apple和Orange两者存在较大关联，因此可以得到后面为juice的预测结果。这就是我们说的Word Embeddings\n\n下一个问题相对简单，就是如何可视化word embeddings。因为按照之前的理论，每一个词，有300个维度（假设这里有300个feature）。为了可视化，我们把它降为展开到2D上。这个被叫做t-SNE\n\n## Using word embeddings 使用word embeddings\n\n这一节，主要是学习如何应用word embeddings到NLP，从而完成自然语言模型的训练。还是从例子开始\n\n+\tSally Johnson is an orange farmer\n\n根据前面的学习，我们大概知道了Sally和Johnson是两个名字。根据之前我们的学习，\n\n+\tSally是$x^{<1>}$, Johnsan是$x^{<2>}$, is是$x^{<3>}$, an是$x^{<4>}$, orange是$x^{<5>}$, farmer是$x^{<6>}$\n\n接下来就是通过这个的训练，来生成下面的处理结果\n\n+\tRobert Lin is an apple farmer (a durian cultivator)\n\n通过网上上百万千万的词汇和特性关联，我们尝试寻找到durian和apple与orange之间的关联，以及farmer和cultivator之间的关联性。transfer learning\n\n### Transfer learning and word embeddings\n\n1.\t从大量词汇文集中（1-100B words）学习word embeddings；当然也可以从线上下载一些被pre-trained embedding\n2.\t把这些embedding，通过使用一个相对小的训练集，迁移到新的任务中（比如100k的词汇），在这里我们就可以使用一个小得多的特征向量（比如300个，而不是10000个）\n3.\t可选项：通过新的数据，持续调整（finetune）word embeddings，来改进模型\n\n个人觉得，这个其实很容易理解，我们每个人都在学很多的基础知识，然后因为各种不同的场景，我们需要学习一些上下文。比如同样一个词，在军事领域和民用领域就不一样。这个在Wikipedia里查词的时候非常常见。尤其是缩写\n\nAndrew在这之后介绍了，face encoding（DeepFace）和word embedding的雷同之处。两者都是把一个“object”转化成了一系列特征向量，然后进行对比的方法\n\n## Properties and word embeddings\n\n一个关键词：analogies（类比），这个确实是一种人类很神奇的东西，但这也是NLP应用最重要的东西\n\n接下来主要描述的是如何让机器理解类比。课程中描述了，一个类比，如果Man到Woman，如何类别出King到Queen。文章使用的方法是\n\n$ e_{man}-e_{woman} \\approx \\begin {pmatrix}\n     -2 \\\\\n     0 \\\\\n     0 \\\\\n     0 \\\\\n\\end {pmatrix} \n$ 和 $ e_{king}-e_{quene} \\approx \\begin {pmatrix}\n     -2 \\\\\n     0 \\\\\n     0 \\\\\n     0 \\\\\n\\end {pmatrix} \n$ 最终我们得到 $ (e_{man}-e_{woman}) \\approx (e_{king}-e_{quene})$ 以此来表明类比关系\n\n这个的总结公式是 \n\n$$ Find word(w): arg max_w sim(e_w, e_{king}-e_{man}+e_{woman}) $$\n\n进一步数学化这个公式，来解释$ sim(e_w, e_{king}-e_{man}+e_{woman}) $. 常用的解释方法是Cosine similarity.\n\n$$ sim(u,v) = \\frac{u^Tv}{||u||_2 ||v||_2} $$\n\n用余弦函数来描述sim的数值。根据余弦函数，$cos \\phi$ 是一个在$(1，-1)$区间的值。现实中，也有人使用方差来表示$ ||u-v||^2 $\n\nSome more examples like:\n\n+\tMan:Woman as Boy:Girl\n+ \tOttawa:Canada as Noirobi:Kenya\n+\tBig:Bigger as Tall:Taller\n+\tYen:Japan as Ruble:Russia\n\n## Embedding matrix\n\n$ E \\cdot O_{6257} = \\begin {pmatrix}\n     0 \\\\\n     0 \\\\\n     \\vdots \\\\\n     1 \\\\\n     \\vdots \\\\\n     0 \\\\\n     0 \\\\\n\\end {pmatrix} = e_{6257} = e_{orange}\n$  这是一个 $(300, 1)$ 的矩阵，来表示Orange这个词对应的embeddings\n\nin common，总结一下\n\n$ E \\cdot O_j = e_j $ 等于 embedding for word (j)\n\n因此我们就得到了，对于模型而言，我们的训练目标就是获得这个Embedding Matrix $E$。在Keas里面，事实上使用embedding layer来解决问题，这样更加有效\n\n# Learning Word Embeddings: Word2vec & GloVw\n\n好了，进入第二部分，在上一部分，学习了关于embedding，和模型训练目标Embedding Matrix $E$。这个部分就是来讲述如何训练模型$E$\n\n## Learning Word Embeddings\n\n按照andew的介绍，现在的算法变得越来越简单。但是为了方便和便于理解，介绍还是从相对复杂的算法开始\n\n```\nI       want a glass  of   orange ______\n4242    9665 1 3852  6163   6257\n```\n\nI represent as $ O_{4343} \\longrightarrow E \\longrightarrow e_{4343} $ \nwant represent as $ O_{9665} \\longrightarrow E \\longrightarrow e_{9665} $\na represent as $ O_{1} \\longrightarrow E \\longrightarrow e_{1} $\n\n$ e_{x} $ is a 300 dimentional embedding vector. fill all e into a neural network and then feed to a softmax into a 10000 output vector. neural network with $w^{[1]}$, $b^{[1]}$; softmax parameters are $w^{[2]}$, $b^{[2]}$. the dimensional of neural network is 6 words times 300 dimentional word, which is a 1800 dimentional network layer. also we can decide a window like \"a glass of orange ______\", which removed \"I want\"\n\n接下来文章讲述了不同的context上下文组合方式，列举例子如：\n\n原句是：I want a glass of orange juice to go along with my cereal\n\n+\tLast 4 words (a glass of orange _____)\n+\t4 words on left & right  (a glass of orange _____ to go along with)\n+\tLast 1 word (orange _____)\n\n作者表达了不同的应用上下文学习的方法，如果the goal is just to learn word embedding那么，使用后集中简单方法，被认为已经可以很好地学习到了\n\n## Word2Vec算法\n\n一种跟简单而有效的算法，学习Word Embeddings。先来看一下Skip-grams，依然是刚刚那个句子：\n```\nI want a glass of orange juice to go along with my cereal.\n```\n\n这个算法里面，随机的选取一个word作为context word，例如在上面那个句子里面，我们选择orange作为context word。接下来继续在一个window的去区间里面，选择一个target，比如选择了下一个word，那就是juice，选择之前两个的那个word，那就是glass等等。接下来，对于supervise learning模型而言，以context word为准，让系统去学习预测制定的target，不断校正其对应的W和b参数。\n\n### Model\n\nVocab size = 10000k\n\n$$ ContentC(\"orange\") \\longrightarrow TargetT(\"juice\") $$\n$$ O_c \\longrightarrow E \\longrightarrow e_c \\longrightarrow softmax \\longrightarrow \\hat y $$\n\n这里的softmax是个相对特殊的公式\n\n$$ softmax = \\frac{e^{\\theta^T_te_c}}{\\sum_{j=1}^{10000} e^{\\theta^T_je_c}} $$\n\n关于选择context的问题：\n\n课程中提出，to，the，a，of，for，在英语中是非常常见的词语。因此在随机选择时，会把这些常见词和非常见词分开来，以保证非常见词语，比如apple，orange甚至durian能够被（sampling）采样到。\n\n## Negative Sampling算法\n\n定义一种新形式的supervised learning problem，举例原句不变\n```\nI want a glass of orange juice to go along with my cereal.\n```\n\n接下来会有两组不同的pair\n\n$$ \\begin {matrix}\n     context & word & target \\\\\n     orange & juice & 1 \\\\\n     orange & king & 0 \\\\\n     orange & book & 0 \\\\\n     orange & the & 0 \\\\\n     orange & of & 0 \\\\\n\\end {matrix} $$\n\n第一排，是和之前的算法一样，通过在句子中进行选取，得到的一组pair，我们把这样的pair对应target值写成1，然后把从vacabulary里面随机选择出来的word，比如king，对应的target数值叫做0。事实上，所有随机从vacabulary里面抽取的数值，都会视为0\n\n### Model\n\n$$ p(y=1 | c,t) = \\sigma (\\theta^T_t e_t) $$\n\n$$ o_{6357} \\longrightarrow E \\longrightarrow e_{6357} $$\n\n接下来的意思是，$ e_{6357} $ 和 在vacabulary里面的10000个词汇进行配对，生成10000个0和1的target(logistic classification). 在实际应用中，每一次迭代选择也不是10000个，而是k个，k一般在5-20之间。每次迭代只需要计算k+1个logistic classification就可以了\n\n同样的问题，如何选择negtive example，作者介绍了一种方法，用来做sampling，不过好像就是一种在词汇出现频次的基础上人为调整了数值的方法而已\n\n## GloVe算法\n\n这是本课程介绍learning word embeddings的最后一种算法。这个算法可能并没有Word2Vec和Skip-gram那么普遍使用，但是因为他更简单，所以也值得被介绍一下。\n\nGloVe算法的全称是全向量词语表达(Global Vector for word representation)\n```\nI want a glass of orange juice to go along with my cereal.\n```\n\n之前我们用的c和t来表示配对关系。在GloVe算法里有如下的公式\n\n$$ X_{ij} = #times $$\n\n$ X_{ij} $ 可以表示为i显示在j的上下文中出现的次数。因此类比一下，这里的i可以相当于Word2Vec里面的target(t), j是context(c)\n\n### Model\n\n具体算法暂时不陈述了，因为没太听懂，其实还是使用了上面算法里面的那个 $ \\theta^T_te_c $. 我觉得我在这个地方的确没太明白他代表了什么。需要在复习的时候重新看一下。眼下先把考试过了再说\n\n\n# Applications using Word Embeddings\n\n最后一部分了，主要讲述对于Word Embeddings的应用方法。\n\n## Sentiment Classification\n\n开始没懂什么意思，看图一下子就明白了\n{% asset_img sentiment.png [sentiment classification] %}\n\n### Simple sentiment classification model\n\n```\nThe    desert   is   excellent    # goto 4 stars\n8928    2468   4694    3180\n```\n\n$$ \\begin {matrix}\n     The & o_{8928} & \\longrightarrow & E & \\longrightarrow & e_{8928} \\\\\n     desert & o_{2468} & \\longrightarrow & E & \\longrightarrow & e_{2468} \\\\\n     is & o_{4694} & \\longrightarrow & E & \\longrightarrow & e_{4694} \\\\\n     excellent & o_{3180} & \\longrightarrow & E & \\longrightarrow & e_{3180} \\\\\n\\end {matrix} $$\n\n假设每一个是300个dimentional embeddings. 把这四个词放在一起，就是一个由1200个dimention组成的neural network layer。err，这里好像不太一样，这里用的是sum或avg这300个特征向量，然后把这个扔给softmax，得到1-5的一个y\n\n### RNN for sentiment classification\n\n```\nCompletely lacking in good taste, good service and good ambience    # goto 1 star\n```\n\n对于这样一句话，刚刚的算法表示很无奈。这里面虽然出现了许多的good，但是并不是说好的，而是lacking in。所以如果完全的sum或者avg很难实现 goto 1 star的效果。\n\n使用Many-to-one RNN Achitecture\n{% asset_img rnn.png [RNN for sentiment classification] %}\n\n这里说明使用word embeddings的方法$ e_{4966} $我们训练的是结果可以有更好的类比性，比如lacking和obsent，而不再只是拘泥于一个固定词汇的训练。这让语言有了更强的灵活性。\n\n## Debiasing Word Embeddings\n\n去除偏差，这里指的不是deeplearning里面的bias，与embeddings相关的预测结果的偏差问题。比如\n\n+\tMan:Woman as King:Queen 这个是对的\n+\tMan: Programmer as Woman:Homemaker 这个就不对了\n+\tFather:Doctor as Mother:Nurse 这个也不合适\n\n源自于我们生活的显示，基于性别，信仰，年龄，性别等造成了许多偏差认知，这在人类社会也广泛存在。但是我们并不希望计算机也有这种所谓偏差，甚至“歧视”出现。所以有了本文中所描述的关于如何去除偏差的方法。\n\n## Addressing bias in Word Embeddings\n\n因为内容感觉和我差的有点远，知道就好，所以这里做个特别简要的描述\n\n1.\tIdentifiy bias direction（比如性别，年龄）\n2.\tNeutralize: For every word that is not definitional, project to ge rid of bias\n3.\tEqualize pairs\n\n有篇论文，[Bolukbasi et. al., 2016. Man is to computer programmmer as woman is homemaker?]\n\n\n\n","source":"_posts/deep-learning-sequence-models-w2.md","raw":"title: 深度学习顺序模型第二周\ndate: 2018-09-08 13:15:35\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\nmathjax: true\n---\n\n第二周啦，这一周开始进入更深入的顺序模型的训练，这一周分为了三个部分，一共10课。\n\n# Introduct to word embedding\n\n第一部分是NLP and Word Embeddings，词语的嵌入\n\n## Word Representation 语言表示\n\n根据先前所学，每一个word被表现在一个Vocabulary的词典里面\n```Python\nV=[a, aaron, ..., zulu, <UNK>]  #假设 |V|=10,000\n```\n之前的表示，使用1-hot表达法。比如需要表示“Man”，假设这个词在词汇表的5391位置\n\n$$ Man： O_{5291} =\n\\begin {pmatrix}\n     0 \\\\\n     0 \\\\\n     \\vdots \\\\\n     1 \\\\\n     \\vdots \\\\\n     0 \\\\\n     0 \\\\\n\\end {pmatrix} \n$$\n\n在实际应用场景中，当我们有一个训练模型来预测下一个单次的时候，例如\n```\nI want a glass of orange ____ (juice)\nI want a glass of apple ____\n```\n按照常识，我们可以猜到下一个可能是juice，因为orange juice是比较流行常见的词汇。在训练中，我们当然也是这样做的。这样机器可以知道orange的下一个词可能是juice。然而，如果换成Apple呢？按理说，apple juice应该也是个很组合词汇。按照我们人类的推理，我们大约知道orange和apple是很相近的东西，所以既然有orange juice，大约也就有apple juice。但是根据以目前从训练模型的角度，因为在1-hot的词语表示下，每两个词之间相乘（product）得到的结果都是0。因此在这种情况下，我们说单词与单词之间是没有距离的。也就没有关联性可言，我们无法让机器从orange juice推演出apple后面是juice的预测结果。\n\n\n### Featurized representation: word embedding\n\n这里我们新学了一种方法叫做 word embedding\n{% asset_img featurized.png [featurized representation] %}\n\n每一个单词都会对应有一系列features，比如Man，对应Gender（性别），Royal（皇室），Age（年龄），Food（食物）。把这些feature和Man这个单词的关联关系进行数据化描述，得到一个数组用字母e来表示，比如Man，表示为$e_{5291}$。如图所示，我们可以通过对比$e_{456}$和$e_{4257}$，得到Apple和Orange两者存在较大关联，因此可以得到后面为juice的预测结果。这就是我们说的Word Embeddings\n\n下一个问题相对简单，就是如何可视化word embeddings。因为按照之前的理论，每一个词，有300个维度（假设这里有300个feature）。为了可视化，我们把它降为展开到2D上。这个被叫做t-SNE\n\n## Using word embeddings 使用word embeddings\n\n这一节，主要是学习如何应用word embeddings到NLP，从而完成自然语言模型的训练。还是从例子开始\n\n+\tSally Johnson is an orange farmer\n\n根据前面的学习，我们大概知道了Sally和Johnson是两个名字。根据之前我们的学习，\n\n+\tSally是$x^{<1>}$, Johnsan是$x^{<2>}$, is是$x^{<3>}$, an是$x^{<4>}$, orange是$x^{<5>}$, farmer是$x^{<6>}$\n\n接下来就是通过这个的训练，来生成下面的处理结果\n\n+\tRobert Lin is an apple farmer (a durian cultivator)\n\n通过网上上百万千万的词汇和特性关联，我们尝试寻找到durian和apple与orange之间的关联，以及farmer和cultivator之间的关联性。transfer learning\n\n### Transfer learning and word embeddings\n\n1.\t从大量词汇文集中（1-100B words）学习word embeddings；当然也可以从线上下载一些被pre-trained embedding\n2.\t把这些embedding，通过使用一个相对小的训练集，迁移到新的任务中（比如100k的词汇），在这里我们就可以使用一个小得多的特征向量（比如300个，而不是10000个）\n3.\t可选项：通过新的数据，持续调整（finetune）word embeddings，来改进模型\n\n个人觉得，这个其实很容易理解，我们每个人都在学很多的基础知识，然后因为各种不同的场景，我们需要学习一些上下文。比如同样一个词，在军事领域和民用领域就不一样。这个在Wikipedia里查词的时候非常常见。尤其是缩写\n\nAndrew在这之后介绍了，face encoding（DeepFace）和word embedding的雷同之处。两者都是把一个“object”转化成了一系列特征向量，然后进行对比的方法\n\n## Properties and word embeddings\n\n一个关键词：analogies（类比），这个确实是一种人类很神奇的东西，但这也是NLP应用最重要的东西\n\n接下来主要描述的是如何让机器理解类比。课程中描述了，一个类比，如果Man到Woman，如何类别出King到Queen。文章使用的方法是\n\n$ e_{man}-e_{woman} \\approx \\begin {pmatrix}\n     -2 \\\\\n     0 \\\\\n     0 \\\\\n     0 \\\\\n\\end {pmatrix} \n$ 和 $ e_{king}-e_{quene} \\approx \\begin {pmatrix}\n     -2 \\\\\n     0 \\\\\n     0 \\\\\n     0 \\\\\n\\end {pmatrix} \n$ 最终我们得到 $ (e_{man}-e_{woman}) \\approx (e_{king}-e_{quene})$ 以此来表明类比关系\n\n这个的总结公式是 \n\n$$ Find word(w): arg max_w sim(e_w, e_{king}-e_{man}+e_{woman}) $$\n\n进一步数学化这个公式，来解释$ sim(e_w, e_{king}-e_{man}+e_{woman}) $. 常用的解释方法是Cosine similarity.\n\n$$ sim(u,v) = \\frac{u^Tv}{||u||_2 ||v||_2} $$\n\n用余弦函数来描述sim的数值。根据余弦函数，$cos \\phi$ 是一个在$(1，-1)$区间的值。现实中，也有人使用方差来表示$ ||u-v||^2 $\n\nSome more examples like:\n\n+\tMan:Woman as Boy:Girl\n+ \tOttawa:Canada as Noirobi:Kenya\n+\tBig:Bigger as Tall:Taller\n+\tYen:Japan as Ruble:Russia\n\n## Embedding matrix\n\n$ E \\cdot O_{6257} = \\begin {pmatrix}\n     0 \\\\\n     0 \\\\\n     \\vdots \\\\\n     1 \\\\\n     \\vdots \\\\\n     0 \\\\\n     0 \\\\\n\\end {pmatrix} = e_{6257} = e_{orange}\n$  这是一个 $(300, 1)$ 的矩阵，来表示Orange这个词对应的embeddings\n\nin common，总结一下\n\n$ E \\cdot O_j = e_j $ 等于 embedding for word (j)\n\n因此我们就得到了，对于模型而言，我们的训练目标就是获得这个Embedding Matrix $E$。在Keas里面，事实上使用embedding layer来解决问题，这样更加有效\n\n# Learning Word Embeddings: Word2vec & GloVw\n\n好了，进入第二部分，在上一部分，学习了关于embedding，和模型训练目标Embedding Matrix $E$。这个部分就是来讲述如何训练模型$E$\n\n## Learning Word Embeddings\n\n按照andew的介绍，现在的算法变得越来越简单。但是为了方便和便于理解，介绍还是从相对复杂的算法开始\n\n```\nI       want a glass  of   orange ______\n4242    9665 1 3852  6163   6257\n```\n\nI represent as $ O_{4343} \\longrightarrow E \\longrightarrow e_{4343} $ \nwant represent as $ O_{9665} \\longrightarrow E \\longrightarrow e_{9665} $\na represent as $ O_{1} \\longrightarrow E \\longrightarrow e_{1} $\n\n$ e_{x} $ is a 300 dimentional embedding vector. fill all e into a neural network and then feed to a softmax into a 10000 output vector. neural network with $w^{[1]}$, $b^{[1]}$; softmax parameters are $w^{[2]}$, $b^{[2]}$. the dimensional of neural network is 6 words times 300 dimentional word, which is a 1800 dimentional network layer. also we can decide a window like \"a glass of orange ______\", which removed \"I want\"\n\n接下来文章讲述了不同的context上下文组合方式，列举例子如：\n\n原句是：I want a glass of orange juice to go along with my cereal\n\n+\tLast 4 words (a glass of orange _____)\n+\t4 words on left & right  (a glass of orange _____ to go along with)\n+\tLast 1 word (orange _____)\n\n作者表达了不同的应用上下文学习的方法，如果the goal is just to learn word embedding那么，使用后集中简单方法，被认为已经可以很好地学习到了\n\n## Word2Vec算法\n\n一种跟简单而有效的算法，学习Word Embeddings。先来看一下Skip-grams，依然是刚刚那个句子：\n```\nI want a glass of orange juice to go along with my cereal.\n```\n\n这个算法里面，随机的选取一个word作为context word，例如在上面那个句子里面，我们选择orange作为context word。接下来继续在一个window的去区间里面，选择一个target，比如选择了下一个word，那就是juice，选择之前两个的那个word，那就是glass等等。接下来，对于supervise learning模型而言，以context word为准，让系统去学习预测制定的target，不断校正其对应的W和b参数。\n\n### Model\n\nVocab size = 10000k\n\n$$ ContentC(\"orange\") \\longrightarrow TargetT(\"juice\") $$\n$$ O_c \\longrightarrow E \\longrightarrow e_c \\longrightarrow softmax \\longrightarrow \\hat y $$\n\n这里的softmax是个相对特殊的公式\n\n$$ softmax = \\frac{e^{\\theta^T_te_c}}{\\sum_{j=1}^{10000} e^{\\theta^T_je_c}} $$\n\n关于选择context的问题：\n\n课程中提出，to，the，a，of，for，在英语中是非常常见的词语。因此在随机选择时，会把这些常见词和非常见词分开来，以保证非常见词语，比如apple，orange甚至durian能够被（sampling）采样到。\n\n## Negative Sampling算法\n\n定义一种新形式的supervised learning problem，举例原句不变\n```\nI want a glass of orange juice to go along with my cereal.\n```\n\n接下来会有两组不同的pair\n\n$$ \\begin {matrix}\n     context & word & target \\\\\n     orange & juice & 1 \\\\\n     orange & king & 0 \\\\\n     orange & book & 0 \\\\\n     orange & the & 0 \\\\\n     orange & of & 0 \\\\\n\\end {matrix} $$\n\n第一排，是和之前的算法一样，通过在句子中进行选取，得到的一组pair，我们把这样的pair对应target值写成1，然后把从vacabulary里面随机选择出来的word，比如king，对应的target数值叫做0。事实上，所有随机从vacabulary里面抽取的数值，都会视为0\n\n### Model\n\n$$ p(y=1 | c,t) = \\sigma (\\theta^T_t e_t) $$\n\n$$ o_{6357} \\longrightarrow E \\longrightarrow e_{6357} $$\n\n接下来的意思是，$ e_{6357} $ 和 在vacabulary里面的10000个词汇进行配对，生成10000个0和1的target(logistic classification). 在实际应用中，每一次迭代选择也不是10000个，而是k个，k一般在5-20之间。每次迭代只需要计算k+1个logistic classification就可以了\n\n同样的问题，如何选择negtive example，作者介绍了一种方法，用来做sampling，不过好像就是一种在词汇出现频次的基础上人为调整了数值的方法而已\n\n## GloVe算法\n\n这是本课程介绍learning word embeddings的最后一种算法。这个算法可能并没有Word2Vec和Skip-gram那么普遍使用，但是因为他更简单，所以也值得被介绍一下。\n\nGloVe算法的全称是全向量词语表达(Global Vector for word representation)\n```\nI want a glass of orange juice to go along with my cereal.\n```\n\n之前我们用的c和t来表示配对关系。在GloVe算法里有如下的公式\n\n$$ X_{ij} = #times $$\n\n$ X_{ij} $ 可以表示为i显示在j的上下文中出现的次数。因此类比一下，这里的i可以相当于Word2Vec里面的target(t), j是context(c)\n\n### Model\n\n具体算法暂时不陈述了，因为没太听懂，其实还是使用了上面算法里面的那个 $ \\theta^T_te_c $. 我觉得我在这个地方的确没太明白他代表了什么。需要在复习的时候重新看一下。眼下先把考试过了再说\n\n\n# Applications using Word Embeddings\n\n最后一部分了，主要讲述对于Word Embeddings的应用方法。\n\n## Sentiment Classification\n\n开始没懂什么意思，看图一下子就明白了\n{% asset_img sentiment.png [sentiment classification] %}\n\n### Simple sentiment classification model\n\n```\nThe    desert   is   excellent    # goto 4 stars\n8928    2468   4694    3180\n```\n\n$$ \\begin {matrix}\n     The & o_{8928} & \\longrightarrow & E & \\longrightarrow & e_{8928} \\\\\n     desert & o_{2468} & \\longrightarrow & E & \\longrightarrow & e_{2468} \\\\\n     is & o_{4694} & \\longrightarrow & E & \\longrightarrow & e_{4694} \\\\\n     excellent & o_{3180} & \\longrightarrow & E & \\longrightarrow & e_{3180} \\\\\n\\end {matrix} $$\n\n假设每一个是300个dimentional embeddings. 把这四个词放在一起，就是一个由1200个dimention组成的neural network layer。err，这里好像不太一样，这里用的是sum或avg这300个特征向量，然后把这个扔给softmax，得到1-5的一个y\n\n### RNN for sentiment classification\n\n```\nCompletely lacking in good taste, good service and good ambience    # goto 1 star\n```\n\n对于这样一句话，刚刚的算法表示很无奈。这里面虽然出现了许多的good，但是并不是说好的，而是lacking in。所以如果完全的sum或者avg很难实现 goto 1 star的效果。\n\n使用Many-to-one RNN Achitecture\n{% asset_img rnn.png [RNN for sentiment classification] %}\n\n这里说明使用word embeddings的方法$ e_{4966} $我们训练的是结果可以有更好的类比性，比如lacking和obsent，而不再只是拘泥于一个固定词汇的训练。这让语言有了更强的灵活性。\n\n## Debiasing Word Embeddings\n\n去除偏差，这里指的不是deeplearning里面的bias，与embeddings相关的预测结果的偏差问题。比如\n\n+\tMan:Woman as King:Queen 这个是对的\n+\tMan: Programmer as Woman:Homemaker 这个就不对了\n+\tFather:Doctor as Mother:Nurse 这个也不合适\n\n源自于我们生活的显示，基于性别，信仰，年龄，性别等造成了许多偏差认知，这在人类社会也广泛存在。但是我们并不希望计算机也有这种所谓偏差，甚至“歧视”出现。所以有了本文中所描述的关于如何去除偏差的方法。\n\n## Addressing bias in Word Embeddings\n\n因为内容感觉和我差的有点远，知道就好，所以这里做个特别简要的描述\n\n1.\tIdentifiy bias direction（比如性别，年龄）\n2.\tNeutralize: For every word that is not definitional, project to ge rid of bias\n3.\tEqualize pairs\n\n有篇论文，[Bolukbasi et. al., 2016. Man is to computer programmmer as woman is homemaker?]\n\n\n\n","slug":"deep-learning-sequence-models-w2","published":1,"updated":"2018-09-09T04:55:58.167Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcom000jrlfyietbw3bm","content":"<p>第二周啦，这一周开始进入更深入的顺序模型的训练，这一周分为了三个部分，一共10课。</p>\n<h1 id=\"Introduct-to-word-embedding\"><a href=\"#Introduct-to-word-embedding\" class=\"headerlink\" title=\"Introduct to word embedding\"></a>Introduct to word embedding</h1><p>第一部分是NLP and Word Embeddings，词语的嵌入</p>\n<h2 id=\"Word-Representation-语言表示\"><a href=\"#Word-Representation-语言表示\" class=\"headerlink\" title=\"Word Representation 语言表示\"></a>Word Representation 语言表示</h2><p>根据先前所学，每一个word被表现在一个Vocabulary的词典里面<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">V=[a, aaron, ..., zulu, &lt;UNK&gt;]  <span class=\"comment\">#假设 |V|=10,000</span></span><br></pre></td></tr></table></figure></p>\n<p>之前的表示，使用1-hot表达法。比如需要表示“Man”，假设这个词在词汇表的5391位置</p>\n<script type=\"math/tex; mode=display\">Man： O_{5291} =\n\\begin {pmatrix}\n     0 \\\\\n     0 \\\\\n     \\vdots \\\\\n     1 \\\\\n     \\vdots \\\\\n     0 \\\\\n     0 \\\\\n\\end {pmatrix}</script><p>在实际应用场景中，当我们有一个训练模型来预测下一个单次的时候，例如<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I want a glass of orange ____ (juice)</span><br><span class=\"line\">I want a glass of apple ____</span><br></pre></td></tr></table></figure></p>\n<p>按照常识，我们可以猜到下一个可能是juice，因为orange juice是比较流行常见的词汇。在训练中，我们当然也是这样做的。这样机器可以知道orange的下一个词可能是juice。然而，如果换成Apple呢？按理说，apple juice应该也是个很组合词汇。按照我们人类的推理，我们大约知道orange和apple是很相近的东西，所以既然有orange juice，大约也就有apple juice。但是根据以目前从训练模型的角度，因为在1-hot的词语表示下，每两个词之间相乘（product）得到的结果都是0。因此在这种情况下，我们说单词与单词之间是没有距离的。也就没有关联性可言，我们无法让机器从orange juice推演出apple后面是juice的预测结果。</p>\n<h3 id=\"Featurized-representation-word-embedding\"><a href=\"#Featurized-representation-word-embedding\" class=\"headerlink\" title=\"Featurized representation: word embedding\"></a>Featurized representation: word embedding</h3><p>这里我们新学了一种方法叫做 word embedding<br><img src=\"/2018/09/08/deep-learning-sequence-models-w2/featurized.png\" title=\"[featurized representation]\"></p>\n<p>每一个单词都会对应有一系列features，比如Man，对应Gender（性别），Royal（皇室），Age（年龄），Food（食物）。把这些feature和Man这个单词的关联关系进行数据化描述，得到一个数组用字母e来表示，比如Man，表示为$e_{5291}$。如图所示，我们可以通过对比$e_{456}$和$e_{4257}$，得到Apple和Orange两者存在较大关联，因此可以得到后面为juice的预测结果。这就是我们说的Word Embeddings</p>\n<p>下一个问题相对简单，就是如何可视化word embeddings。因为按照之前的理论，每一个词，有300个维度（假设这里有300个feature）。为了可视化，我们把它降为展开到2D上。这个被叫做t-SNE</p>\n<h2 id=\"Using-word-embeddings-使用word-embeddings\"><a href=\"#Using-word-embeddings-使用word-embeddings\" class=\"headerlink\" title=\"Using word embeddings 使用word embeddings\"></a>Using word embeddings 使用word embeddings</h2><p>这一节，主要是学习如何应用word embeddings到NLP，从而完成自然语言模型的训练。还是从例子开始</p>\n<ul>\n<li>Sally Johnson is an orange farmer</li>\n</ul>\n<p>根据前面的学习，我们大概知道了Sally和Johnson是两个名字。根据之前我们的学习，</p>\n<ul>\n<li>Sally是$x^{<1>}$, Johnsan是$x^{<2>}$, is是$x^{<3>}$, an是$x^{<4>}$, orange是$x^{<5>}$, farmer是$x^{<6>}$</6></5></4></3></2></1></li>\n</ul>\n<p>接下来就是通过这个的训练，来生成下面的处理结果</p>\n<ul>\n<li>Robert Lin is an apple farmer (a durian cultivator)</li>\n</ul>\n<p>通过网上上百万千万的词汇和特性关联，我们尝试寻找到durian和apple与orange之间的关联，以及farmer和cultivator之间的关联性。transfer learning</p>\n<h3 id=\"Transfer-learning-and-word-embeddings\"><a href=\"#Transfer-learning-and-word-embeddings\" class=\"headerlink\" title=\"Transfer learning and word embeddings\"></a>Transfer learning and word embeddings</h3><ol>\n<li>从大量词汇文集中（1-100B words）学习word embeddings；当然也可以从线上下载一些被pre-trained embedding</li>\n<li>把这些embedding，通过使用一个相对小的训练集，迁移到新的任务中（比如100k的词汇），在这里我们就可以使用一个小得多的特征向量（比如300个，而不是10000个）</li>\n<li>可选项：通过新的数据，持续调整（finetune）word embeddings，来改进模型</li>\n</ol>\n<p>个人觉得，这个其实很容易理解，我们每个人都在学很多的基础知识，然后因为各种不同的场景，我们需要学习一些上下文。比如同样一个词，在军事领域和民用领域就不一样。这个在Wikipedia里查词的时候非常常见。尤其是缩写</p>\n<p>Andrew在这之后介绍了，face encoding（DeepFace）和word embedding的雷同之处。两者都是把一个“object”转化成了一系列特征向量，然后进行对比的方法</p>\n<h2 id=\"Properties-and-word-embeddings\"><a href=\"#Properties-and-word-embeddings\" class=\"headerlink\" title=\"Properties and word embeddings\"></a>Properties and word embeddings</h2><p>一个关键词：analogies（类比），这个确实是一种人类很神奇的东西，但这也是NLP应用最重要的东西</p>\n<p>接下来主要描述的是如何让机器理解类比。课程中描述了，一个类比，如果Man到Woman，如何类别出King到Queen。文章使用的方法是</p>\n<p>$ e_{man}-e_{woman} \\approx \\begin {pmatrix}<br>     -2 \\\\<br>     0 \\\\<br>     0 \\\\<br>     0 \\\\<br>\\end {pmatrix}<br>$ 和 $ e_{king}-e_{quene} \\approx \\begin {pmatrix}<br>     -2 \\\\<br>     0 \\\\<br>     0 \\\\<br>     0 \\\\<br>\\end {pmatrix}<br>$ 最终我们得到 $ (e_{man}-e_{woman}) \\approx (e_{king}-e_{quene})$ 以此来表明类比关系</p>\n<p>这个的总结公式是 </p>\n<script type=\"math/tex; mode=display\">Find word(w): arg max_w sim(e_w, e_{king}-e_{man}+e_{woman})</script><p>进一步数学化这个公式，来解释$ sim(e_w, e_{king}-e_{man}+e_{woman}) $. 常用的解释方法是Cosine similarity.</p>\n<script type=\"math/tex; mode=display\">sim(u,v) = \\frac{u^Tv}{||u||_2 ||v||_2}</script><p>用余弦函数来描述sim的数值。根据余弦函数，$cos \\phi$ 是一个在$(1，-1)$区间的值。现实中，也有人使用方差来表示$ ||u-v||^2 $</p>\n<p>Some more examples like:</p>\n<ul>\n<li>Man:Woman as Boy:Girl</li>\n<li>Ottawa:Canada as Noirobi:Kenya</li>\n<li>Big:Bigger as Tall:Taller</li>\n<li>Yen:Japan as Ruble:Russia</li>\n</ul>\n<h2 id=\"Embedding-matrix\"><a href=\"#Embedding-matrix\" class=\"headerlink\" title=\"Embedding matrix\"></a>Embedding matrix</h2><p>$ E \\cdot O_{6257} = \\begin {pmatrix}<br>     0 \\\\<br>     0 \\\\<br>     \\vdots \\\\<br>     1 \\\\<br>     \\vdots \\\\<br>     0 \\\\<br>     0 \\\\<br>\\end {pmatrix} = e_{6257} = e_{orange}<br>$  这是一个 $(300, 1)$ 的矩阵，来表示Orange这个词对应的embeddings</p>\n<p>in common，总结一下</p>\n<p>$ E \\cdot O_j = e_j $ 等于 embedding for word (j)</p>\n<p>因此我们就得到了，对于模型而言，我们的训练目标就是获得这个Embedding Matrix $E$。在Keas里面，事实上使用embedding layer来解决问题，这样更加有效</p>\n<h1 id=\"Learning-Word-Embeddings-Word2vec-amp-GloVw\"><a href=\"#Learning-Word-Embeddings-Word2vec-amp-GloVw\" class=\"headerlink\" title=\"Learning Word Embeddings: Word2vec &amp; GloVw\"></a>Learning Word Embeddings: Word2vec &amp; GloVw</h1><p>好了，进入第二部分，在上一部分，学习了关于embedding，和模型训练目标Embedding Matrix $E$。这个部分就是来讲述如何训练模型$E$</p>\n<h2 id=\"Learning-Word-Embeddings\"><a href=\"#Learning-Word-Embeddings\" class=\"headerlink\" title=\"Learning Word Embeddings\"></a>Learning Word Embeddings</h2><p>按照andew的介绍，现在的算法变得越来越简单。但是为了方便和便于理解，介绍还是从相对复杂的算法开始</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I       want a glass  of   orange ______</span><br><span class=\"line\">4242    9665 1 3852  6163   6257</span><br></pre></td></tr></table></figure>\n<p>I represent as $ O_{4343} \\longrightarrow E \\longrightarrow e_{4343} $<br>want represent as $ O_{9665} \\longrightarrow E \\longrightarrow e_{9665} $<br>a represent as $ O_{1} \\longrightarrow E \\longrightarrow e_{1} $</p>\n<p>$ e_{x} $ is a 300 dimentional embedding vector. fill all e into a neural network and then feed to a softmax into a 10000 output vector. neural network with $w^{[1]}$, $b^{[1]}$; softmax parameters are $w^{[2]}$, $b^{[2]}$. the dimensional of neural network is 6 words times 300 dimentional word, which is a 1800 dimentional network layer. also we can decide a window like “a glass of orange <strong>__</strong>“, which removed “I want”</p>\n<p>接下来文章讲述了不同的context上下文组合方式，列举例子如：</p>\n<p>原句是：I want a glass of orange juice to go along with my cereal</p>\n<ul>\n<li>Last 4 words (a glass of orange <strong>_</strong>)</li>\n<li>4 words on left &amp; right  (a glass of orange <strong>_</strong> to go along with)</li>\n<li>Last 1 word (orange <strong>_</strong>)</li>\n</ul>\n<p>作者表达了不同的应用上下文学习的方法，如果the goal is just to learn word embedding那么，使用后集中简单方法，被认为已经可以很好地学习到了</p>\n<h2 id=\"Word2Vec算法\"><a href=\"#Word2Vec算法\" class=\"headerlink\" title=\"Word2Vec算法\"></a>Word2Vec算法</h2><p>一种跟简单而有效的算法，学习Word Embeddings。先来看一下Skip-grams，依然是刚刚那个句子：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I want a glass of orange juice to go along with my cereal.</span><br></pre></td></tr></table></figure></p>\n<p>这个算法里面，随机的选取一个word作为context word，例如在上面那个句子里面，我们选择orange作为context word。接下来继续在一个window的去区间里面，选择一个target，比如选择了下一个word，那就是juice，选择之前两个的那个word，那就是glass等等。接下来，对于supervise learning模型而言，以context word为准，让系统去学习预测制定的target，不断校正其对应的W和b参数。</p>\n<h3 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h3><p>Vocab size = 10000k</p>\n<script type=\"math/tex; mode=display\">ContentC(\"orange\") \\longrightarrow TargetT(\"juice\")</script><script type=\"math/tex; mode=display\">O_c \\longrightarrow E \\longrightarrow e_c \\longrightarrow softmax \\longrightarrow \\hat y</script><p>这里的softmax是个相对特殊的公式</p>\n<script type=\"math/tex; mode=display\">softmax = \\frac{e^{\\theta^T_te_c}}{\\sum_{j=1}^{10000} e^{\\theta^T_je_c}}</script><p>关于选择context的问题：</p>\n<p>课程中提出，to，the，a，of，for，在英语中是非常常见的词语。因此在随机选择时，会把这些常见词和非常见词分开来，以保证非常见词语，比如apple，orange甚至durian能够被（sampling）采样到。</p>\n<h2 id=\"Negative-Sampling算法\"><a href=\"#Negative-Sampling算法\" class=\"headerlink\" title=\"Negative Sampling算法\"></a>Negative Sampling算法</h2><p>定义一种新形式的supervised learning problem，举例原句不变<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I want a glass of orange juice to go along with my cereal.</span><br></pre></td></tr></table></figure></p>\n<p>接下来会有两组不同的pair</p>\n<script type=\"math/tex; mode=display\">\\begin {matrix}\n     context & word & target \\\\\n     orange & juice & 1 \\\\\n     orange & king & 0 \\\\\n     orange & book & 0 \\\\\n     orange & the & 0 \\\\\n     orange & of & 0 \\\\\n\\end {matrix}</script><p>第一排，是和之前的算法一样，通过在句子中进行选取，得到的一组pair，我们把这样的pair对应target值写成1，然后把从vacabulary里面随机选择出来的word，比如king，对应的target数值叫做0。事实上，所有随机从vacabulary里面抽取的数值，都会视为0</p>\n<h3 id=\"Model-1\"><a href=\"#Model-1\" class=\"headerlink\" title=\"Model\"></a>Model</h3><script type=\"math/tex; mode=display\">p(y=1 | c,t) = \\sigma (\\theta^T_t e_t)</script><script type=\"math/tex; mode=display\">o_{6357} \\longrightarrow E \\longrightarrow e_{6357}</script><p>接下来的意思是，$ e_{6357} $ 和 在vacabulary里面的10000个词汇进行配对，生成10000个0和1的target(logistic classification). 在实际应用中，每一次迭代选择也不是10000个，而是k个，k一般在5-20之间。每次迭代只需要计算k+1个logistic classification就可以了</p>\n<p>同样的问题，如何选择negtive example，作者介绍了一种方法，用来做sampling，不过好像就是一种在词汇出现频次的基础上人为调整了数值的方法而已</p>\n<h2 id=\"GloVe算法\"><a href=\"#GloVe算法\" class=\"headerlink\" title=\"GloVe算法\"></a>GloVe算法</h2><p>这是本课程介绍learning word embeddings的最后一种算法。这个算法可能并没有Word2Vec和Skip-gram那么普遍使用，但是因为他更简单，所以也值得被介绍一下。</p>\n<p>GloVe算法的全称是全向量词语表达(Global Vector for word representation)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I want a glass of orange juice to go along with my cereal.</span><br></pre></td></tr></table></figure></p>\n<p>之前我们用的c和t来表示配对关系。在GloVe算法里有如下的公式</p>\n<script type=\"math/tex; mode=display\">X_{ij} = #times</script><p>$ X_{ij} $ 可以表示为i显示在j的上下文中出现的次数。因此类比一下，这里的i可以相当于Word2Vec里面的target(t), j是context(c)</p>\n<h3 id=\"Model-2\"><a href=\"#Model-2\" class=\"headerlink\" title=\"Model\"></a>Model</h3><p>具体算法暂时不陈述了，因为没太听懂，其实还是使用了上面算法里面的那个 $ \\theta^T_te_c $. 我觉得我在这个地方的确没太明白他代表了什么。需要在复习的时候重新看一下。眼下先把考试过了再说</p>\n<h1 id=\"Applications-using-Word-Embeddings\"><a href=\"#Applications-using-Word-Embeddings\" class=\"headerlink\" title=\"Applications using Word Embeddings\"></a>Applications using Word Embeddings</h1><p>最后一部分了，主要讲述对于Word Embeddings的应用方法。</p>\n<h2 id=\"Sentiment-Classification\"><a href=\"#Sentiment-Classification\" class=\"headerlink\" title=\"Sentiment Classification\"></a>Sentiment Classification</h2><p>开始没懂什么意思，看图一下子就明白了<br><img src=\"/2018/09/08/deep-learning-sequence-models-w2/sentiment.png\" title=\"[sentiment classification]\"></p>\n<h3 id=\"Simple-sentiment-classification-model\"><a href=\"#Simple-sentiment-classification-model\" class=\"headerlink\" title=\"Simple sentiment classification model\"></a>Simple sentiment classification model</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">The    desert   is   excellent    # goto 4 stars</span><br><span class=\"line\">8928    2468   4694    3180</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\begin {matrix}\n     The & o_{8928} & \\longrightarrow & E & \\longrightarrow & e_{8928} \\\\\n     desert & o_{2468} & \\longrightarrow & E & \\longrightarrow & e_{2468} \\\\\n     is & o_{4694} & \\longrightarrow & E & \\longrightarrow & e_{4694} \\\\\n     excellent & o_{3180} & \\longrightarrow & E & \\longrightarrow & e_{3180} \\\\\n\\end {matrix}</script><p>假设每一个是300个dimentional embeddings. 把这四个词放在一起，就是一个由1200个dimention组成的neural network layer。err，这里好像不太一样，这里用的是sum或avg这300个特征向量，然后把这个扔给softmax，得到1-5的一个y</p>\n<h3 id=\"RNN-for-sentiment-classification\"><a href=\"#RNN-for-sentiment-classification\" class=\"headerlink\" title=\"RNN for sentiment classification\"></a>RNN for sentiment classification</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Completely lacking in good taste, good service and good ambience    # goto 1 star</span><br></pre></td></tr></table></figure>\n<p>对于这样一句话，刚刚的算法表示很无奈。这里面虽然出现了许多的good，但是并不是说好的，而是lacking in。所以如果完全的sum或者avg很难实现 goto 1 star的效果。</p>\n<p>使用Many-to-one RNN Achitecture<br><img src=\"/2018/09/08/deep-learning-sequence-models-w2/rnn.png\" title=\"[RNN for sentiment classification]\"></p>\n<p>这里说明使用word embeddings的方法$ e_{4966} $我们训练的是结果可以有更好的类比性，比如lacking和obsent，而不再只是拘泥于一个固定词汇的训练。这让语言有了更强的灵活性。</p>\n<h2 id=\"Debiasing-Word-Embeddings\"><a href=\"#Debiasing-Word-Embeddings\" class=\"headerlink\" title=\"Debiasing Word Embeddings\"></a>Debiasing Word Embeddings</h2><p>去除偏差，这里指的不是deeplearning里面的bias，与embeddings相关的预测结果的偏差问题。比如</p>\n<ul>\n<li>Man:Woman as King:Queen 这个是对的</li>\n<li>Man: Programmer as Woman:Homemaker 这个就不对了</li>\n<li>Father:Doctor as Mother:Nurse 这个也不合适</li>\n</ul>\n<p>源自于我们生活的显示，基于性别，信仰，年龄，性别等造成了许多偏差认知，这在人类社会也广泛存在。但是我们并不希望计算机也有这种所谓偏差，甚至“歧视”出现。所以有了本文中所描述的关于如何去除偏差的方法。</p>\n<h2 id=\"Addressing-bias-in-Word-Embeddings\"><a href=\"#Addressing-bias-in-Word-Embeddings\" class=\"headerlink\" title=\"Addressing bias in Word Embeddings\"></a>Addressing bias in Word Embeddings</h2><p>因为内容感觉和我差的有点远，知道就好，所以这里做个特别简要的描述</p>\n<ol>\n<li>Identifiy bias direction（比如性别，年龄）</li>\n<li>Neutralize: For every word that is not definitional, project to ge rid of bias</li>\n<li>Equalize pairs</li>\n</ol>\n<p>有篇论文，[Bolukbasi et. al., 2016. Man is to computer programmmer as woman is homemaker?]</p>\n","site":{"data":{}},"excerpt":"","more":"<p>第二周啦，这一周开始进入更深入的顺序模型的训练，这一周分为了三个部分，一共10课。</p>\n<h1 id=\"Introduct-to-word-embedding\"><a href=\"#Introduct-to-word-embedding\" class=\"headerlink\" title=\"Introduct to word embedding\"></a>Introduct to word embedding</h1><p>第一部分是NLP and Word Embeddings，词语的嵌入</p>\n<h2 id=\"Word-Representation-语言表示\"><a href=\"#Word-Representation-语言表示\" class=\"headerlink\" title=\"Word Representation 语言表示\"></a>Word Representation 语言表示</h2><p>根据先前所学，每一个word被表现在一个Vocabulary的词典里面<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">V=[a, aaron, ..., zulu, &lt;UNK&gt;]  <span class=\"comment\">#假设 |V|=10,000</span></span><br></pre></td></tr></table></figure></p>\n<p>之前的表示，使用1-hot表达法。比如需要表示“Man”，假设这个词在词汇表的5391位置</p>\n<script type=\"math/tex; mode=display\">Man： O_{5291} =\n\\begin {pmatrix}\n     0 \\\\\n     0 \\\\\n     \\vdots \\\\\n     1 \\\\\n     \\vdots \\\\\n     0 \\\\\n     0 \\\\\n\\end {pmatrix}</script><p>在实际应用场景中，当我们有一个训练模型来预测下一个单次的时候，例如<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I want a glass of orange ____ (juice)</span><br><span class=\"line\">I want a glass of apple ____</span><br></pre></td></tr></table></figure></p>\n<p>按照常识，我们可以猜到下一个可能是juice，因为orange juice是比较流行常见的词汇。在训练中，我们当然也是这样做的。这样机器可以知道orange的下一个词可能是juice。然而，如果换成Apple呢？按理说，apple juice应该也是个很组合词汇。按照我们人类的推理，我们大约知道orange和apple是很相近的东西，所以既然有orange juice，大约也就有apple juice。但是根据以目前从训练模型的角度，因为在1-hot的词语表示下，每两个词之间相乘（product）得到的结果都是0。因此在这种情况下，我们说单词与单词之间是没有距离的。也就没有关联性可言，我们无法让机器从orange juice推演出apple后面是juice的预测结果。</p>\n<h3 id=\"Featurized-representation-word-embedding\"><a href=\"#Featurized-representation-word-embedding\" class=\"headerlink\" title=\"Featurized representation: word embedding\"></a>Featurized representation: word embedding</h3><p>这里我们新学了一种方法叫做 word embedding<br><img src=\"/2018/09/08/deep-learning-sequence-models-w2/featurized.png\" title=\"[featurized representation]\"></p>\n<p>每一个单词都会对应有一系列features，比如Man，对应Gender（性别），Royal（皇室），Age（年龄），Food（食物）。把这些feature和Man这个单词的关联关系进行数据化描述，得到一个数组用字母e来表示，比如Man，表示为$e_{5291}$。如图所示，我们可以通过对比$e_{456}$和$e_{4257}$，得到Apple和Orange两者存在较大关联，因此可以得到后面为juice的预测结果。这就是我们说的Word Embeddings</p>\n<p>下一个问题相对简单，就是如何可视化word embeddings。因为按照之前的理论，每一个词，有300个维度（假设这里有300个feature）。为了可视化，我们把它降为展开到2D上。这个被叫做t-SNE</p>\n<h2 id=\"Using-word-embeddings-使用word-embeddings\"><a href=\"#Using-word-embeddings-使用word-embeddings\" class=\"headerlink\" title=\"Using word embeddings 使用word embeddings\"></a>Using word embeddings 使用word embeddings</h2><p>这一节，主要是学习如何应用word embeddings到NLP，从而完成自然语言模型的训练。还是从例子开始</p>\n<ul>\n<li>Sally Johnson is an orange farmer</li>\n</ul>\n<p>根据前面的学习，我们大概知道了Sally和Johnson是两个名字。根据之前我们的学习，</p>\n<ul>\n<li>Sally是$x^{<1>}$, Johnsan是$x^{<2>}$, is是$x^{<3>}$, an是$x^{<4>}$, orange是$x^{<5>}$, farmer是$x^{<6>}$</6></5></4></3></2></1></li>\n</ul>\n<p>接下来就是通过这个的训练，来生成下面的处理结果</p>\n<ul>\n<li>Robert Lin is an apple farmer (a durian cultivator)</li>\n</ul>\n<p>通过网上上百万千万的词汇和特性关联，我们尝试寻找到durian和apple与orange之间的关联，以及farmer和cultivator之间的关联性。transfer learning</p>\n<h3 id=\"Transfer-learning-and-word-embeddings\"><a href=\"#Transfer-learning-and-word-embeddings\" class=\"headerlink\" title=\"Transfer learning and word embeddings\"></a>Transfer learning and word embeddings</h3><ol>\n<li>从大量词汇文集中（1-100B words）学习word embeddings；当然也可以从线上下载一些被pre-trained embedding</li>\n<li>把这些embedding，通过使用一个相对小的训练集，迁移到新的任务中（比如100k的词汇），在这里我们就可以使用一个小得多的特征向量（比如300个，而不是10000个）</li>\n<li>可选项：通过新的数据，持续调整（finetune）word embeddings，来改进模型</li>\n</ol>\n<p>个人觉得，这个其实很容易理解，我们每个人都在学很多的基础知识，然后因为各种不同的场景，我们需要学习一些上下文。比如同样一个词，在军事领域和民用领域就不一样。这个在Wikipedia里查词的时候非常常见。尤其是缩写</p>\n<p>Andrew在这之后介绍了，face encoding（DeepFace）和word embedding的雷同之处。两者都是把一个“object”转化成了一系列特征向量，然后进行对比的方法</p>\n<h2 id=\"Properties-and-word-embeddings\"><a href=\"#Properties-and-word-embeddings\" class=\"headerlink\" title=\"Properties and word embeddings\"></a>Properties and word embeddings</h2><p>一个关键词：analogies（类比），这个确实是一种人类很神奇的东西，但这也是NLP应用最重要的东西</p>\n<p>接下来主要描述的是如何让机器理解类比。课程中描述了，一个类比，如果Man到Woman，如何类别出King到Queen。文章使用的方法是</p>\n<p>$ e_{man}-e_{woman} \\approx \\begin {pmatrix}<br>     -2 \\\\<br>     0 \\\\<br>     0 \\\\<br>     0 \\\\<br>\\end {pmatrix}<br>$ 和 $ e_{king}-e_{quene} \\approx \\begin {pmatrix}<br>     -2 \\\\<br>     0 \\\\<br>     0 \\\\<br>     0 \\\\<br>\\end {pmatrix}<br>$ 最终我们得到 $ (e_{man}-e_{woman}) \\approx (e_{king}-e_{quene})$ 以此来表明类比关系</p>\n<p>这个的总结公式是 </p>\n<script type=\"math/tex; mode=display\">Find word(w): arg max_w sim(e_w, e_{king}-e_{man}+e_{woman})</script><p>进一步数学化这个公式，来解释$ sim(e_w, e_{king}-e_{man}+e_{woman}) $. 常用的解释方法是Cosine similarity.</p>\n<script type=\"math/tex; mode=display\">sim(u,v) = \\frac{u^Tv}{||u||_2 ||v||_2}</script><p>用余弦函数来描述sim的数值。根据余弦函数，$cos \\phi$ 是一个在$(1，-1)$区间的值。现实中，也有人使用方差来表示$ ||u-v||^2 $</p>\n<p>Some more examples like:</p>\n<ul>\n<li>Man:Woman as Boy:Girl</li>\n<li>Ottawa:Canada as Noirobi:Kenya</li>\n<li>Big:Bigger as Tall:Taller</li>\n<li>Yen:Japan as Ruble:Russia</li>\n</ul>\n<h2 id=\"Embedding-matrix\"><a href=\"#Embedding-matrix\" class=\"headerlink\" title=\"Embedding matrix\"></a>Embedding matrix</h2><p>$ E \\cdot O_{6257} = \\begin {pmatrix}<br>     0 \\\\<br>     0 \\\\<br>     \\vdots \\\\<br>     1 \\\\<br>     \\vdots \\\\<br>     0 \\\\<br>     0 \\\\<br>\\end {pmatrix} = e_{6257} = e_{orange}<br>$  这是一个 $(300, 1)$ 的矩阵，来表示Orange这个词对应的embeddings</p>\n<p>in common，总结一下</p>\n<p>$ E \\cdot O_j = e_j $ 等于 embedding for word (j)</p>\n<p>因此我们就得到了，对于模型而言，我们的训练目标就是获得这个Embedding Matrix $E$。在Keas里面，事实上使用embedding layer来解决问题，这样更加有效</p>\n<h1 id=\"Learning-Word-Embeddings-Word2vec-amp-GloVw\"><a href=\"#Learning-Word-Embeddings-Word2vec-amp-GloVw\" class=\"headerlink\" title=\"Learning Word Embeddings: Word2vec &amp; GloVw\"></a>Learning Word Embeddings: Word2vec &amp; GloVw</h1><p>好了，进入第二部分，在上一部分，学习了关于embedding，和模型训练目标Embedding Matrix $E$。这个部分就是来讲述如何训练模型$E$</p>\n<h2 id=\"Learning-Word-Embeddings\"><a href=\"#Learning-Word-Embeddings\" class=\"headerlink\" title=\"Learning Word Embeddings\"></a>Learning Word Embeddings</h2><p>按照andew的介绍，现在的算法变得越来越简单。但是为了方便和便于理解，介绍还是从相对复杂的算法开始</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I       want a glass  of   orange ______</span><br><span class=\"line\">4242    9665 1 3852  6163   6257</span><br></pre></td></tr></table></figure>\n<p>I represent as $ O_{4343} \\longrightarrow E \\longrightarrow e_{4343} $<br>want represent as $ O_{9665} \\longrightarrow E \\longrightarrow e_{9665} $<br>a represent as $ O_{1} \\longrightarrow E \\longrightarrow e_{1} $</p>\n<p>$ e_{x} $ is a 300 dimentional embedding vector. fill all e into a neural network and then feed to a softmax into a 10000 output vector. neural network with $w^{[1]}$, $b^{[1]}$; softmax parameters are $w^{[2]}$, $b^{[2]}$. the dimensional of neural network is 6 words times 300 dimentional word, which is a 1800 dimentional network layer. also we can decide a window like “a glass of orange <strong>__</strong>“, which removed “I want”</p>\n<p>接下来文章讲述了不同的context上下文组合方式，列举例子如：</p>\n<p>原句是：I want a glass of orange juice to go along with my cereal</p>\n<ul>\n<li>Last 4 words (a glass of orange <strong>_</strong>)</li>\n<li>4 words on left &amp; right  (a glass of orange <strong>_</strong> to go along with)</li>\n<li>Last 1 word (orange <strong>_</strong>)</li>\n</ul>\n<p>作者表达了不同的应用上下文学习的方法，如果the goal is just to learn word embedding那么，使用后集中简单方法，被认为已经可以很好地学习到了</p>\n<h2 id=\"Word2Vec算法\"><a href=\"#Word2Vec算法\" class=\"headerlink\" title=\"Word2Vec算法\"></a>Word2Vec算法</h2><p>一种跟简单而有效的算法，学习Word Embeddings。先来看一下Skip-grams，依然是刚刚那个句子：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I want a glass of orange juice to go along with my cereal.</span><br></pre></td></tr></table></figure></p>\n<p>这个算法里面，随机的选取一个word作为context word，例如在上面那个句子里面，我们选择orange作为context word。接下来继续在一个window的去区间里面，选择一个target，比如选择了下一个word，那就是juice，选择之前两个的那个word，那就是glass等等。接下来，对于supervise learning模型而言，以context word为准，让系统去学习预测制定的target，不断校正其对应的W和b参数。</p>\n<h3 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h3><p>Vocab size = 10000k</p>\n<script type=\"math/tex; mode=display\">ContentC(\"orange\") \\longrightarrow TargetT(\"juice\")</script><script type=\"math/tex; mode=display\">O_c \\longrightarrow E \\longrightarrow e_c \\longrightarrow softmax \\longrightarrow \\hat y</script><p>这里的softmax是个相对特殊的公式</p>\n<script type=\"math/tex; mode=display\">softmax = \\frac{e^{\\theta^T_te_c}}{\\sum_{j=1}^{10000} e^{\\theta^T_je_c}}</script><p>关于选择context的问题：</p>\n<p>课程中提出，to，the，a，of，for，在英语中是非常常见的词语。因此在随机选择时，会把这些常见词和非常见词分开来，以保证非常见词语，比如apple，orange甚至durian能够被（sampling）采样到。</p>\n<h2 id=\"Negative-Sampling算法\"><a href=\"#Negative-Sampling算法\" class=\"headerlink\" title=\"Negative Sampling算法\"></a>Negative Sampling算法</h2><p>定义一种新形式的supervised learning problem，举例原句不变<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I want a glass of orange juice to go along with my cereal.</span><br></pre></td></tr></table></figure></p>\n<p>接下来会有两组不同的pair</p>\n<script type=\"math/tex; mode=display\">\\begin {matrix}\n     context & word & target \\\\\n     orange & juice & 1 \\\\\n     orange & king & 0 \\\\\n     orange & book & 0 \\\\\n     orange & the & 0 \\\\\n     orange & of & 0 \\\\\n\\end {matrix}</script><p>第一排，是和之前的算法一样，通过在句子中进行选取，得到的一组pair，我们把这样的pair对应target值写成1，然后把从vacabulary里面随机选择出来的word，比如king，对应的target数值叫做0。事实上，所有随机从vacabulary里面抽取的数值，都会视为0</p>\n<h3 id=\"Model-1\"><a href=\"#Model-1\" class=\"headerlink\" title=\"Model\"></a>Model</h3><script type=\"math/tex; mode=display\">p(y=1 | c,t) = \\sigma (\\theta^T_t e_t)</script><script type=\"math/tex; mode=display\">o_{6357} \\longrightarrow E \\longrightarrow e_{6357}</script><p>接下来的意思是，$ e_{6357} $ 和 在vacabulary里面的10000个词汇进行配对，生成10000个0和1的target(logistic classification). 在实际应用中，每一次迭代选择也不是10000个，而是k个，k一般在5-20之间。每次迭代只需要计算k+1个logistic classification就可以了</p>\n<p>同样的问题，如何选择negtive example，作者介绍了一种方法，用来做sampling，不过好像就是一种在词汇出现频次的基础上人为调整了数值的方法而已</p>\n<h2 id=\"GloVe算法\"><a href=\"#GloVe算法\" class=\"headerlink\" title=\"GloVe算法\"></a>GloVe算法</h2><p>这是本课程介绍learning word embeddings的最后一种算法。这个算法可能并没有Word2Vec和Skip-gram那么普遍使用，但是因为他更简单，所以也值得被介绍一下。</p>\n<p>GloVe算法的全称是全向量词语表达(Global Vector for word representation)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I want a glass of orange juice to go along with my cereal.</span><br></pre></td></tr></table></figure></p>\n<p>之前我们用的c和t来表示配对关系。在GloVe算法里有如下的公式</p>\n<script type=\"math/tex; mode=display\">X_{ij} = #times</script><p>$ X_{ij} $ 可以表示为i显示在j的上下文中出现的次数。因此类比一下，这里的i可以相当于Word2Vec里面的target(t), j是context(c)</p>\n<h3 id=\"Model-2\"><a href=\"#Model-2\" class=\"headerlink\" title=\"Model\"></a>Model</h3><p>具体算法暂时不陈述了，因为没太听懂，其实还是使用了上面算法里面的那个 $ \\theta^T_te_c $. 我觉得我在这个地方的确没太明白他代表了什么。需要在复习的时候重新看一下。眼下先把考试过了再说</p>\n<h1 id=\"Applications-using-Word-Embeddings\"><a href=\"#Applications-using-Word-Embeddings\" class=\"headerlink\" title=\"Applications using Word Embeddings\"></a>Applications using Word Embeddings</h1><p>最后一部分了，主要讲述对于Word Embeddings的应用方法。</p>\n<h2 id=\"Sentiment-Classification\"><a href=\"#Sentiment-Classification\" class=\"headerlink\" title=\"Sentiment Classification\"></a>Sentiment Classification</h2><p>开始没懂什么意思，看图一下子就明白了<br><img src=\"/2018/09/08/deep-learning-sequence-models-w2/sentiment.png\" title=\"[sentiment classification]\"></p>\n<h3 id=\"Simple-sentiment-classification-model\"><a href=\"#Simple-sentiment-classification-model\" class=\"headerlink\" title=\"Simple sentiment classification model\"></a>Simple sentiment classification model</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">The    desert   is   excellent    # goto 4 stars</span><br><span class=\"line\">8928    2468   4694    3180</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\begin {matrix}\n     The & o_{8928} & \\longrightarrow & E & \\longrightarrow & e_{8928} \\\\\n     desert & o_{2468} & \\longrightarrow & E & \\longrightarrow & e_{2468} \\\\\n     is & o_{4694} & \\longrightarrow & E & \\longrightarrow & e_{4694} \\\\\n     excellent & o_{3180} & \\longrightarrow & E & \\longrightarrow & e_{3180} \\\\\n\\end {matrix}</script><p>假设每一个是300个dimentional embeddings. 把这四个词放在一起，就是一个由1200个dimention组成的neural network layer。err，这里好像不太一样，这里用的是sum或avg这300个特征向量，然后把这个扔给softmax，得到1-5的一个y</p>\n<h3 id=\"RNN-for-sentiment-classification\"><a href=\"#RNN-for-sentiment-classification\" class=\"headerlink\" title=\"RNN for sentiment classification\"></a>RNN for sentiment classification</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Completely lacking in good taste, good service and good ambience    # goto 1 star</span><br></pre></td></tr></table></figure>\n<p>对于这样一句话，刚刚的算法表示很无奈。这里面虽然出现了许多的good，但是并不是说好的，而是lacking in。所以如果完全的sum或者avg很难实现 goto 1 star的效果。</p>\n<p>使用Many-to-one RNN Achitecture<br><img src=\"/2018/09/08/deep-learning-sequence-models-w2/rnn.png\" title=\"[RNN for sentiment classification]\"></p>\n<p>这里说明使用word embeddings的方法$ e_{4966} $我们训练的是结果可以有更好的类比性，比如lacking和obsent，而不再只是拘泥于一个固定词汇的训练。这让语言有了更强的灵活性。</p>\n<h2 id=\"Debiasing-Word-Embeddings\"><a href=\"#Debiasing-Word-Embeddings\" class=\"headerlink\" title=\"Debiasing Word Embeddings\"></a>Debiasing Word Embeddings</h2><p>去除偏差，这里指的不是deeplearning里面的bias，与embeddings相关的预测结果的偏差问题。比如</p>\n<ul>\n<li>Man:Woman as King:Queen 这个是对的</li>\n<li>Man: Programmer as Woman:Homemaker 这个就不对了</li>\n<li>Father:Doctor as Mother:Nurse 这个也不合适</li>\n</ul>\n<p>源自于我们生活的显示，基于性别，信仰，年龄，性别等造成了许多偏差认知，这在人类社会也广泛存在。但是我们并不希望计算机也有这种所谓偏差，甚至“歧视”出现。所以有了本文中所描述的关于如何去除偏差的方法。</p>\n<h2 id=\"Addressing-bias-in-Word-Embeddings\"><a href=\"#Addressing-bias-in-Word-Embeddings\" class=\"headerlink\" title=\"Addressing bias in Word Embeddings\"></a>Addressing bias in Word Embeddings</h2><p>因为内容感觉和我差的有点远，知道就好，所以这里做个特别简要的描述</p>\n<ol>\n<li>Identifiy bias direction（比如性别，年龄）</li>\n<li>Neutralize: For every word that is not definitional, project to ge rid of bias</li>\n<li>Equalize pairs</li>\n</ol>\n<p>有篇论文，[Bolukbasi et. al., 2016. Man is to computer programmmer as woman is homemaker?]</p>\n"},{"title":"ISO27001工作记录之一声叹息","date":"2016-07-03T14:26:22.000Z","_content":"\n因为要和许多中国／国际企业合作，我们需要一套更加完备的信息安全认证。所以这两个礼拜搞ISO27001搞的很心烦，于是写写27001的概念，顺便聊聊认证体系。\n\n+\t关于ISO27001:\n\t\n\tISO27001认证，由英国标准协会（BSI）于1995年2月提出，中文全称《信息安全管理实施细则》（BS7799-1:1995）。它提供了一套综合性的、由信息安全最佳惯例构成的实施细则，目的是为确定各类信息系统通用控制提供唯一的参考基准。1998，提出了第二个部分，中文全称《信息安全管理体系规范》，它规定了信息安全管理体系的要求和对信息安全控制的要求。2005年被ISO国际标准化组织采纳，形成了ISO/IEC 27001:2005。完整的ISO27001包括了11个方面、39个控制目标和133项控制措施。是一个不折不扣让一个互联网人晕到吐血的管理制度。主要11个方向包括：\n\t1.\t安全方针\n\t2.\t信息安全组织\n\t3.\t资产管理\n\t4.\t人力资源安全\n\t5.\t物理和环境安全\n\t6.\t通信和操作管理\n\t7.\t访问控制\n\t8.\t信息系统获取、开发和维护\n\t9.\t信息安全事故管理\n\t10.\t业务连续性管理\n\t11.\t符合性\n\t\n\t通过对整个ISO27001这11个方面的落地，实现一个ISMS(信息安全管理系统)。这里特别说明，这个系统不是互联网概念上的系统，指的是一个体系化的东西，包括了上面所属管理、行政、人事、信息技术等各个方面。详细的就不在这里说了，实在是好多。目前这个工作已经在做了，这里给大家看一下我们的列表，这里面大概60%是技术团队要做的。\n\t\n\t{% asset_img list.png \"杏树林ISO27001工作清单\" %}\n\t\n+\tISO27001与HIPAA\n\t\n\t之前有不少人问HIPAA的事情，现在估计也会有不少人问ISO27991和HIPAA的区别。所以这里简单说明一下。从概念上说，ISO是一个标准，可以由相关评级机构进行评级，办法符合标准的认证证书。这个证书可以用来和企业进行合作，以表明我司在信息安全方面的准备和能力是充分的。而HIPAA是一个法案（也可以说是一个法律），就是每个医疗相关企业遵守是必须的，不遵守如果被举报就要吃官司。哦，对了，这里还要特别说明，ISO和医疗无关，HIPAA是专门为医疗信息保护准备。所以ISO主要讲的是信息保护这个系统工程要做哪些和怎么做；而HIPAA虽然原则上提供了一些强制性的标准和工作方法，但是其核心内容更加强掉哪些信息是禁止泄露的。\n\t\n\t所以说，本次公司的ISO27001认证，我们也是请了相关具有评估资质的公司来进行评审。\n\n+\tISO27001我们所做的和下一步需要做的事情\n\t\n\t下一步主要要做的事情就是完善文档和制度了。这里我想特别说明，之所以我会很重视这件事情，不是为了完善文档和制度，反而是从一个专业技术的角度，想想怎么从里面尽可能找些方法，避免繁琐的文档和制度。在我心里，过于完善的制度和方法，对研发和快速迭代，快速试错本事是有害的。所以，尽最大程度的减少ISO27001对互联网团队的伤害，是我最近想研究的一个话题。有了结果的话，可以再写一篇博文了，呵呵。看一下一片标准的ISO27001文档的样子，像下面这种文档理论上是每次都要写的。做过企业软件的人，应该知道。\n\t\n\t{% asset_img doc_list.png \"ISO27001工作文档清单\" %}\n\t\n\t{% asset_img doc.png \"ISO27001文档内容\" %}\n\n\n总之，ISO27001确实是一个挺麻烦的东西。但是，从企业合作的角度，又不得不去做，并且也可以成为公司系统安全方面比较好的一个PR背书，从这一点上看，质量体系的工作比HIPAA的工作实际上在中国更有公信力。所以吧，对于我来讲对于这东西属于虽然很反感，不屑，但是又知道很有用，不得不去做的东西。希望看到文章的技术兄弟，也是对此表示一种尊敬吧。我自己承认，了解还是有必要的，只是要做多少，大家根据实际情况做到心中有数就好。","source":"_posts/iso27001.md","raw":"title: ISO27001工作记录之一声叹息\ndate: 2016-07-03 22:26:22\ncategories:\n- Technology\ntags:\n- Security\n---\n\n因为要和许多中国／国际企业合作，我们需要一套更加完备的信息安全认证。所以这两个礼拜搞ISO27001搞的很心烦，于是写写27001的概念，顺便聊聊认证体系。\n\n+\t关于ISO27001:\n\t\n\tISO27001认证，由英国标准协会（BSI）于1995年2月提出，中文全称《信息安全管理实施细则》（BS7799-1:1995）。它提供了一套综合性的、由信息安全最佳惯例构成的实施细则，目的是为确定各类信息系统通用控制提供唯一的参考基准。1998，提出了第二个部分，中文全称《信息安全管理体系规范》，它规定了信息安全管理体系的要求和对信息安全控制的要求。2005年被ISO国际标准化组织采纳，形成了ISO/IEC 27001:2005。完整的ISO27001包括了11个方面、39个控制目标和133项控制措施。是一个不折不扣让一个互联网人晕到吐血的管理制度。主要11个方向包括：\n\t1.\t安全方针\n\t2.\t信息安全组织\n\t3.\t资产管理\n\t4.\t人力资源安全\n\t5.\t物理和环境安全\n\t6.\t通信和操作管理\n\t7.\t访问控制\n\t8.\t信息系统获取、开发和维护\n\t9.\t信息安全事故管理\n\t10.\t业务连续性管理\n\t11.\t符合性\n\t\n\t通过对整个ISO27001这11个方面的落地，实现一个ISMS(信息安全管理系统)。这里特别说明，这个系统不是互联网概念上的系统，指的是一个体系化的东西，包括了上面所属管理、行政、人事、信息技术等各个方面。详细的就不在这里说了，实在是好多。目前这个工作已经在做了，这里给大家看一下我们的列表，这里面大概60%是技术团队要做的。\n\t\n\t{% asset_img list.png \"杏树林ISO27001工作清单\" %}\n\t\n+\tISO27001与HIPAA\n\t\n\t之前有不少人问HIPAA的事情，现在估计也会有不少人问ISO27991和HIPAA的区别。所以这里简单说明一下。从概念上说，ISO是一个标准，可以由相关评级机构进行评级，办法符合标准的认证证书。这个证书可以用来和企业进行合作，以表明我司在信息安全方面的准备和能力是充分的。而HIPAA是一个法案（也可以说是一个法律），就是每个医疗相关企业遵守是必须的，不遵守如果被举报就要吃官司。哦，对了，这里还要特别说明，ISO和医疗无关，HIPAA是专门为医疗信息保护准备。所以ISO主要讲的是信息保护这个系统工程要做哪些和怎么做；而HIPAA虽然原则上提供了一些强制性的标准和工作方法，但是其核心内容更加强掉哪些信息是禁止泄露的。\n\t\n\t所以说，本次公司的ISO27001认证，我们也是请了相关具有评估资质的公司来进行评审。\n\n+\tISO27001我们所做的和下一步需要做的事情\n\t\n\t下一步主要要做的事情就是完善文档和制度了。这里我想特别说明，之所以我会很重视这件事情，不是为了完善文档和制度，反而是从一个专业技术的角度，想想怎么从里面尽可能找些方法，避免繁琐的文档和制度。在我心里，过于完善的制度和方法，对研发和快速迭代，快速试错本事是有害的。所以，尽最大程度的减少ISO27001对互联网团队的伤害，是我最近想研究的一个话题。有了结果的话，可以再写一篇博文了，呵呵。看一下一片标准的ISO27001文档的样子，像下面这种文档理论上是每次都要写的。做过企业软件的人，应该知道。\n\t\n\t{% asset_img doc_list.png \"ISO27001工作文档清单\" %}\n\t\n\t{% asset_img doc.png \"ISO27001文档内容\" %}\n\n\n总之，ISO27001确实是一个挺麻烦的东西。但是，从企业合作的角度，又不得不去做，并且也可以成为公司系统安全方面比较好的一个PR背书，从这一点上看，质量体系的工作比HIPAA的工作实际上在中国更有公信力。所以吧，对于我来讲对于这东西属于虽然很反感，不屑，但是又知道很有用，不得不去做的东西。希望看到文章的技术兄弟，也是对此表示一种尊敬吧。我自己承认，了解还是有必要的，只是要做多少，大家根据实际情况做到心中有数就好。","slug":"iso27001","published":1,"updated":"2018-04-16T09:59:37.076Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcon000krlfylh1g08kf","content":"<p>因为要和许多中国／国际企业合作，我们需要一套更加完备的信息安全认证。所以这两个礼拜搞ISO27001搞的很心烦，于是写写27001的概念，顺便聊聊认证体系。</p>\n<ul>\n<li><p>关于ISO27001:</p>\n<p>ISO27001认证，由英国标准协会（BSI）于1995年2月提出，中文全称《信息安全管理实施细则》（BS7799-1:1995）。它提供了一套综合性的、由信息安全最佳惯例构成的实施细则，目的是为确定各类信息系统通用控制提供唯一的参考基准。1998，提出了第二个部分，中文全称《信息安全管理体系规范》，它规定了信息安全管理体系的要求和对信息安全控制的要求。2005年被ISO国际标准化组织采纳，形成了ISO/IEC 27001:2005。完整的ISO27001包括了11个方面、39个控制目标和133项控制措施。是一个不折不扣让一个互联网人晕到吐血的管理制度。主要11个方向包括：</p>\n<ol>\n<li>安全方针</li>\n<li>信息安全组织</li>\n<li>资产管理</li>\n<li>人力资源安全</li>\n<li>物理和环境安全</li>\n<li>通信和操作管理</li>\n<li>访问控制</li>\n<li>信息系统获取、开发和维护</li>\n<li>信息安全事故管理</li>\n<li>业务连续性管理</li>\n<li>符合性</li>\n</ol>\n<p>通过对整个ISO27001这11个方面的落地，实现一个ISMS(信息安全管理系统)。这里特别说明，这个系统不是互联网概念上的系统，指的是一个体系化的东西，包括了上面所属管理、行政、人事、信息技术等各个方面。详细的就不在这里说了，实在是好多。目前这个工作已经在做了，这里给大家看一下我们的列表，这里面大概60%是技术团队要做的。</p>\n<img src=\"/2016/07/03/iso27001/list.png\" title=\"杏树林ISO27001工作清单\">\n</li>\n<li><p>ISO27001与HIPAA</p>\n<p>之前有不少人问HIPAA的事情，现在估计也会有不少人问ISO27991和HIPAA的区别。所以这里简单说明一下。从概念上说，ISO是一个标准，可以由相关评级机构进行评级，办法符合标准的认证证书。这个证书可以用来和企业进行合作，以表明我司在信息安全方面的准备和能力是充分的。而HIPAA是一个法案（也可以说是一个法律），就是每个医疗相关企业遵守是必须的，不遵守如果被举报就要吃官司。哦，对了，这里还要特别说明，ISO和医疗无关，HIPAA是专门为医疗信息保护准备。所以ISO主要讲的是信息保护这个系统工程要做哪些和怎么做；而HIPAA虽然原则上提供了一些强制性的标准和工作方法，但是其核心内容更加强掉哪些信息是禁止泄露的。</p>\n<p>所以说，本次公司的ISO27001认证，我们也是请了相关具有评估资质的公司来进行评审。</p>\n</li>\n<li><p>ISO27001我们所做的和下一步需要做的事情</p>\n<p>下一步主要要做的事情就是完善文档和制度了。这里我想特别说明，之所以我会很重视这件事情，不是为了完善文档和制度，反而是从一个专业技术的角度，想想怎么从里面尽可能找些方法，避免繁琐的文档和制度。在我心里，过于完善的制度和方法，对研发和快速迭代，快速试错本事是有害的。所以，尽最大程度的减少ISO27001对互联网团队的伤害，是我最近想研究的一个话题。有了结果的话，可以再写一篇博文了，呵呵。看一下一片标准的ISO27001文档的样子，像下面这种文档理论上是每次都要写的。做过企业软件的人，应该知道。</p>\n<img src=\"/2016/07/03/iso27001/doc_list.png\" title=\"ISO27001工作文档清单\">\n<img src=\"/2016/07/03/iso27001/doc.png\" title=\"ISO27001文档内容\">\n</li>\n</ul>\n<p>总之，ISO27001确实是一个挺麻烦的东西。但是，从企业合作的角度，又不得不去做，并且也可以成为公司系统安全方面比较好的一个PR背书，从这一点上看，质量体系的工作比HIPAA的工作实际上在中国更有公信力。所以吧，对于我来讲对于这东西属于虽然很反感，不屑，但是又知道很有用，不得不去做的东西。希望看到文章的技术兄弟，也是对此表示一种尊敬吧。我自己承认，了解还是有必要的，只是要做多少，大家根据实际情况做到心中有数就好。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>因为要和许多中国／国际企业合作，我们需要一套更加完备的信息安全认证。所以这两个礼拜搞ISO27001搞的很心烦，于是写写27001的概念，顺便聊聊认证体系。</p>\n<ul>\n<li><p>关于ISO27001:</p>\n<p>ISO27001认证，由英国标准协会（BSI）于1995年2月提出，中文全称《信息安全管理实施细则》（BS7799-1:1995）。它提供了一套综合性的、由信息安全最佳惯例构成的实施细则，目的是为确定各类信息系统通用控制提供唯一的参考基准。1998，提出了第二个部分，中文全称《信息安全管理体系规范》，它规定了信息安全管理体系的要求和对信息安全控制的要求。2005年被ISO国际标准化组织采纳，形成了ISO/IEC 27001:2005。完整的ISO27001包括了11个方面、39个控制目标和133项控制措施。是一个不折不扣让一个互联网人晕到吐血的管理制度。主要11个方向包括：</p>\n<ol>\n<li>安全方针</li>\n<li>信息安全组织</li>\n<li>资产管理</li>\n<li>人力资源安全</li>\n<li>物理和环境安全</li>\n<li>通信和操作管理</li>\n<li>访问控制</li>\n<li>信息系统获取、开发和维护</li>\n<li>信息安全事故管理</li>\n<li>业务连续性管理</li>\n<li>符合性</li>\n</ol>\n<p>通过对整个ISO27001这11个方面的落地，实现一个ISMS(信息安全管理系统)。这里特别说明，这个系统不是互联网概念上的系统，指的是一个体系化的东西，包括了上面所属管理、行政、人事、信息技术等各个方面。详细的就不在这里说了，实在是好多。目前这个工作已经在做了，这里给大家看一下我们的列表，这里面大概60%是技术团队要做的。</p>\n<img src=\"/2016/07/03/iso27001/list.png\" title=\"杏树林ISO27001工作清单\">\n</li>\n<li><p>ISO27001与HIPAA</p>\n<p>之前有不少人问HIPAA的事情，现在估计也会有不少人问ISO27991和HIPAA的区别。所以这里简单说明一下。从概念上说，ISO是一个标准，可以由相关评级机构进行评级，办法符合标准的认证证书。这个证书可以用来和企业进行合作，以表明我司在信息安全方面的准备和能力是充分的。而HIPAA是一个法案（也可以说是一个法律），就是每个医疗相关企业遵守是必须的，不遵守如果被举报就要吃官司。哦，对了，这里还要特别说明，ISO和医疗无关，HIPAA是专门为医疗信息保护准备。所以ISO主要讲的是信息保护这个系统工程要做哪些和怎么做；而HIPAA虽然原则上提供了一些强制性的标准和工作方法，但是其核心内容更加强掉哪些信息是禁止泄露的。</p>\n<p>所以说，本次公司的ISO27001认证，我们也是请了相关具有评估资质的公司来进行评审。</p>\n</li>\n<li><p>ISO27001我们所做的和下一步需要做的事情</p>\n<p>下一步主要要做的事情就是完善文档和制度了。这里我想特别说明，之所以我会很重视这件事情，不是为了完善文档和制度，反而是从一个专业技术的角度，想想怎么从里面尽可能找些方法，避免繁琐的文档和制度。在我心里，过于完善的制度和方法，对研发和快速迭代，快速试错本事是有害的。所以，尽最大程度的减少ISO27001对互联网团队的伤害，是我最近想研究的一个话题。有了结果的话，可以再写一篇博文了，呵呵。看一下一片标准的ISO27001文档的样子，像下面这种文档理论上是每次都要写的。做过企业软件的人，应该知道。</p>\n<img src=\"/2016/07/03/iso27001/doc_list.png\" title=\"ISO27001工作文档清单\">\n<img src=\"/2016/07/03/iso27001/doc.png\" title=\"ISO27001文档内容\">\n</li>\n</ul>\n<p>总之，ISO27001确实是一个挺麻烦的东西。但是，从企业合作的角度，又不得不去做，并且也可以成为公司系统安全方面比较好的一个PR背书，从这一点上看，质量体系的工作比HIPAA的工作实际上在中国更有公信力。所以吧，对于我来讲对于这东西属于虽然很反感，不屑，但是又知道很有用，不得不去做的东西。希望看到文章的技术兄弟，也是对此表示一种尊敬吧。我自己承认，了解还是有必要的，只是要做多少，大家根据实际情况做到心中有数就好。</p>\n"},{"title":"Lesson & Learn","date":"2016-09-13T15:01:00.000Z","_content":"\n2014年，让我引以为自豪的是学会了“运营是中国互联网的核心”\n2015年，让我引以为自豪的是在不懈的努力和好友的陪伴下，组建了“JS端团队”，让杏树林称为具有一线互联网技术的公司\n2016年，希望我可以让数据驱动彻底称为公司的基因，try my best！\n\n一些具体做大事儿的历史记录：\n\n2014年：\n\n+\t产品2周迭代化（发布数据）\n+\t病历夹Crash问题实现完全可控（umeng数据）\n+\t开发自动化（持续打包和发布）\n+\t口袋／文献／病历夹APP产品数据可还原化（每个月的全部流量数据）\n\n2015年：\n\n+\t产品目标的数据化（让迭代更能拥有反馈）\n+\t产品客户反馈的及时列表\n+\t将迭代测试时间从5-8天缩短到3天\n+\t运维的自动化（功能运维中ssh比例）\n\n2016年：\n\n+\t运维体系化（DevOps发展，产品团队自运维）","source":"_posts/milestone.md","raw":"title: Lesson & Learn\ndate: 2016-09-13 23:01:00\ncategories:\n- Diary\ntags:\n- misc\n---\n\n2014年，让我引以为自豪的是学会了“运营是中国互联网的核心”\n2015年，让我引以为自豪的是在不懈的努力和好友的陪伴下，组建了“JS端团队”，让杏树林称为具有一线互联网技术的公司\n2016年，希望我可以让数据驱动彻底称为公司的基因，try my best！\n\n一些具体做大事儿的历史记录：\n\n2014年：\n\n+\t产品2周迭代化（发布数据）\n+\t病历夹Crash问题实现完全可控（umeng数据）\n+\t开发自动化（持续打包和发布）\n+\t口袋／文献／病历夹APP产品数据可还原化（每个月的全部流量数据）\n\n2015年：\n\n+\t产品目标的数据化（让迭代更能拥有反馈）\n+\t产品客户反馈的及时列表\n+\t将迭代测试时间从5-8天缩短到3天\n+\t运维的自动化（功能运维中ssh比例）\n\n2016年：\n\n+\t运维体系化（DevOps发展，产品团队自运维）","slug":"milestone","published":1,"updated":"2018-04-16T09:59:37.071Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoo000lrlfyv3ukx06r","content":"<p>2014年，让我引以为自豪的是学会了“运营是中国互联网的核心”<br>2015年，让我引以为自豪的是在不懈的努力和好友的陪伴下，组建了“JS端团队”，让杏树林称为具有一线互联网技术的公司<br>2016年，希望我可以让数据驱动彻底称为公司的基因，try my best！</p>\n<p>一些具体做大事儿的历史记录：</p>\n<p>2014年：</p>\n<ul>\n<li>产品2周迭代化（发布数据）</li>\n<li>病历夹Crash问题实现完全可控（umeng数据）</li>\n<li>开发自动化（持续打包和发布）</li>\n<li>口袋／文献／病历夹APP产品数据可还原化（每个月的全部流量数据）</li>\n</ul>\n<p>2015年：</p>\n<ul>\n<li>产品目标的数据化（让迭代更能拥有反馈）</li>\n<li>产品客户反馈的及时列表</li>\n<li>将迭代测试时间从5-8天缩短到3天</li>\n<li>运维的自动化（功能运维中ssh比例）</li>\n</ul>\n<p>2016年：</p>\n<ul>\n<li>运维体系化（DevOps发展，产品团队自运维）</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>2014年，让我引以为自豪的是学会了“运营是中国互联网的核心”<br>2015年，让我引以为自豪的是在不懈的努力和好友的陪伴下，组建了“JS端团队”，让杏树林称为具有一线互联网技术的公司<br>2016年，希望我可以让数据驱动彻底称为公司的基因，try my best！</p>\n<p>一些具体做大事儿的历史记录：</p>\n<p>2014年：</p>\n<ul>\n<li>产品2周迭代化（发布数据）</li>\n<li>病历夹Crash问题实现完全可控（umeng数据）</li>\n<li>开发自动化（持续打包和发布）</li>\n<li>口袋／文献／病历夹APP产品数据可还原化（每个月的全部流量数据）</li>\n</ul>\n<p>2015年：</p>\n<ul>\n<li>产品目标的数据化（让迭代更能拥有反馈）</li>\n<li>产品客户反馈的及时列表</li>\n<li>将迭代测试时间从5-8天缩短到3天</li>\n<li>运维的自动化（功能运维中ssh比例）</li>\n</ul>\n<p>2016年：</p>\n<ul>\n<li>运维体系化（DevOps发展，产品团队自运维）</li>\n</ul>\n"},{"title":"IT背景的人想做移动医疗应该怎么做准备？","date":"2016-04-25T05:39:48.000Z","_content":"\n**刚好在回答一个知乎话题，但是觉得挺值的分享一下的，供有兴趣的童靴，尤其是开始关心业务的童靴参考**\n\nIT背景的人想做移动医疗应该怎么做准备？我觉得其实最重要的就是同理心。IT技术是一个用作所有行业皆可的东西，但是正因为他的“干什么都行”，也就决定了如果不是爱自己所做的产品或者方向的话，也很难把一个行业做好做好。前几天，碰到一个做女性社区的CTO，聊起来说自己的团队不喜欢和产品一次思考，不喜欢往前跨一步去为用户考虑。当然我们也交流了许多执行层面的方法，但我问他了一个我很看重的话题，就是“作为一个男人来讲，你对帮助身边的女性解决她们（比如化妆、美甲等）的问题是否有兴趣”。他沉默，没有给我一个明确的答案，其实我大概也能猜的出来，至少我自己不是很有兴趣吧。那我说，如果作为技术的最高负责人，你对所做的业务没有力图改变的兴趣，那很难说你的技术团队能真正感兴趣于为用户提供服务。不如做点技术的事情，让团队先对技术产生兴趣吧，虽然这件事的投入产出比相对比于前者有点低，但也算是一种选择。\n\n就我而言，对医生的同理心感受从很小就有了，各种因素不多说了。总是医生和技术人员一样，是工匠文化的典型代表，说深一点是Geek文化的典型代表。为啥？因为技术人员可以自己写各种好玩的应用，做Demo，但是脱离了业务，其实技术本身什么也不是。同理，医生也一样，你看医生自己可以做实验，研究片子，研究药理等等吧，但是本身医生离开了病人，其实也什么都不是。无论医生宣称自己有再高的成就，如果他没治好一个病人，恐怕没人相信。同理，一个技术人员无论宣称自己有在高的技能，做出fancy的页面，通过高压力的测试，外人也很难确信他所做的工作有多牛。说“医生是把患者背过河”，其实技术也一样“是把一个个严峻痛苦中的业务背过河”。说的好像有点高尚，但这就是医生，这就是技术。技术领域有活跃的Geek，也有混口饭吃的，甚至被迫从事这个行业的普通人。同理，医生领域也一样，只不过他们有两个更加对应的名字“神医”和“庸医”。\n\n所以，我深信医生和技术在工匠文化的表现上是如此的一致。也是因为此，我相信，既然Github，Stack Overflow，Heroku，Amazon Cloud成就了一批又一批技术，让技术迈向新的台阶，那么我相信医生也可以有属于自己的医疗自动化存储、研究、交流、服务平台。所以，我做了病历夹，就像Github一样让医生存储自己的“代码”，让感兴趣的人互相交流，做了诊疗圈，让医生可以互相提问实现能力提升。我还会做更多，做更多让医生这群工匠变得更加方便的事情，成就更多医疗领域的Geek。我解决不了每个医生的工作问题，但我相信通过我的努力，能够让更多人知道、辨别什么人是好医生，是爱医疗这门学科的医生。这就好像我们现在招聘必看面试者的Github和博客账号一样。\n\n如果这个问题是在问要不要投身于医疗这个行业，那么请准备好同理心，和真的想要颠覆一个行业去解脱那些痛苦中的医生们，让好医生获得人们更广泛的尊重，得到本来就应该属于他们的名誉和利益，让庸医无地自容。因为帮助了他们，也就帮助了作为患者的我们自己。这里多举一个身边的例子，我的一个同事，好兄弟，家里姨妈因为胃痛去了一家医院，医生针对胃炎给了些药打发回家了。偶然的机会，姨妈陪兄弟的妈妈检查癌症，于是在子女的要求下，自己也做了个检查，结果发现是早起胃癌，好在发现的早，应该无大碍。我们每个人，不可能每天去医院挂专家。但是如果这类庸医变得越来越少，我想至少他能想到让这位姨妈去做个检查；如果这类庸医变得越来越少，也许我们的兄弟姐妹父母子女会多一分生存的机会。\n\n说回到话题，如果关注点不在医疗这件事情，也没什么对错的；那就关注在技术本身，医疗领域里面好玩有趣的技术也很多，比如和很多其他行业类似的用户体验方面，数据处理方面等等，可以做出很多不错的方面事情。但至于具体是不是要投入移动医疗，或者什么其他行业，那就没有什么特别的了。\n\n","source":"_posts/it-in-med.md","raw":"title: IT背景的人想做移动医疗应该怎么做准备？\ndate: 2016-04-25 13:39:48\ncategories:\n- Technology\ntags:\n- tech\n- career\n---\n\n**刚好在回答一个知乎话题，但是觉得挺值的分享一下的，供有兴趣的童靴，尤其是开始关心业务的童靴参考**\n\nIT背景的人想做移动医疗应该怎么做准备？我觉得其实最重要的就是同理心。IT技术是一个用作所有行业皆可的东西，但是正因为他的“干什么都行”，也就决定了如果不是爱自己所做的产品或者方向的话，也很难把一个行业做好做好。前几天，碰到一个做女性社区的CTO，聊起来说自己的团队不喜欢和产品一次思考，不喜欢往前跨一步去为用户考虑。当然我们也交流了许多执行层面的方法，但我问他了一个我很看重的话题，就是“作为一个男人来讲，你对帮助身边的女性解决她们（比如化妆、美甲等）的问题是否有兴趣”。他沉默，没有给我一个明确的答案，其实我大概也能猜的出来，至少我自己不是很有兴趣吧。那我说，如果作为技术的最高负责人，你对所做的业务没有力图改变的兴趣，那很难说你的技术团队能真正感兴趣于为用户提供服务。不如做点技术的事情，让团队先对技术产生兴趣吧，虽然这件事的投入产出比相对比于前者有点低，但也算是一种选择。\n\n就我而言，对医生的同理心感受从很小就有了，各种因素不多说了。总是医生和技术人员一样，是工匠文化的典型代表，说深一点是Geek文化的典型代表。为啥？因为技术人员可以自己写各种好玩的应用，做Demo，但是脱离了业务，其实技术本身什么也不是。同理，医生也一样，你看医生自己可以做实验，研究片子，研究药理等等吧，但是本身医生离开了病人，其实也什么都不是。无论医生宣称自己有再高的成就，如果他没治好一个病人，恐怕没人相信。同理，一个技术人员无论宣称自己有在高的技能，做出fancy的页面，通过高压力的测试，外人也很难确信他所做的工作有多牛。说“医生是把患者背过河”，其实技术也一样“是把一个个严峻痛苦中的业务背过河”。说的好像有点高尚，但这就是医生，这就是技术。技术领域有活跃的Geek，也有混口饭吃的，甚至被迫从事这个行业的普通人。同理，医生领域也一样，只不过他们有两个更加对应的名字“神医”和“庸医”。\n\n所以，我深信医生和技术在工匠文化的表现上是如此的一致。也是因为此，我相信，既然Github，Stack Overflow，Heroku，Amazon Cloud成就了一批又一批技术，让技术迈向新的台阶，那么我相信医生也可以有属于自己的医疗自动化存储、研究、交流、服务平台。所以，我做了病历夹，就像Github一样让医生存储自己的“代码”，让感兴趣的人互相交流，做了诊疗圈，让医生可以互相提问实现能力提升。我还会做更多，做更多让医生这群工匠变得更加方便的事情，成就更多医疗领域的Geek。我解决不了每个医生的工作问题，但我相信通过我的努力，能够让更多人知道、辨别什么人是好医生，是爱医疗这门学科的医生。这就好像我们现在招聘必看面试者的Github和博客账号一样。\n\n如果这个问题是在问要不要投身于医疗这个行业，那么请准备好同理心，和真的想要颠覆一个行业去解脱那些痛苦中的医生们，让好医生获得人们更广泛的尊重，得到本来就应该属于他们的名誉和利益，让庸医无地自容。因为帮助了他们，也就帮助了作为患者的我们自己。这里多举一个身边的例子，我的一个同事，好兄弟，家里姨妈因为胃痛去了一家医院，医生针对胃炎给了些药打发回家了。偶然的机会，姨妈陪兄弟的妈妈检查癌症，于是在子女的要求下，自己也做了个检查，结果发现是早起胃癌，好在发现的早，应该无大碍。我们每个人，不可能每天去医院挂专家。但是如果这类庸医变得越来越少，我想至少他能想到让这位姨妈去做个检查；如果这类庸医变得越来越少，也许我们的兄弟姐妹父母子女会多一分生存的机会。\n\n说回到话题，如果关注点不在医疗这件事情，也没什么对错的；那就关注在技术本身，医疗领域里面好玩有趣的技术也很多，比如和很多其他行业类似的用户体验方面，数据处理方面等等，可以做出很多不错的方面事情。但至于具体是不是要投入移动医疗，或者什么其他行业，那就没有什么特别的了。\n\n","slug":"it-in-med","published":1,"updated":"2018-04-16T09:59:37.037Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcop000mrlfyd7enfz6c","content":"<p><strong>刚好在回答一个知乎话题，但是觉得挺值的分享一下的，供有兴趣的童靴，尤其是开始关心业务的童靴参考</strong></p>\n<p>IT背景的人想做移动医疗应该怎么做准备？我觉得其实最重要的就是同理心。IT技术是一个用作所有行业皆可的东西，但是正因为他的“干什么都行”，也就决定了如果不是爱自己所做的产品或者方向的话，也很难把一个行业做好做好。前几天，碰到一个做女性社区的CTO，聊起来说自己的团队不喜欢和产品一次思考，不喜欢往前跨一步去为用户考虑。当然我们也交流了许多执行层面的方法，但我问他了一个我很看重的话题，就是“作为一个男人来讲，你对帮助身边的女性解决她们（比如化妆、美甲等）的问题是否有兴趣”。他沉默，没有给我一个明确的答案，其实我大概也能猜的出来，至少我自己不是很有兴趣吧。那我说，如果作为技术的最高负责人，你对所做的业务没有力图改变的兴趣，那很难说你的技术团队能真正感兴趣于为用户提供服务。不如做点技术的事情，让团队先对技术产生兴趣吧，虽然这件事的投入产出比相对比于前者有点低，但也算是一种选择。</p>\n<p>就我而言，对医生的同理心感受从很小就有了，各种因素不多说了。总是医生和技术人员一样，是工匠文化的典型代表，说深一点是Geek文化的典型代表。为啥？因为技术人员可以自己写各种好玩的应用，做Demo，但是脱离了业务，其实技术本身什么也不是。同理，医生也一样，你看医生自己可以做实验，研究片子，研究药理等等吧，但是本身医生离开了病人，其实也什么都不是。无论医生宣称自己有再高的成就，如果他没治好一个病人，恐怕没人相信。同理，一个技术人员无论宣称自己有在高的技能，做出fancy的页面，通过高压力的测试，外人也很难确信他所做的工作有多牛。说“医生是把患者背过河”，其实技术也一样“是把一个个严峻痛苦中的业务背过河”。说的好像有点高尚，但这就是医生，这就是技术。技术领域有活跃的Geek，也有混口饭吃的，甚至被迫从事这个行业的普通人。同理，医生领域也一样，只不过他们有两个更加对应的名字“神医”和“庸医”。</p>\n<p>所以，我深信医生和技术在工匠文化的表现上是如此的一致。也是因为此，我相信，既然Github，Stack Overflow，Heroku，Amazon Cloud成就了一批又一批技术，让技术迈向新的台阶，那么我相信医生也可以有属于自己的医疗自动化存储、研究、交流、服务平台。所以，我做了病历夹，就像Github一样让医生存储自己的“代码”，让感兴趣的人互相交流，做了诊疗圈，让医生可以互相提问实现能力提升。我还会做更多，做更多让医生这群工匠变得更加方便的事情，成就更多医疗领域的Geek。我解决不了每个医生的工作问题，但我相信通过我的努力，能够让更多人知道、辨别什么人是好医生，是爱医疗这门学科的医生。这就好像我们现在招聘必看面试者的Github和博客账号一样。</p>\n<p>如果这个问题是在问要不要投身于医疗这个行业，那么请准备好同理心，和真的想要颠覆一个行业去解脱那些痛苦中的医生们，让好医生获得人们更广泛的尊重，得到本来就应该属于他们的名誉和利益，让庸医无地自容。因为帮助了他们，也就帮助了作为患者的我们自己。这里多举一个身边的例子，我的一个同事，好兄弟，家里姨妈因为胃痛去了一家医院，医生针对胃炎给了些药打发回家了。偶然的机会，姨妈陪兄弟的妈妈检查癌症，于是在子女的要求下，自己也做了个检查，结果发现是早起胃癌，好在发现的早，应该无大碍。我们每个人，不可能每天去医院挂专家。但是如果这类庸医变得越来越少，我想至少他能想到让这位姨妈去做个检查；如果这类庸医变得越来越少，也许我们的兄弟姐妹父母子女会多一分生存的机会。</p>\n<p>说回到话题，如果关注点不在医疗这件事情，也没什么对错的；那就关注在技术本身，医疗领域里面好玩有趣的技术也很多，比如和很多其他行业类似的用户体验方面，数据处理方面等等，可以做出很多不错的方面事情。但至于具体是不是要投入移动医疗，或者什么其他行业，那就没有什么特别的了。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>刚好在回答一个知乎话题，但是觉得挺值的分享一下的，供有兴趣的童靴，尤其是开始关心业务的童靴参考</strong></p>\n<p>IT背景的人想做移动医疗应该怎么做准备？我觉得其实最重要的就是同理心。IT技术是一个用作所有行业皆可的东西，但是正因为他的“干什么都行”，也就决定了如果不是爱自己所做的产品或者方向的话，也很难把一个行业做好做好。前几天，碰到一个做女性社区的CTO，聊起来说自己的团队不喜欢和产品一次思考，不喜欢往前跨一步去为用户考虑。当然我们也交流了许多执行层面的方法，但我问他了一个我很看重的话题，就是“作为一个男人来讲，你对帮助身边的女性解决她们（比如化妆、美甲等）的问题是否有兴趣”。他沉默，没有给我一个明确的答案，其实我大概也能猜的出来，至少我自己不是很有兴趣吧。那我说，如果作为技术的最高负责人，你对所做的业务没有力图改变的兴趣，那很难说你的技术团队能真正感兴趣于为用户提供服务。不如做点技术的事情，让团队先对技术产生兴趣吧，虽然这件事的投入产出比相对比于前者有点低，但也算是一种选择。</p>\n<p>就我而言，对医生的同理心感受从很小就有了，各种因素不多说了。总是医生和技术人员一样，是工匠文化的典型代表，说深一点是Geek文化的典型代表。为啥？因为技术人员可以自己写各种好玩的应用，做Demo，但是脱离了业务，其实技术本身什么也不是。同理，医生也一样，你看医生自己可以做实验，研究片子，研究药理等等吧，但是本身医生离开了病人，其实也什么都不是。无论医生宣称自己有再高的成就，如果他没治好一个病人，恐怕没人相信。同理，一个技术人员无论宣称自己有在高的技能，做出fancy的页面，通过高压力的测试，外人也很难确信他所做的工作有多牛。说“医生是把患者背过河”，其实技术也一样“是把一个个严峻痛苦中的业务背过河”。说的好像有点高尚，但这就是医生，这就是技术。技术领域有活跃的Geek，也有混口饭吃的，甚至被迫从事这个行业的普通人。同理，医生领域也一样，只不过他们有两个更加对应的名字“神医”和“庸医”。</p>\n<p>所以，我深信医生和技术在工匠文化的表现上是如此的一致。也是因为此，我相信，既然Github，Stack Overflow，Heroku，Amazon Cloud成就了一批又一批技术，让技术迈向新的台阶，那么我相信医生也可以有属于自己的医疗自动化存储、研究、交流、服务平台。所以，我做了病历夹，就像Github一样让医生存储自己的“代码”，让感兴趣的人互相交流，做了诊疗圈，让医生可以互相提问实现能力提升。我还会做更多，做更多让医生这群工匠变得更加方便的事情，成就更多医疗领域的Geek。我解决不了每个医生的工作问题，但我相信通过我的努力，能够让更多人知道、辨别什么人是好医生，是爱医疗这门学科的医生。这就好像我们现在招聘必看面试者的Github和博客账号一样。</p>\n<p>如果这个问题是在问要不要投身于医疗这个行业，那么请准备好同理心，和真的想要颠覆一个行业去解脱那些痛苦中的医生们，让好医生获得人们更广泛的尊重，得到本来就应该属于他们的名誉和利益，让庸医无地自容。因为帮助了他们，也就帮助了作为患者的我们自己。这里多举一个身边的例子，我的一个同事，好兄弟，家里姨妈因为胃痛去了一家医院，医生针对胃炎给了些药打发回家了。偶然的机会，姨妈陪兄弟的妈妈检查癌症，于是在子女的要求下，自己也做了个检查，结果发现是早起胃癌，好在发现的早，应该无大碍。我们每个人，不可能每天去医院挂专家。但是如果这类庸医变得越来越少，我想至少他能想到让这位姨妈去做个检查；如果这类庸医变得越来越少，也许我们的兄弟姐妹父母子女会多一分生存的机会。</p>\n<p>说回到话题，如果关注点不在医疗这件事情，也没什么对错的；那就关注在技术本身，医疗领域里面好玩有趣的技术也很多，比如和很多其他行业类似的用户体验方面，数据处理方面等等，可以做出很多不错的方面事情。但至于具体是不是要投入移动医疗，或者什么其他行业，那就没有什么特别的了。</p>\n"},{"title":"无UI系统设计之实现","date":"2016-07-03T12:40:59.000Z","_content":"\n上一个","source":"_posts/no-back-end-design.md","raw":"title: 无UI系统设计之实现\ndate: 2016-07-03 20:40:59\ncategories:\n- Technology\ntags:\n- tech\n---\n\n上一个","slug":"no-back-end-design","published":1,"updated":"2018-04-16T09:59:37.070Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcop000nrlfyv36n66p4","content":"<p>上一个</p>\n","site":{"data":{}},"excerpt":"","more":"<p>上一个</p>\n"},{"title":"你好，世界","date":"2015-01-19T12:26:40.000Z","_content":"\n在杏树林我们曾经很努力的早出晚归，曾经欣喜的拿到一个又一个月的薪水，那么除此以外我们还在追求些什么？\n\n有些人会说我们做出了一个个app，我们做了丰富的交互，提高了技术。是的，这些都是我们在做的。作为一个工匠，我们追求无可厚非，那作为人生呢？\n\n我自己有时候也不知道这个问题的答案是什么，但是今天的一个插曲还是让我觉得很喜出望外。大早上起来，看到会议室在开会，出乎意料的是，这次会议室开会竟然没有关门，蹑手蹑脚的装作去找笔，探了探头。发现有个医生哥哥在讲牙科医生讲种植牙流程。于是在那里听了听，是不是装作很懂的点点头。于是乎在某一次点头后，对视上了一个眼神，那一刻我觉得有点被电到了。一个医生，一个没有穿白大褂的医生，一个标准大众脸，一个说起话来很有底气的中年男人。我觉得那一刻我们就站在一起，在聊一个在不同不过的话题。\n\n{% asset_img img1.jpg \"医生在讲课1\" %}\n\n我得承认，我今天依然没有找到如题的人生答案，但是这一刻，今天的早上，我发现我的职业在做除了工匠技术之外，更有意义的事情，那就是可能在帮一个人做点什么！\n\n{% asset_img img2.jpg \"医生在讲课2\" %}\n\n今天感谢武靖和高瑜今天没有关会议室的门，让我这个偷窥者能有感触公司事情的机会\n也肯定各个组，如果可以，还是叫上你们的TL一起。在我看来，运营＋产品＋TL是一家，三个人一起才是一家子！\n\n最后，Quote一下今天的封面，感谢文亮给我买的笔记本，越来越喜欢，尤其这句话\n\n```\n辛苦敲下的一行行代码不知能否改变这个世界\n其实我的世界，不过就是你的心\n```\n\nthanks @文亮\n\n{% asset_img cover.jpg \"HELLO WOLRD\" %}","source":"_posts/hello-world2.md","raw":"title: 你好，世界\ndate: 2015-01-19 20:26:40\ncategories:\n- Diary\ntags:\n- misc\n---\n\n在杏树林我们曾经很努力的早出晚归，曾经欣喜的拿到一个又一个月的薪水，那么除此以外我们还在追求些什么？\n\n有些人会说我们做出了一个个app，我们做了丰富的交互，提高了技术。是的，这些都是我们在做的。作为一个工匠，我们追求无可厚非，那作为人生呢？\n\n我自己有时候也不知道这个问题的答案是什么，但是今天的一个插曲还是让我觉得很喜出望外。大早上起来，看到会议室在开会，出乎意料的是，这次会议室开会竟然没有关门，蹑手蹑脚的装作去找笔，探了探头。发现有个医生哥哥在讲牙科医生讲种植牙流程。于是在那里听了听，是不是装作很懂的点点头。于是乎在某一次点头后，对视上了一个眼神，那一刻我觉得有点被电到了。一个医生，一个没有穿白大褂的医生，一个标准大众脸，一个说起话来很有底气的中年男人。我觉得那一刻我们就站在一起，在聊一个在不同不过的话题。\n\n{% asset_img img1.jpg \"医生在讲课1\" %}\n\n我得承认，我今天依然没有找到如题的人生答案，但是这一刻，今天的早上，我发现我的职业在做除了工匠技术之外，更有意义的事情，那就是可能在帮一个人做点什么！\n\n{% asset_img img2.jpg \"医生在讲课2\" %}\n\n今天感谢武靖和高瑜今天没有关会议室的门，让我这个偷窥者能有感触公司事情的机会\n也肯定各个组，如果可以，还是叫上你们的TL一起。在我看来，运营＋产品＋TL是一家，三个人一起才是一家子！\n\n最后，Quote一下今天的封面，感谢文亮给我买的笔记本，越来越喜欢，尤其这句话\n\n```\n辛苦敲下的一行行代码不知能否改变这个世界\n其实我的世界，不过就是你的心\n```\n\nthanks @文亮\n\n{% asset_img cover.jpg \"HELLO WOLRD\" %}","slug":"hello-world2","published":1,"updated":"2018-04-16T09:59:37.082Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoq000orlfy6cpgno7z","content":"<p>在杏树林我们曾经很努力的早出晚归，曾经欣喜的拿到一个又一个月的薪水，那么除此以外我们还在追求些什么？</p>\n<p>有些人会说我们做出了一个个app，我们做了丰富的交互，提高了技术。是的，这些都是我们在做的。作为一个工匠，我们追求无可厚非，那作为人生呢？</p>\n<p>我自己有时候也不知道这个问题的答案是什么，但是今天的一个插曲还是让我觉得很喜出望外。大早上起来，看到会议室在开会，出乎意料的是，这次会议室开会竟然没有关门，蹑手蹑脚的装作去找笔，探了探头。发现有个医生哥哥在讲牙科医生讲种植牙流程。于是在那里听了听，是不是装作很懂的点点头。于是乎在某一次点头后，对视上了一个眼神，那一刻我觉得有点被电到了。一个医生，一个没有穿白大褂的医生，一个标准大众脸，一个说起话来很有底气的中年男人。我觉得那一刻我们就站在一起，在聊一个在不同不过的话题。</p>\n<img src=\"/2015/01/19/hello-world2/img1.jpg\" title=\"医生在讲课1\">\n<p>我得承认，我今天依然没有找到如题的人生答案，但是这一刻，今天的早上，我发现我的职业在做除了工匠技术之外，更有意义的事情，那就是可能在帮一个人做点什么！</p>\n<img src=\"/2015/01/19/hello-world2/img2.jpg\" title=\"医生在讲课2\">\n<p>今天感谢武靖和高瑜今天没有关会议室的门，让我这个偷窥者能有感触公司事情的机会<br>也肯定各个组，如果可以，还是叫上你们的TL一起。在我看来，运营＋产品＋TL是一家，三个人一起才是一家子！</p>\n<p>最后，Quote一下今天的封面，感谢文亮给我买的笔记本，越来越喜欢，尤其这句话</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">辛苦敲下的一行行代码不知能否改变这个世界</span><br><span class=\"line\">其实我的世界，不过就是你的心</span><br></pre></td></tr></table></figure>\n<p>thanks @文亮</p>\n<img src=\"/2015/01/19/hello-world2/cover.jpg\" title=\"HELLO WOLRD\">","site":{"data":{}},"excerpt":"","more":"<p>在杏树林我们曾经很努力的早出晚归，曾经欣喜的拿到一个又一个月的薪水，那么除此以外我们还在追求些什么？</p>\n<p>有些人会说我们做出了一个个app，我们做了丰富的交互，提高了技术。是的，这些都是我们在做的。作为一个工匠，我们追求无可厚非，那作为人生呢？</p>\n<p>我自己有时候也不知道这个问题的答案是什么，但是今天的一个插曲还是让我觉得很喜出望外。大早上起来，看到会议室在开会，出乎意料的是，这次会议室开会竟然没有关门，蹑手蹑脚的装作去找笔，探了探头。发现有个医生哥哥在讲牙科医生讲种植牙流程。于是在那里听了听，是不是装作很懂的点点头。于是乎在某一次点头后，对视上了一个眼神，那一刻我觉得有点被电到了。一个医生，一个没有穿白大褂的医生，一个标准大众脸，一个说起话来很有底气的中年男人。我觉得那一刻我们就站在一起，在聊一个在不同不过的话题。</p>\n<img src=\"/2015/01/19/hello-world2/img1.jpg\" title=\"医生在讲课1\">\n<p>我得承认，我今天依然没有找到如题的人生答案，但是这一刻，今天的早上，我发现我的职业在做除了工匠技术之外，更有意义的事情，那就是可能在帮一个人做点什么！</p>\n<img src=\"/2015/01/19/hello-world2/img2.jpg\" title=\"医生在讲课2\">\n<p>今天感谢武靖和高瑜今天没有关会议室的门，让我这个偷窥者能有感触公司事情的机会<br>也肯定各个组，如果可以，还是叫上你们的TL一起。在我看来，运营＋产品＋TL是一家，三个人一起才是一家子！</p>\n<p>最后，Quote一下今天的封面，感谢文亮给我买的笔记本，越来越喜欢，尤其这句话</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">辛苦敲下的一行行代码不知能否改变这个世界</span><br><span class=\"line\">其实我的世界，不过就是你的心</span><br></pre></td></tr></table></figure>\n<p>thanks @文亮</p>\n<img src=\"/2015/01/19/hello-world2/cover.jpg\" title=\"HELLO WOLRD\">"},{"title":"物体检测","date":"2018-08-12T12:58:16.000Z","mathjax":true,"_content":"\n终于有点时间，可以写继续写notes，这样可以让整个学习过程的印象更加深入。\n\n本节课为CNN的第三周，总的来说就是讲述如何通过pre-train的物体模型，识别整张照片上的物体。和以往前两周的课程一样，围绕着若干篇论文算法展开 Detecting Algorithm\n\n+\tObject Localization\n+\tLandmark Detection, which describe less in the class about how to detect the interal features of an object by key landmarks\n+\tObject Detection, talking about how to detect an object with bounding box\n+\tSliding Window\n\n\n这里主要重点回顾YOLO（you only look once）。这是本课重点阐述的内容，video有两个，作业也是直接就是讲述YOLO，顺带一点其他算法。\n\n{% asset_img startyolo.png [Encoding architecture for YOLO] %}\n\n-\t起点，将原图划分成19x19的区块 （方便简化计算）\n-\tInput image (608, 608, 3)\n-\tThe input image goes through a CNN, resulting in a (19,19,5,85) dimensional output.\n-\tAfter flattening the last two dimensions, the output is a volume of shape (19, 19, 425):\n\t+\tEach cell in a 19x19 grid over the input image gives 425 numbers.\n\t+\t425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture.\n\t+\t85 = 5 + 80 where 5 is because  (pc,bx,by,bh,bw)(pc,bx,by,bh,bw)  has 5 numbers, and and 80 is the number of classes we'd like to detect\n-\tYou then select only few boxes based on:\n\t+\tScore-thresholding: throw away boxes that have detected a class with a score less than the threshold\n\t+\tNon-max suppression: Compute the Intersection over Union and avoid selecting overlapping boxes\n-\tThis gives you YOLO's final output.\n\n\nWhat you should remember:\n\n-\tYOLO is a state-of-the-art object detection model that is fast and accurate\n-\tIt runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume.\n-\tThe encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.\n-\tYou filter through all the boxes using non-max suppression. Specifically:\n\t+\tScore thresholding on the probability of detecting a class to keep only accurate (high probability) boxes\n\t+\tIntersection over Union (IoU) thresholding to eliminate overlapping boxes\n-\tBecause training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise.\n","source":"_posts/object-detection.md","raw":"title: 物体检测\ndate: 2018-08-12 20:58:16\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\nmathjax: true\n---\n\n终于有点时间，可以写继续写notes，这样可以让整个学习过程的印象更加深入。\n\n本节课为CNN的第三周，总的来说就是讲述如何通过pre-train的物体模型，识别整张照片上的物体。和以往前两周的课程一样，围绕着若干篇论文算法展开 Detecting Algorithm\n\n+\tObject Localization\n+\tLandmark Detection, which describe less in the class about how to detect the interal features of an object by key landmarks\n+\tObject Detection, talking about how to detect an object with bounding box\n+\tSliding Window\n\n\n这里主要重点回顾YOLO（you only look once）。这是本课重点阐述的内容，video有两个，作业也是直接就是讲述YOLO，顺带一点其他算法。\n\n{% asset_img startyolo.png [Encoding architecture for YOLO] %}\n\n-\t起点，将原图划分成19x19的区块 （方便简化计算）\n-\tInput image (608, 608, 3)\n-\tThe input image goes through a CNN, resulting in a (19,19,5,85) dimensional output.\n-\tAfter flattening the last two dimensions, the output is a volume of shape (19, 19, 425):\n\t+\tEach cell in a 19x19 grid over the input image gives 425 numbers.\n\t+\t425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture.\n\t+\t85 = 5 + 80 where 5 is because  (pc,bx,by,bh,bw)(pc,bx,by,bh,bw)  has 5 numbers, and and 80 is the number of classes we'd like to detect\n-\tYou then select only few boxes based on:\n\t+\tScore-thresholding: throw away boxes that have detected a class with a score less than the threshold\n\t+\tNon-max suppression: Compute the Intersection over Union and avoid selecting overlapping boxes\n-\tThis gives you YOLO's final output.\n\n\nWhat you should remember:\n\n-\tYOLO is a state-of-the-art object detection model that is fast and accurate\n-\tIt runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume.\n-\tThe encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.\n-\tYou filter through all the boxes using non-max suppression. Specifically:\n\t+\tScore thresholding on the probability of detecting a class to keep only accurate (high probability) boxes\n\t+\tIntersection over Union (IoU) thresholding to eliminate overlapping boxes\n-\tBecause training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise.\n","slug":"object-detection","published":1,"updated":"2018-08-12T16:18:47.041Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcou000prlfyantaeowz","content":"<p>终于有点时间，可以写继续写notes，这样可以让整个学习过程的印象更加深入。</p>\n<p>本节课为CNN的第三周，总的来说就是讲述如何通过pre-train的物体模型，识别整张照片上的物体。和以往前两周的课程一样，围绕着若干篇论文算法展开 Detecting Algorithm</p>\n<ul>\n<li>Object Localization</li>\n<li>Landmark Detection, which describe less in the class about how to detect the interal features of an object by key landmarks</li>\n<li>Object Detection, talking about how to detect an object with bounding box</li>\n<li>Sliding Window</li>\n</ul>\n<p>这里主要重点回顾YOLO（you only look once）。这是本课重点阐述的内容，video有两个，作业也是直接就是讲述YOLO，顺带一点其他算法。</p>\n<img src=\"/2018/08/12/object-detection/startyolo.png\" title=\"[Encoding architecture for YOLO]\">\n<ul>\n<li>起点，将原图划分成19x19的区块 （方便简化计算）</li>\n<li>Input image (608, 608, 3)</li>\n<li>The input image goes through a CNN, resulting in a (19,19,5,85) dimensional output.</li>\n<li>After flattening the last two dimensions, the output is a volume of shape (19, 19, 425):<ul>\n<li>Each cell in a 19x19 grid over the input image gives 425 numbers.</li>\n<li>425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture.</li>\n<li>85 = 5 + 80 where 5 is because  (pc,bx,by,bh,bw)(pc,bx,by,bh,bw)  has 5 numbers, and and 80 is the number of classes we’d like to detect</li>\n</ul>\n</li>\n<li>You then select only few boxes based on:<ul>\n<li>Score-thresholding: throw away boxes that have detected a class with a score less than the threshold</li>\n<li>Non-max suppression: Compute the Intersection over Union and avoid selecting overlapping boxes</li>\n</ul>\n</li>\n<li>This gives you YOLO’s final output.</li>\n</ul>\n<p>What you should remember:</p>\n<ul>\n<li>YOLO is a state-of-the-art object detection model that is fast and accurate</li>\n<li>It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume.</li>\n<li>The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.</li>\n<li>You filter through all the boxes using non-max suppression. Specifically:<ul>\n<li>Score thresholding on the probability of detecting a class to keep only accurate (high probability) boxes</li>\n<li>Intersection over Union (IoU) thresholding to eliminate overlapping boxes</li>\n</ul>\n</li>\n<li>Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>终于有点时间，可以写继续写notes，这样可以让整个学习过程的印象更加深入。</p>\n<p>本节课为CNN的第三周，总的来说就是讲述如何通过pre-train的物体模型，识别整张照片上的物体。和以往前两周的课程一样，围绕着若干篇论文算法展开 Detecting Algorithm</p>\n<ul>\n<li>Object Localization</li>\n<li>Landmark Detection, which describe less in the class about how to detect the interal features of an object by key landmarks</li>\n<li>Object Detection, talking about how to detect an object with bounding box</li>\n<li>Sliding Window</li>\n</ul>\n<p>这里主要重点回顾YOLO（you only look once）。这是本课重点阐述的内容，video有两个，作业也是直接就是讲述YOLO，顺带一点其他算法。</p>\n<img src=\"/2018/08/12/object-detection/startyolo.png\" title=\"[Encoding architecture for YOLO]\">\n<ul>\n<li>起点，将原图划分成19x19的区块 （方便简化计算）</li>\n<li>Input image (608, 608, 3)</li>\n<li>The input image goes through a CNN, resulting in a (19,19,5,85) dimensional output.</li>\n<li>After flattening the last two dimensions, the output is a volume of shape (19, 19, 425):<ul>\n<li>Each cell in a 19x19 grid over the input image gives 425 numbers.</li>\n<li>425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture.</li>\n<li>85 = 5 + 80 where 5 is because  (pc,bx,by,bh,bw)(pc,bx,by,bh,bw)  has 5 numbers, and and 80 is the number of classes we’d like to detect</li>\n</ul>\n</li>\n<li>You then select only few boxes based on:<ul>\n<li>Score-thresholding: throw away boxes that have detected a class with a score less than the threshold</li>\n<li>Non-max suppression: Compute the Intersection over Union and avoid selecting overlapping boxes</li>\n</ul>\n</li>\n<li>This gives you YOLO’s final output.</li>\n</ul>\n<p>What you should remember:</p>\n<ul>\n<li>YOLO is a state-of-the-art object detection model that is fast and accurate</li>\n<li>It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume.</li>\n<li>The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.</li>\n<li>You filter through all the boxes using non-max suppression. Specifically:<ul>\n<li>Score thresholding on the probability of detecting a class to keep only accurate (high probability) boxes</li>\n<li>Intersection over Union (IoU) thresholding to eliminate overlapping boxes</li>\n</ul>\n</li>\n<li>Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise.</li>\n</ul>\n"},{"title":"ocr","date":"2014-12-19T02:51:30.000Z","_content":"最近这两天稍微深入一些研究了一下OCR的系统技术，沟通和研究了几个主流的公司Abbyy、文通、汉王和经纬名片通。大体对这方面的东西有了些了解。写篇blog记录一下。","source":"_posts/ocr.md","raw":"title: ocr\ndate: 2014-12-19 10:51:30\ncategories:\n- Technology\ntags:\n- tech\n---\n最近这两天稍微深入一些研究了一下OCR的系统技术，沟通和研究了几个主流的公司Abbyy、文通、汉王和经纬名片通。大体对这方面的东西有了些了解。写篇blog记录一下。","slug":"ocr","published":1,"updated":"2018-04-16T09:59:37.039Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcou000qrlfy8hq4f83l","content":"<p>最近这两天稍微深入一些研究了一下OCR的系统技术，沟通和研究了几个主流的公司Abbyy、文通、汉王和经纬名片通。大体对这方面的东西有了些了解。写篇blog记录一下。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近这两天稍微深入一些研究了一下OCR的系统技术，沟通和研究了几个主流的公司Abbyy、文通、汉王和经纬名片通。大体对这方面的东西有了些了解。写篇blog记录一下。</p>\n"},{"title":"OKR学习总结","date":"2016-03-27T01:25:04.000Z","_content":"\nOKR已经尝试了3个季度了，感觉从一团迷雾里渐渐看出了点眉目，我很难说现在的一切是对的，但是总结出一些东西，为了今后更加努力的改进。既然OKR是目标，既然OKR是数据驱动的基础，那么就要坚持演进下去。以下想表达的，更多是基于自我OKR回顾的一种总结，并不是OKR完成的组成。只是像以此描述我在定义OKR的过程中所走过的那些坑坑洼洼。\n\n**OKR疑问有所斩获：**\n\n+\tAlign目标\n\t这个话题其实以前是知道的，但是体会不深。直到上一次我遇到了一个世纪的问题。当时所有“基础架构”团队的OKR基本上都是1on1来制定的，基本上纯从每个人的想法出发，我这边进行了一些修改和补充。然后，后来做到一半，技术团队OKR出来了，发现之前定义的好多和技术团队OKR关系不大。包括后来又改过几次，才发现慢慢上到。所以OKR以上上下下做做右右信息透明为第一要务，为的就是让大家目标能够algin到一起去。\n\t具体改进方法：OKR一定要从公司级别开始定义，以此来推导出与公司目标align的技术团队OKR，这个OKR，理论上还应该回过头来和其他业务团队的OKR（针对杏树林而言，可能更多是业务团队）进行align。不过在实际目前的操作中可能并不需要从一开始就这么做，毕竟所有团队也都是为了公司的OKR完成的。因此，出于效率考虑可以继续下去。那么接下来就是小组或者个人级别的工作了。最终，通过一系列的OKR制定工作，可以保证每个季度大家都在一个方向上工作，帮助公司完成商业目标。\t\n\t\n+\t团队／个人对OKR负责\n\t说到OKR，就不得不说到很多技术团队的同学们反应，在公司缺乏团队感，不知道向谁汇报，谁该有权利给谁分配任务。其实OKR就是很好的一个衔接点。每一个人为自己的OKR负责，OKR的来源可能是多方面的。在初期一定会有个人帮助他订立OKR，收集需求和反馈。对于业务团队内部来说，往往这件事情比较简单，就是业务团队的技术Lead和业务本身。对于基础架构或者跨团队支持的技术同事，往往这个时候技术同事的级别本身都比较高，那就需要对技术总监团队负责。总体而言，谁帮助订立OKR，就应该谁是负责人。但是OKR一旦被制定下来，每个人应该为OKR负责。\n\t\n+\tOKR为了制度改变\n\tOKR不是绩效目标。因为假设人是优秀和自驱动的，不满足这个条件的人，可以不存在于OKR体系，而实用类似KPI等其他体系。这个话题缘起于对OKR Review的一次讨论，集体内容不说。反思大概是这样。如果OKR的订立前提是，组织内每一个人都是积极的，主动的，乐于思考和勇于承担的，那么OKR的作用是为了帮助大家，对没有做到或没有做好的问题，寻求解决方案。而这种解决方案在实践中，往往就是公司的一些制度和方法的演进。也就是说，任何事情一定要寻找一种新的积极的流程去解决掉。这个前提就是去寻找当时订立OKR之后，发现执行过程中没想到的东西。然后寻找方法，避免掉没想到问题的再次发生。这样可以将OKR订立的更好。\n\t{% asset_img OKR_review.png \"OKR为了制度改进\" %}\n\n\n**时间计划值得坚持：**\n\n+\t定理时间的经验\n\t根据Q1的经验，我们在给Q2定力OKR的时候做了一些改动\n\t*\t为了更高效的完成OKR，我们采用组级别进行OKR定义，分为基础架构，公共服务组，PTL团队。\n\t*\t将技术总监团队先完成OKR的制定，然后是基础架构团队，然后是PTL团队。\n\t*\t先进行上季度OKR Review，然后进行新季度OKR制定\n\t*\t继续讲人员限定在基础架构组＋PTL范围内，力图通过Q2完成人员培训工作\n\t{% asset_img okr_timeline.png \"OKR的时间轴\" %}\n\n+\t双周回顾的经验\n\t技术团队目前还无法形成单周回顾，效率过于频繁可能导致精力分散，而且实践执行中发现难以完成。主要还是大家没有养成这类工作的习惯。所以目前还是我一个人在推广，测算下来，基础架构组＋公共＋PTL的总人数目前有12个人，接下来Q2可能会有15个人，数量庞大，而且没有一个标准，所以尚无法完成扩展。但是下面的表格值得推广，确实效果很好。\n\t{% asset_img fortynight_review.png \"双周回顾\" %}\n\t\n\n**回顾方法有待改善：**\n\n+\tQ1本季度感觉回顾方法有待改善。学习了一下文亮的OKR Checklist，打算应用到接下来的制定和回顾工作中。接下来会专门做一个表来进行工作。\n\n_OKR检查表_\n\n1.\tKey Result 是否能够实现Objective，套用句型：通过实现XXX（某一条具体的KR），就能促进XXX（某一条具体的O）的实现。\n2.\tObjective 和 Key Result 描述的是达到什么，而非做了什么\n3.\t检查是否满足基本明确定义和可度量的要求\n4.\t检查是否完整反映了个人的主要工作\n5.\t提醒合作方、同事来给出反馈\n6.\t检查是否和团队的目标一致\n7.\t检查是否和个人能力一致 (过易和过难都不好)\n8.\t检查是否足够专注、优先级是否清楚\n\n**还没有完成的话题：**\n\n*\tOKR如何打分，如何数据结果导向","source":"_posts/okr-review.md","raw":"title: OKR学习总结\ndate: 2016-03-27 09:25:04\ncategories:\n- Management\ntags:\n- book\n- mgnt\n---\n\nOKR已经尝试了3个季度了，感觉从一团迷雾里渐渐看出了点眉目，我很难说现在的一切是对的，但是总结出一些东西，为了今后更加努力的改进。既然OKR是目标，既然OKR是数据驱动的基础，那么就要坚持演进下去。以下想表达的，更多是基于自我OKR回顾的一种总结，并不是OKR完成的组成。只是像以此描述我在定义OKR的过程中所走过的那些坑坑洼洼。\n\n**OKR疑问有所斩获：**\n\n+\tAlign目标\n\t这个话题其实以前是知道的，但是体会不深。直到上一次我遇到了一个世纪的问题。当时所有“基础架构”团队的OKR基本上都是1on1来制定的，基本上纯从每个人的想法出发，我这边进行了一些修改和补充。然后，后来做到一半，技术团队OKR出来了，发现之前定义的好多和技术团队OKR关系不大。包括后来又改过几次，才发现慢慢上到。所以OKR以上上下下做做右右信息透明为第一要务，为的就是让大家目标能够algin到一起去。\n\t具体改进方法：OKR一定要从公司级别开始定义，以此来推导出与公司目标align的技术团队OKR，这个OKR，理论上还应该回过头来和其他业务团队的OKR（针对杏树林而言，可能更多是业务团队）进行align。不过在实际目前的操作中可能并不需要从一开始就这么做，毕竟所有团队也都是为了公司的OKR完成的。因此，出于效率考虑可以继续下去。那么接下来就是小组或者个人级别的工作了。最终，通过一系列的OKR制定工作，可以保证每个季度大家都在一个方向上工作，帮助公司完成商业目标。\t\n\t\n+\t团队／个人对OKR负责\n\t说到OKR，就不得不说到很多技术团队的同学们反应，在公司缺乏团队感，不知道向谁汇报，谁该有权利给谁分配任务。其实OKR就是很好的一个衔接点。每一个人为自己的OKR负责，OKR的来源可能是多方面的。在初期一定会有个人帮助他订立OKR，收集需求和反馈。对于业务团队内部来说，往往这件事情比较简单，就是业务团队的技术Lead和业务本身。对于基础架构或者跨团队支持的技术同事，往往这个时候技术同事的级别本身都比较高，那就需要对技术总监团队负责。总体而言，谁帮助订立OKR，就应该谁是负责人。但是OKR一旦被制定下来，每个人应该为OKR负责。\n\t\n+\tOKR为了制度改变\n\tOKR不是绩效目标。因为假设人是优秀和自驱动的，不满足这个条件的人，可以不存在于OKR体系，而实用类似KPI等其他体系。这个话题缘起于对OKR Review的一次讨论，集体内容不说。反思大概是这样。如果OKR的订立前提是，组织内每一个人都是积极的，主动的，乐于思考和勇于承担的，那么OKR的作用是为了帮助大家，对没有做到或没有做好的问题，寻求解决方案。而这种解决方案在实践中，往往就是公司的一些制度和方法的演进。也就是说，任何事情一定要寻找一种新的积极的流程去解决掉。这个前提就是去寻找当时订立OKR之后，发现执行过程中没想到的东西。然后寻找方法，避免掉没想到问题的再次发生。这样可以将OKR订立的更好。\n\t{% asset_img OKR_review.png \"OKR为了制度改进\" %}\n\n\n**时间计划值得坚持：**\n\n+\t定理时间的经验\n\t根据Q1的经验，我们在给Q2定力OKR的时候做了一些改动\n\t*\t为了更高效的完成OKR，我们采用组级别进行OKR定义，分为基础架构，公共服务组，PTL团队。\n\t*\t将技术总监团队先完成OKR的制定，然后是基础架构团队，然后是PTL团队。\n\t*\t先进行上季度OKR Review，然后进行新季度OKR制定\n\t*\t继续讲人员限定在基础架构组＋PTL范围内，力图通过Q2完成人员培训工作\n\t{% asset_img okr_timeline.png \"OKR的时间轴\" %}\n\n+\t双周回顾的经验\n\t技术团队目前还无法形成单周回顾，效率过于频繁可能导致精力分散，而且实践执行中发现难以完成。主要还是大家没有养成这类工作的习惯。所以目前还是我一个人在推广，测算下来，基础架构组＋公共＋PTL的总人数目前有12个人，接下来Q2可能会有15个人，数量庞大，而且没有一个标准，所以尚无法完成扩展。但是下面的表格值得推广，确实效果很好。\n\t{% asset_img fortynight_review.png \"双周回顾\" %}\n\t\n\n**回顾方法有待改善：**\n\n+\tQ1本季度感觉回顾方法有待改善。学习了一下文亮的OKR Checklist，打算应用到接下来的制定和回顾工作中。接下来会专门做一个表来进行工作。\n\n_OKR检查表_\n\n1.\tKey Result 是否能够实现Objective，套用句型：通过实现XXX（某一条具体的KR），就能促进XXX（某一条具体的O）的实现。\n2.\tObjective 和 Key Result 描述的是达到什么，而非做了什么\n3.\t检查是否满足基本明确定义和可度量的要求\n4.\t检查是否完整反映了个人的主要工作\n5.\t提醒合作方、同事来给出反馈\n6.\t检查是否和团队的目标一致\n7.\t检查是否和个人能力一致 (过易和过难都不好)\n8.\t检查是否足够专注、优先级是否清楚\n\n**还没有完成的话题：**\n\n*\tOKR如何打分，如何数据结果导向","slug":"okr-review","published":1,"updated":"2018-04-16T09:59:37.071Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcov000rrlfyqe0j8z40","content":"<p>OKR已经尝试了3个季度了，感觉从一团迷雾里渐渐看出了点眉目，我很难说现在的一切是对的，但是总结出一些东西，为了今后更加努力的改进。既然OKR是目标，既然OKR是数据驱动的基础，那么就要坚持演进下去。以下想表达的，更多是基于自我OKR回顾的一种总结，并不是OKR完成的组成。只是像以此描述我在定义OKR的过程中所走过的那些坑坑洼洼。</p>\n<p><strong>OKR疑问有所斩获：</strong></p>\n<ul>\n<li><p>Align目标<br>这个话题其实以前是知道的，但是体会不深。直到上一次我遇到了一个世纪的问题。当时所有“基础架构”团队的OKR基本上都是1on1来制定的，基本上纯从每个人的想法出发，我这边进行了一些修改和补充。然后，后来做到一半，技术团队OKR出来了，发现之前定义的好多和技术团队OKR关系不大。包括后来又改过几次，才发现慢慢上到。所以OKR以上上下下做做右右信息透明为第一要务，为的就是让大家目标能够algin到一起去。<br>具体改进方法：OKR一定要从公司级别开始定义，以此来推导出与公司目标align的技术团队OKR，这个OKR，理论上还应该回过头来和其他业务团队的OKR（针对杏树林而言，可能更多是业务团队）进行align。不过在实际目前的操作中可能并不需要从一开始就这么做，毕竟所有团队也都是为了公司的OKR完成的。因此，出于效率考虑可以继续下去。那么接下来就是小组或者个人级别的工作了。最终，通过一系列的OKR制定工作，可以保证每个季度大家都在一个方向上工作，帮助公司完成商业目标。    </p>\n</li>\n<li><p>团队／个人对OKR负责<br>说到OKR，就不得不说到很多技术团队的同学们反应，在公司缺乏团队感，不知道向谁汇报，谁该有权利给谁分配任务。其实OKR就是很好的一个衔接点。每一个人为自己的OKR负责，OKR的来源可能是多方面的。在初期一定会有个人帮助他订立OKR，收集需求和反馈。对于业务团队内部来说，往往这件事情比较简单，就是业务团队的技术Lead和业务本身。对于基础架构或者跨团队支持的技术同事，往往这个时候技术同事的级别本身都比较高，那就需要对技术总监团队负责。总体而言，谁帮助订立OKR，就应该谁是负责人。但是OKR一旦被制定下来，每个人应该为OKR负责。</p>\n</li>\n<li><p>OKR为了制度改变<br>OKR不是绩效目标。因为假设人是优秀和自驱动的，不满足这个条件的人，可以不存在于OKR体系，而实用类似KPI等其他体系。这个话题缘起于对OKR Review的一次讨论，集体内容不说。反思大概是这样。如果OKR的订立前提是，组织内每一个人都是积极的，主动的，乐于思考和勇于承担的，那么OKR的作用是为了帮助大家，对没有做到或没有做好的问题，寻求解决方案。而这种解决方案在实践中，往往就是公司的一些制度和方法的演进。也就是说，任何事情一定要寻找一种新的积极的流程去解决掉。这个前提就是去寻找当时订立OKR之后，发现执行过程中没想到的东西。然后寻找方法，避免掉没想到问题的再次发生。这样可以将OKR订立的更好。</p>\n<img src=\"/2016/03/27/okr-review/OKR_review.png\" title=\"OKR为了制度改进\">\n</li>\n</ul>\n<p><strong>时间计划值得坚持：</strong></p>\n<ul>\n<li><p>定理时间的经验<br>根据Q1的经验，我们在给Q2定力OKR的时候做了一些改动</p>\n<ul>\n<li>为了更高效的完成OKR，我们采用组级别进行OKR定义，分为基础架构，公共服务组，PTL团队。</li>\n<li>将技术总监团队先完成OKR的制定，然后是基础架构团队，然后是PTL团队。</li>\n<li>先进行上季度OKR Review，然后进行新季度OKR制定</li>\n<li>继续讲人员限定在基础架构组＋PTL范围内，力图通过Q2完成人员培训工作<img src=\"/2016/03/27/okr-review/okr_timeline.png\" title=\"OKR的时间轴\">\n</li>\n</ul>\n</li>\n<li><p>双周回顾的经验<br>技术团队目前还无法形成单周回顾，效率过于频繁可能导致精力分散，而且实践执行中发现难以完成。主要还是大家没有养成这类工作的习惯。所以目前还是我一个人在推广，测算下来，基础架构组＋公共＋PTL的总人数目前有12个人，接下来Q2可能会有15个人，数量庞大，而且没有一个标准，所以尚无法完成扩展。但是下面的表格值得推广，确实效果很好。</p>\n<img src=\"/2016/03/27/okr-review/fortynight_review.png\" title=\"双周回顾\">\n</li>\n</ul>\n<p><strong>回顾方法有待改善：</strong></p>\n<ul>\n<li>Q1本季度感觉回顾方法有待改善。学习了一下文亮的OKR Checklist，打算应用到接下来的制定和回顾工作中。接下来会专门做一个表来进行工作。</li>\n</ul>\n<p>_OKR检查表_</p>\n<ol>\n<li>Key Result 是否能够实现Objective，套用句型：通过实现XXX（某一条具体的KR），就能促进XXX（某一条具体的O）的实现。</li>\n<li>Objective 和 Key Result 描述的是达到什么，而非做了什么</li>\n<li>检查是否满足基本明确定义和可度量的要求</li>\n<li>检查是否完整反映了个人的主要工作</li>\n<li>提醒合作方、同事来给出反馈</li>\n<li>检查是否和团队的目标一致</li>\n<li>检查是否和个人能力一致 (过易和过难都不好)</li>\n<li>检查是否足够专注、优先级是否清楚</li>\n</ol>\n<p><strong>还没有完成的话题：</strong></p>\n<ul>\n<li>OKR如何打分，如何数据结果导向</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>OKR已经尝试了3个季度了，感觉从一团迷雾里渐渐看出了点眉目，我很难说现在的一切是对的，但是总结出一些东西，为了今后更加努力的改进。既然OKR是目标，既然OKR是数据驱动的基础，那么就要坚持演进下去。以下想表达的，更多是基于自我OKR回顾的一种总结，并不是OKR完成的组成。只是像以此描述我在定义OKR的过程中所走过的那些坑坑洼洼。</p>\n<p><strong>OKR疑问有所斩获：</strong></p>\n<ul>\n<li><p>Align目标<br>这个话题其实以前是知道的，但是体会不深。直到上一次我遇到了一个世纪的问题。当时所有“基础架构”团队的OKR基本上都是1on1来制定的，基本上纯从每个人的想法出发，我这边进行了一些修改和补充。然后，后来做到一半，技术团队OKR出来了，发现之前定义的好多和技术团队OKR关系不大。包括后来又改过几次，才发现慢慢上到。所以OKR以上上下下做做右右信息透明为第一要务，为的就是让大家目标能够algin到一起去。<br>具体改进方法：OKR一定要从公司级别开始定义，以此来推导出与公司目标align的技术团队OKR，这个OKR，理论上还应该回过头来和其他业务团队的OKR（针对杏树林而言，可能更多是业务团队）进行align。不过在实际目前的操作中可能并不需要从一开始就这么做，毕竟所有团队也都是为了公司的OKR完成的。因此，出于效率考虑可以继续下去。那么接下来就是小组或者个人级别的工作了。最终，通过一系列的OKR制定工作，可以保证每个季度大家都在一个方向上工作，帮助公司完成商业目标。    </p>\n</li>\n<li><p>团队／个人对OKR负责<br>说到OKR，就不得不说到很多技术团队的同学们反应，在公司缺乏团队感，不知道向谁汇报，谁该有权利给谁分配任务。其实OKR就是很好的一个衔接点。每一个人为自己的OKR负责，OKR的来源可能是多方面的。在初期一定会有个人帮助他订立OKR，收集需求和反馈。对于业务团队内部来说，往往这件事情比较简单，就是业务团队的技术Lead和业务本身。对于基础架构或者跨团队支持的技术同事，往往这个时候技术同事的级别本身都比较高，那就需要对技术总监团队负责。总体而言，谁帮助订立OKR，就应该谁是负责人。但是OKR一旦被制定下来，每个人应该为OKR负责。</p>\n</li>\n<li><p>OKR为了制度改变<br>OKR不是绩效目标。因为假设人是优秀和自驱动的，不满足这个条件的人，可以不存在于OKR体系，而实用类似KPI等其他体系。这个话题缘起于对OKR Review的一次讨论，集体内容不说。反思大概是这样。如果OKR的订立前提是，组织内每一个人都是积极的，主动的，乐于思考和勇于承担的，那么OKR的作用是为了帮助大家，对没有做到或没有做好的问题，寻求解决方案。而这种解决方案在实践中，往往就是公司的一些制度和方法的演进。也就是说，任何事情一定要寻找一种新的积极的流程去解决掉。这个前提就是去寻找当时订立OKR之后，发现执行过程中没想到的东西。然后寻找方法，避免掉没想到问题的再次发生。这样可以将OKR订立的更好。</p>\n<img src=\"/2016/03/27/okr-review/OKR_review.png\" title=\"OKR为了制度改进\">\n</li>\n</ul>\n<p><strong>时间计划值得坚持：</strong></p>\n<ul>\n<li><p>定理时间的经验<br>根据Q1的经验，我们在给Q2定力OKR的时候做了一些改动</p>\n<ul>\n<li>为了更高效的完成OKR，我们采用组级别进行OKR定义，分为基础架构，公共服务组，PTL团队。</li>\n<li>将技术总监团队先完成OKR的制定，然后是基础架构团队，然后是PTL团队。</li>\n<li>先进行上季度OKR Review，然后进行新季度OKR制定</li>\n<li>继续讲人员限定在基础架构组＋PTL范围内，力图通过Q2完成人员培训工作<img src=\"/2016/03/27/okr-review/okr_timeline.png\" title=\"OKR的时间轴\">\n</li>\n</ul>\n</li>\n<li><p>双周回顾的经验<br>技术团队目前还无法形成单周回顾，效率过于频繁可能导致精力分散，而且实践执行中发现难以完成。主要还是大家没有养成这类工作的习惯。所以目前还是我一个人在推广，测算下来，基础架构组＋公共＋PTL的总人数目前有12个人，接下来Q2可能会有15个人，数量庞大，而且没有一个标准，所以尚无法完成扩展。但是下面的表格值得推广，确实效果很好。</p>\n<img src=\"/2016/03/27/okr-review/fortynight_review.png\" title=\"双周回顾\">\n</li>\n</ul>\n<p><strong>回顾方法有待改善：</strong></p>\n<ul>\n<li>Q1本季度感觉回顾方法有待改善。学习了一下文亮的OKR Checklist，打算应用到接下来的制定和回顾工作中。接下来会专门做一个表来进行工作。</li>\n</ul>\n<p>_OKR检查表_</p>\n<ol>\n<li>Key Result 是否能够实现Objective，套用句型：通过实现XXX（某一条具体的KR），就能促进XXX（某一条具体的O）的实现。</li>\n<li>Objective 和 Key Result 描述的是达到什么，而非做了什么</li>\n<li>检查是否满足基本明确定义和可度量的要求</li>\n<li>检查是否完整反映了个人的主要工作</li>\n<li>提醒合作方、同事来给出反馈</li>\n<li>检查是否和团队的目标一致</li>\n<li>检查是否和个人能力一致 (过易和过难都不好)</li>\n<li>检查是否足够专注、优先级是否清楚</li>\n</ol>\n<p><strong>还没有完成的话题：</strong></p>\n<ul>\n<li>OKR如何打分，如何数据结果导向</li>\n</ul>\n"},{"title":"回顾北极星之病毒增长","date":"2015-07-07T16:38:42.000Z","_content":"上周做了一次关于北极星的回顾，一直想写也忘记了。今天提起来，其中印象比较深的部分还是在讨论这次送iPhone活动的成效。\n\n复盘的结果是这样的：总的来讲，本次活动的成效还是比较好的，收获了大量的新用户注册。但是，如果说病毒效应的话，这次活动并不算事一个很好的病毒效应传播。从数据上看，并未达到预期的增长效果。\n\n总结复盘，看了一下这篇知乎上的文章，我认为说的非常棒。\nhttp://www.zhihu.com/question/20901811\n\n“在facebook出现之前，有sixdegrees, Friendster, Myspace之类的社交网站，有一本书上说：facebook之所以能打败Friendster之类是因为用到了病毒式传播。如下图, 病毒式传播开始一般是线性增长,到到达临界点(Critical mass),然后以指数方式增长,直到饱和点。如果没办法到达临界点,一段周期后,就废了。”\n\n觉得这一段话比较准确的说明了病毒式增长的方法和意义。下面这张图是原文中的曲线表述。这个图形，和我们自己的活动图形还是颇为相似。但是如何界定什么是“Saturation”饱和点，什么事“Critical mass”这个问题上，我还是有些困惑。关于我们图形里的第二个波峰，是哪一个点，还不是很清楚。我的感觉是，还需要一、两个活动，进行持续观察，寻找图像之间的拟合系数。通过这种曲线拟合关系，也许可以寻找到医疗互联网的一些有意思的数据。也就是这大约280万医生的市场运作数据。\n\n希望大家每一个人和我一样，每周对数据关注一点点。作为一家互联网公司，数据驱动，才是核心。身为精英文化，目前的我不完全认同“无脑强运营”的文化，更加倾向于精英就是努力寻找对的事情，用数据说话，把方向找准。都说互联网没有对错，只有试一试。我觉得这是对的，但应该在数据驱动前提下完成的。而不是试了，然后呢？再是下一个。中国的互联网大量资本注入，已经不允许我们万事浅尝辄止，应该可以精细剖析。这也是现在所谓精细化运营的重要体现。","source":"_posts/polar-review.md","raw":"title: 回顾北极星之病毒增长\ndate: 2015-07-08 00:38:42\ncategories:\n- Business Strategy\ntags:\n- tech\n- python\n---\n上周做了一次关于北极星的回顾，一直想写也忘记了。今天提起来，其中印象比较深的部分还是在讨论这次送iPhone活动的成效。\n\n复盘的结果是这样的：总的来讲，本次活动的成效还是比较好的，收获了大量的新用户注册。但是，如果说病毒效应的话，这次活动并不算事一个很好的病毒效应传播。从数据上看，并未达到预期的增长效果。\n\n总结复盘，看了一下这篇知乎上的文章，我认为说的非常棒。\nhttp://www.zhihu.com/question/20901811\n\n“在facebook出现之前，有sixdegrees, Friendster, Myspace之类的社交网站，有一本书上说：facebook之所以能打败Friendster之类是因为用到了病毒式传播。如下图, 病毒式传播开始一般是线性增长,到到达临界点(Critical mass),然后以指数方式增长,直到饱和点。如果没办法到达临界点,一段周期后,就废了。”\n\n觉得这一段话比较准确的说明了病毒式增长的方法和意义。下面这张图是原文中的曲线表述。这个图形，和我们自己的活动图形还是颇为相似。但是如何界定什么是“Saturation”饱和点，什么事“Critical mass”这个问题上，我还是有些困惑。关于我们图形里的第二个波峰，是哪一个点，还不是很清楚。我的感觉是，还需要一、两个活动，进行持续观察，寻找图像之间的拟合系数。通过这种曲线拟合关系，也许可以寻找到医疗互联网的一些有意思的数据。也就是这大约280万医生的市场运作数据。\n\n希望大家每一个人和我一样，每周对数据关注一点点。作为一家互联网公司，数据驱动，才是核心。身为精英文化，目前的我不完全认同“无脑强运营”的文化，更加倾向于精英就是努力寻找对的事情，用数据说话，把方向找准。都说互联网没有对错，只有试一试。我觉得这是对的，但应该在数据驱动前提下完成的。而不是试了，然后呢？再是下一个。中国的互联网大量资本注入，已经不允许我们万事浅尝辄止，应该可以精细剖析。这也是现在所谓精细化运营的重要体现。","slug":"polar-review","published":1,"updated":"2018-04-16T09:59:37.056Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcov000srlfy4htgkoph","content":"<p>上周做了一次关于北极星的回顾，一直想写也忘记了。今天提起来，其中印象比较深的部分还是在讨论这次送iPhone活动的成效。</p>\n<p>复盘的结果是这样的：总的来讲，本次活动的成效还是比较好的，收获了大量的新用户注册。但是，如果说病毒效应的话，这次活动并不算事一个很好的病毒效应传播。从数据上看，并未达到预期的增长效果。</p>\n<p>总结复盘，看了一下这篇知乎上的文章，我认为说的非常棒。<br><a href=\"http://www.zhihu.com/question/20901811\" target=\"_blank\" rel=\"noopener\">http://www.zhihu.com/question/20901811</a></p>\n<p>“在facebook出现之前，有sixdegrees, Friendster, Myspace之类的社交网站，有一本书上说：facebook之所以能打败Friendster之类是因为用到了病毒式传播。如下图, 病毒式传播开始一般是线性增长,到到达临界点(Critical mass),然后以指数方式增长,直到饱和点。如果没办法到达临界点,一段周期后,就废了。”</p>\n<p>觉得这一段话比较准确的说明了病毒式增长的方法和意义。下面这张图是原文中的曲线表述。这个图形，和我们自己的活动图形还是颇为相似。但是如何界定什么是“Saturation”饱和点，什么事“Critical mass”这个问题上，我还是有些困惑。关于我们图形里的第二个波峰，是哪一个点，还不是很清楚。我的感觉是，还需要一、两个活动，进行持续观察，寻找图像之间的拟合系数。通过这种曲线拟合关系，也许可以寻找到医疗互联网的一些有意思的数据。也就是这大约280万医生的市场运作数据。</p>\n<p>希望大家每一个人和我一样，每周对数据关注一点点。作为一家互联网公司，数据驱动，才是核心。身为精英文化，目前的我不完全认同“无脑强运营”的文化，更加倾向于精英就是努力寻找对的事情，用数据说话，把方向找准。都说互联网没有对错，只有试一试。我觉得这是对的，但应该在数据驱动前提下完成的。而不是试了，然后呢？再是下一个。中国的互联网大量资本注入，已经不允许我们万事浅尝辄止，应该可以精细剖析。这也是现在所谓精细化运营的重要体现。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>上周做了一次关于北极星的回顾，一直想写也忘记了。今天提起来，其中印象比较深的部分还是在讨论这次送iPhone活动的成效。</p>\n<p>复盘的结果是这样的：总的来讲，本次活动的成效还是比较好的，收获了大量的新用户注册。但是，如果说病毒效应的话，这次活动并不算事一个很好的病毒效应传播。从数据上看，并未达到预期的增长效果。</p>\n<p>总结复盘，看了一下这篇知乎上的文章，我认为说的非常棒。<br><a href=\"http://www.zhihu.com/question/20901811\" target=\"_blank\" rel=\"noopener\">http://www.zhihu.com/question/20901811</a></p>\n<p>“在facebook出现之前，有sixdegrees, Friendster, Myspace之类的社交网站，有一本书上说：facebook之所以能打败Friendster之类是因为用到了病毒式传播。如下图, 病毒式传播开始一般是线性增长,到到达临界点(Critical mass),然后以指数方式增长,直到饱和点。如果没办法到达临界点,一段周期后,就废了。”</p>\n<p>觉得这一段话比较准确的说明了病毒式增长的方法和意义。下面这张图是原文中的曲线表述。这个图形，和我们自己的活动图形还是颇为相似。但是如何界定什么是“Saturation”饱和点，什么事“Critical mass”这个问题上，我还是有些困惑。关于我们图形里的第二个波峰，是哪一个点，还不是很清楚。我的感觉是，还需要一、两个活动，进行持续观察，寻找图像之间的拟合系数。通过这种曲线拟合关系，也许可以寻找到医疗互联网的一些有意思的数据。也就是这大约280万医生的市场运作数据。</p>\n<p>希望大家每一个人和我一样，每周对数据关注一点点。作为一家互联网公司，数据驱动，才是核心。身为精英文化，目前的我不完全认同“无脑强运营”的文化，更加倾向于精英就是努力寻找对的事情，用数据说话，把方向找准。都说互联网没有对错，只有试一试。我觉得这是对的，但应该在数据驱动前提下完成的。而不是试了，然后呢？再是下一个。中国的互联网大量资本注入，已经不允许我们万事浅尝辄止，应该可以精细剖析。这也是现在所谓精细化运营的重要体现。</p>\n"},{"title":"positioning","date":"2014-12-08T09:39:37.000Z","_content":"给自己一个定位。在杏树林做CTO有一段日子了，过去的很长一段时间，都在做着自己最擅长的事情。我承认，这个对现在的杏树林来讲是非常重要的，无论是流程改造，团队结构化，前端技术和团队精益化的建立，包括对产品经理运营团队的培训，这些东西对我来讲确实太熟悉了，虽然有各种不容易和辛苦，但无论是思路、方法还是执行，都是我擅长的。那么接下来，要做点什么特别的了，我觉得。\n\n最近大约几个月前成立了公司的数据平台中心，好吧，这个中心目前确实还很初级，初级到我们甚至不知道应该以一个什么形式做项目。但是不断的摸索，数据平台中心在成长，我们开始越来越有门路，包括我自己也在成长，是时候给自己接下来的几年做个定位。作为一个CTO，一个行业领域的CTO数据是行业的核心，更是技术的核心。大家都在谈大数据，我不觉得自己在做什么大数据，更多的是研究数据，研究医疗，我希望自己在有限的时间里能够快速成长为一个医疗互联网中最了解数据的人。目标包括以下几个方面：医生的行为数据，病例的基本资料数据，那么接下来还有什么，拭目以待吧。\n\n为什么，我认为自己应该是这个定位，刚刚也说了，作为一个CTO，我是技术团队或者是杏树林发展方向的重要领航者。领航员用罗盘仪和地图领航，而我所有的就是纷繁复杂的数据，虽然他们还不是一个完整的地图。其二，三年专业咨询师的训练，对学习的敏锐和对知识的快速拼接能力是我最起码的职业素养。其三，身为一个从小就是数学爱好者，博士研究数据的数字敏感人士，这是我的最爱。责任，教育和兴趣。Just do it!\n\n我希望：王哲，医疗互联网数据专家，杏树林CTO","source":"_posts/positioning.md","raw":"title: positioning\ndate: 2014-12-08 17:39:37\ncategories:\n- Management\ntags:\n- mgnt\n---\n给自己一个定位。在杏树林做CTO有一段日子了，过去的很长一段时间，都在做着自己最擅长的事情。我承认，这个对现在的杏树林来讲是非常重要的，无论是流程改造，团队结构化，前端技术和团队精益化的建立，包括对产品经理运营团队的培训，这些东西对我来讲确实太熟悉了，虽然有各种不容易和辛苦，但无论是思路、方法还是执行，都是我擅长的。那么接下来，要做点什么特别的了，我觉得。\n\n最近大约几个月前成立了公司的数据平台中心，好吧，这个中心目前确实还很初级，初级到我们甚至不知道应该以一个什么形式做项目。但是不断的摸索，数据平台中心在成长，我们开始越来越有门路，包括我自己也在成长，是时候给自己接下来的几年做个定位。作为一个CTO，一个行业领域的CTO数据是行业的核心，更是技术的核心。大家都在谈大数据，我不觉得自己在做什么大数据，更多的是研究数据，研究医疗，我希望自己在有限的时间里能够快速成长为一个医疗互联网中最了解数据的人。目标包括以下几个方面：医生的行为数据，病例的基本资料数据，那么接下来还有什么，拭目以待吧。\n\n为什么，我认为自己应该是这个定位，刚刚也说了，作为一个CTO，我是技术团队或者是杏树林发展方向的重要领航者。领航员用罗盘仪和地图领航，而我所有的就是纷繁复杂的数据，虽然他们还不是一个完整的地图。其二，三年专业咨询师的训练，对学习的敏锐和对知识的快速拼接能力是我最起码的职业素养。其三，身为一个从小就是数学爱好者，博士研究数据的数字敏感人士，这是我的最爱。责任，教育和兴趣。Just do it!\n\n我希望：王哲，医疗互联网数据专家，杏树林CTO","slug":"positioning","published":1,"updated":"2018-04-16T09:59:37.072Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcow000trlfyeo0vtb90","content":"<p>给自己一个定位。在杏树林做CTO有一段日子了，过去的很长一段时间，都在做着自己最擅长的事情。我承认，这个对现在的杏树林来讲是非常重要的，无论是流程改造，团队结构化，前端技术和团队精益化的建立，包括对产品经理运营团队的培训，这些东西对我来讲确实太熟悉了，虽然有各种不容易和辛苦，但无论是思路、方法还是执行，都是我擅长的。那么接下来，要做点什么特别的了，我觉得。</p>\n<p>最近大约几个月前成立了公司的数据平台中心，好吧，这个中心目前确实还很初级，初级到我们甚至不知道应该以一个什么形式做项目。但是不断的摸索，数据平台中心在成长，我们开始越来越有门路，包括我自己也在成长，是时候给自己接下来的几年做个定位。作为一个CTO，一个行业领域的CTO数据是行业的核心，更是技术的核心。大家都在谈大数据，我不觉得自己在做什么大数据，更多的是研究数据，研究医疗，我希望自己在有限的时间里能够快速成长为一个医疗互联网中最了解数据的人。目标包括以下几个方面：医生的行为数据，病例的基本资料数据，那么接下来还有什么，拭目以待吧。</p>\n<p>为什么，我认为自己应该是这个定位，刚刚也说了，作为一个CTO，我是技术团队或者是杏树林发展方向的重要领航者。领航员用罗盘仪和地图领航，而我所有的就是纷繁复杂的数据，虽然他们还不是一个完整的地图。其二，三年专业咨询师的训练，对学习的敏锐和对知识的快速拼接能力是我最起码的职业素养。其三，身为一个从小就是数学爱好者，博士研究数据的数字敏感人士，这是我的最爱。责任，教育和兴趣。Just do it!</p>\n<p>我希望：王哲，医疗互联网数据专家，杏树林CTO</p>\n","site":{"data":{}},"excerpt":"","more":"<p>给自己一个定位。在杏树林做CTO有一段日子了，过去的很长一段时间，都在做着自己最擅长的事情。我承认，这个对现在的杏树林来讲是非常重要的，无论是流程改造，团队结构化，前端技术和团队精益化的建立，包括对产品经理运营团队的培训，这些东西对我来讲确实太熟悉了，虽然有各种不容易和辛苦，但无论是思路、方法还是执行，都是我擅长的。那么接下来，要做点什么特别的了，我觉得。</p>\n<p>最近大约几个月前成立了公司的数据平台中心，好吧，这个中心目前确实还很初级，初级到我们甚至不知道应该以一个什么形式做项目。但是不断的摸索，数据平台中心在成长，我们开始越来越有门路，包括我自己也在成长，是时候给自己接下来的几年做个定位。作为一个CTO，一个行业领域的CTO数据是行业的核心，更是技术的核心。大家都在谈大数据，我不觉得自己在做什么大数据，更多的是研究数据，研究医疗，我希望自己在有限的时间里能够快速成长为一个医疗互联网中最了解数据的人。目标包括以下几个方面：医生的行为数据，病例的基本资料数据，那么接下来还有什么，拭目以待吧。</p>\n<p>为什么，我认为自己应该是这个定位，刚刚也说了，作为一个CTO，我是技术团队或者是杏树林发展方向的重要领航者。领航员用罗盘仪和地图领航，而我所有的就是纷繁复杂的数据，虽然他们还不是一个完整的地图。其二，三年专业咨询师的训练，对学习的敏锐和对知识的快速拼接能力是我最起码的职业素养。其三，身为一个从小就是数学爱好者，博士研究数据的数字敏感人士，这是我的最爱。责任，教育和兴趣。Just do it!</p>\n<p>我希望：王哲，医疗互联网数据专家，杏树林CTO</p>\n"},{"title":"准备Design Thinking","date":"2016-01-04T04:17:44.000Z","cataories":["Diary"],"_content":"\n为了周五的演讲，开始准备Design Thinking方面的材料。以前虽然参加过，也做过不少相关workshop，但是真的说到理论的学习确实非常有限的。这里做一些学习总结。同时，我想考虑找人聊一聊，比如海生，看看他能不能给我推荐几个这方面的人进行一些学习。那么现在开始，关于Design Thinking。\n\n知乎上有一片著名的文章，https://www.zhihu.com/question/21481878/answer/30543958。\n\n我的学习就从这里开始（我要说明的是，这不是一片摘抄，而是一个对比，自己也曾经参与了大量产品设计方面的实际工作，所以感觉这样的对比可能更佳有效）下面是我做的关于Design Thinking方案的幻灯片，和大家一起分享\n\n{% asset_img 幻灯片01.jpg \"slide01\" %}\n\n{% asset_img 幻灯片02.jpg \"slide02\" %}\n\n{% asset_img 幻灯片03.jpg \"slide03\" %}\n\n{% asset_img 幻灯片04.jpg \"slide04\" %}\n\n{% asset_img 幻灯片05.jpg \"slide05\" %}\n\n{% asset_img 幻灯片06.jpg \"slide06\" %}\n\n{% asset_img 幻灯片07.jpg \"slide07\" %}\n\n{% asset_img 幻灯片08.jpg \"slide08\" %}\n\n{% asset_img 幻灯片09.jpg \"slide09\" %}\n\n{% asset_img 幻灯片10.jpg \"slide10\" %}\n\n{% asset_img 幻灯片11.jpg \"slide11\" %}\n\n{% asset_img 幻灯片12.jpg \"slide12\" %}\n\n{% asset_img 幻灯片13.jpg \"slide13\" %}\n\n{% asset_img 幻灯片14.jpg \"slide14\" %}\n\n{% asset_img 幻灯片15.jpg \"slide15\" %}\n\n{% asset_img 幻灯片16.jpg \"slide16\" %}\n\n{% asset_img 幻灯片17.jpg \"slide17\" %}\n\n{% asset_img 幻灯片18.jpg \"slide18\" %}\n\n{% asset_img 幻灯片19.jpg \"slide19\" %}\n\n{% asset_img 幻灯片20.jpg \"slide20\" %}\n\n{% asset_img 幻灯片21.jpg \"slide21\" %}\n\n{% asset_img 幻灯片22.jpg \"slide22\" %}\n\n{% asset_img 幻灯片23.jpg \"slide23\" %}\n\n{% asset_img 幻灯片24.jpg \"slide24\" %}\n\n{% asset_img 幻灯片25.jpg \"slide25\" %}\n\n{% asset_img 幻灯片26.jpg \"slide26\" %}\n\n{% asset_img 幻灯片27.jpg \"slide27\" %}\n\n{% asset_img 幻灯片28.jpg \"slide28\" %}\n","source":"_posts/prepare-design-thinking.md","raw":"title: 准备Design Thinking\ndate: 2016-01-04 12:17:44\ncataories:\n- Diary\ntags:\n- tech\n---\n\n为了周五的演讲，开始准备Design Thinking方面的材料。以前虽然参加过，也做过不少相关workshop，但是真的说到理论的学习确实非常有限的。这里做一些学习总结。同时，我想考虑找人聊一聊，比如海生，看看他能不能给我推荐几个这方面的人进行一些学习。那么现在开始，关于Design Thinking。\n\n知乎上有一片著名的文章，https://www.zhihu.com/question/21481878/answer/30543958。\n\n我的学习就从这里开始（我要说明的是，这不是一片摘抄，而是一个对比，自己也曾经参与了大量产品设计方面的实际工作，所以感觉这样的对比可能更佳有效）下面是我做的关于Design Thinking方案的幻灯片，和大家一起分享\n\n{% asset_img 幻灯片01.jpg \"slide01\" %}\n\n{% asset_img 幻灯片02.jpg \"slide02\" %}\n\n{% asset_img 幻灯片03.jpg \"slide03\" %}\n\n{% asset_img 幻灯片04.jpg \"slide04\" %}\n\n{% asset_img 幻灯片05.jpg \"slide05\" %}\n\n{% asset_img 幻灯片06.jpg \"slide06\" %}\n\n{% asset_img 幻灯片07.jpg \"slide07\" %}\n\n{% asset_img 幻灯片08.jpg \"slide08\" %}\n\n{% asset_img 幻灯片09.jpg \"slide09\" %}\n\n{% asset_img 幻灯片10.jpg \"slide10\" %}\n\n{% asset_img 幻灯片11.jpg \"slide11\" %}\n\n{% asset_img 幻灯片12.jpg \"slide12\" %}\n\n{% asset_img 幻灯片13.jpg \"slide13\" %}\n\n{% asset_img 幻灯片14.jpg \"slide14\" %}\n\n{% asset_img 幻灯片15.jpg \"slide15\" %}\n\n{% asset_img 幻灯片16.jpg \"slide16\" %}\n\n{% asset_img 幻灯片17.jpg \"slide17\" %}\n\n{% asset_img 幻灯片18.jpg \"slide18\" %}\n\n{% asset_img 幻灯片19.jpg \"slide19\" %}\n\n{% asset_img 幻灯片20.jpg \"slide20\" %}\n\n{% asset_img 幻灯片21.jpg \"slide21\" %}\n\n{% asset_img 幻灯片22.jpg \"slide22\" %}\n\n{% asset_img 幻灯片23.jpg \"slide23\" %}\n\n{% asset_img 幻灯片24.jpg \"slide24\" %}\n\n{% asset_img 幻灯片25.jpg \"slide25\" %}\n\n{% asset_img 幻灯片26.jpg \"slide26\" %}\n\n{% asset_img 幻灯片27.jpg \"slide27\" %}\n\n{% asset_img 幻灯片28.jpg \"slide28\" %}\n","slug":"prepare-design-thinking","published":1,"updated":"2018-04-16T09:59:37.034Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcox000urlfyju6c2vyi","content":"<p>为了周五的演讲，开始准备Design Thinking方面的材料。以前虽然参加过，也做过不少相关workshop，但是真的说到理论的学习确实非常有限的。这里做一些学习总结。同时，我想考虑找人聊一聊，比如海生，看看他能不能给我推荐几个这方面的人进行一些学习。那么现在开始，关于Design Thinking。</p>\n<p>知乎上有一片著名的文章，<a href=\"https://www.zhihu.com/question/21481878/answer/30543958。\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/21481878/answer/30543958。</a></p>\n<p>我的学习就从这里开始（我要说明的是，这不是一片摘抄，而是一个对比，自己也曾经参与了大量产品设计方面的实际工作，所以感觉这样的对比可能更佳有效）下面是我做的关于Design Thinking方案的幻灯片，和大家一起分享</p>\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片01.jpg\" title=\"slide01\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片02.jpg\" title=\"slide02\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片03.jpg\" title=\"slide03\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片04.jpg\" title=\"slide04\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片05.jpg\" title=\"slide05\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片06.jpg\" title=\"slide06\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片07.jpg\" title=\"slide07\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片08.jpg\" title=\"slide08\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片09.jpg\" title=\"slide09\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片10.jpg\" title=\"slide10\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片11.jpg\" title=\"slide11\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片12.jpg\" title=\"slide12\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片13.jpg\" title=\"slide13\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片14.jpg\" title=\"slide14\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片15.jpg\" title=\"slide15\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片16.jpg\" title=\"slide16\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片17.jpg\" title=\"slide17\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片18.jpg\" title=\"slide18\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片19.jpg\" title=\"slide19\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片20.jpg\" title=\"slide20\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片21.jpg\" title=\"slide21\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片22.jpg\" title=\"slide22\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片23.jpg\" title=\"slide23\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片24.jpg\" title=\"slide24\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片25.jpg\" title=\"slide25\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片26.jpg\" title=\"slide26\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片27.jpg\" title=\"slide27\">\n\n","site":{"data":{}},"excerpt":"","more":"<p>为了周五的演讲，开始准备Design Thinking方面的材料。以前虽然参加过，也做过不少相关workshop，但是真的说到理论的学习确实非常有限的。这里做一些学习总结。同时，我想考虑找人聊一聊，比如海生，看看他能不能给我推荐几个这方面的人进行一些学习。那么现在开始，关于Design Thinking。</p>\n<p>知乎上有一片著名的文章，<a href=\"https://www.zhihu.com/question/21481878/answer/30543958。\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/21481878/answer/30543958。</a></p>\n<p>我的学习就从这里开始（我要说明的是，这不是一片摘抄，而是一个对比，自己也曾经参与了大量产品设计方面的实际工作，所以感觉这样的对比可能更佳有效）下面是我做的关于Design Thinking方案的幻灯片，和大家一起分享</p>\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片01.jpg\" title=\"slide01\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片02.jpg\" title=\"slide02\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片03.jpg\" title=\"slide03\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片04.jpg\" title=\"slide04\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片05.jpg\" title=\"slide05\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片06.jpg\" title=\"slide06\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片07.jpg\" title=\"slide07\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片08.jpg\" title=\"slide08\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片09.jpg\" title=\"slide09\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片10.jpg\" title=\"slide10\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片11.jpg\" title=\"slide11\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片12.jpg\" title=\"slide12\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片13.jpg\" title=\"slide13\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片14.jpg\" title=\"slide14\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片15.jpg\" title=\"slide15\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片16.jpg\" title=\"slide16\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片17.jpg\" title=\"slide17\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片18.jpg\" title=\"slide18\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片19.jpg\" title=\"slide19\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片20.jpg\" title=\"slide20\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片21.jpg\" title=\"slide21\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片22.jpg\" title=\"slide22\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片23.jpg\" title=\"slide23\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片24.jpg\" title=\"slide24\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片25.jpg\" title=\"slide25\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片26.jpg\" title=\"slide26\">\n<img src=\"/2016/01/04/prepare-design-thinking/幻灯片27.jpg\" title=\"slide27\">\n\n"},{"title":"关于HIPAA","date":"2015-12-17T06:09:48.000Z","_content":"最近因为工作的关系，必须系统的学习一下HIPAA，把学习结果给大家分享一下，如下：\n\n### 第一部分：什么是HIPAA\n特别词汇说明（terms）：\nPortability：这里特别说明，根据wiki在社会保险（social security）方面的解释，所谓携带型是保存（preserve）、维护（maintain）和交换（transfer）三个词的合意总称，因为很没有中文对应意思，所以特此解释，后面还是翻译成可携带［1］。\n\nHIPAA，全称Health Insurance Portability and Accountability Act. 是美国关于健康保险的携带和责任的法案。HIPAA的提出，旨在改革健康医疗产业，降低费用，简化管理过程和负担，增强隐私保护和个人信息安全保护。可以说，HIPAA是美国健康卫生领域的基础大法，也是整个相关后续政策的基石。自从1996年HIPAA被正式提出以后，法案经历了5次比较大的更新，分别对医疗卫生领域在保险和医疗管理方面的工作，进行了一系列逐渐细致化和现代化的制约。\n\n{% asset_img evolution1996.jpg \"1996年第一次正式形成法案\" %}\n{% asset_img evolution1997.jpg \"1997年引入保险改革\" %}\n{% asset_img evolution2009.jpg \"2009年引入ARRA／HITECH法案\" %}\n在美国2009年更新了HIPAA，引入了美国复苏与再投资法案ARRA（American Recovery and Reinvestment Act），第一次开始引入HITECH（全称Health Information Technology for Economic and Clinic Health Act）。该法案旨在拓展对EHR（Electronic Health Records） 的使用，并开始对ePHI进行了大量升级，详细解释了HIPAA对隐私和安全的保护，同时增加了更多强制实施的内容和相对应的不符合情况下的惩罚措施。该法案与2010年2月强制生效。\n\n{% asset_img evolution2013.jpg \"2013年由美国卫生与公共关系部下发Omnibus政策\" %}\n之后，在2013年1月，由美国卫生与公共服务部（Department of Health and Human）最终发布了Omnibus Rule，将过去的HITECH和GINA（Generic Information Nondiscrimination Act of 2008）进行了融合，并增加了更多详细的指导细节。虽然作为行政命令，Omnibus Rule并不是法案，但是它更加细则。政令与2013年9月正式施行。所有non-compilance将会收到非常严重的惩罚。\n\nHIPAA会影响到两类组群\nA. 健康覆盖实体－医疗健康提供者（如医生、护士）、医疗健康规划者（如保险公司、雇主、政府）、医疗健康数据清洗者\nB. 医疗健康业务工作组织－所有为覆盖实体提供服务的第三方合作伙伴（如律师、会计、医疗公司、咨询师等）\n\n### 第二部分，关于的组成\n\n下图描述了HIPAA的主要结果组成。\n\n{% asset_img hippa.jpg \"HIPAA的结构\" %}\n\nHIPAA与所有涉及健康相关的医疗、保险和个人都有直接关系。法案主要分为五个主题，合并为两大部分组成，分别是简化管理（administrative simplification）［2］和保险改革（insurance reform），这两个部分也分别对应了HIPAA的两个关键要求，前者是责任（Accountability），后者对应可携带（Portability）。通常意义上大家所说的HIPAA，都是指的管理责任这部分。如图所示主要包括三个组成子模块儿：\n1）事务、代码集和身份识别\n*\t关于事务，2002年启用的HIPAA关于事务（Transaction）的规定要求，完整的事物要求包含以下内容：\n提交和收费处信息\n是否任何参与“Health Plan”计划\n准入性、覆盖范围、救济金等\n需要给健康计划提供者的付费内容\n健康保险计划之外的附加费用\n状态请求和相应\n相关证书以及认证\n\n*\t关于代码集（Code Sets）部分，正如HIPAA的初衷，规定代码集大大方便了系统的交互。标准的代码，帮助系统设计之间进行有效的沟通。该部分包含5大集合，主要是为了描述疾病（desease），创伤（injury），症状（Symptoms）和操作行为（Actions）分别是：\nICD-9-CM 国际疾病分类（International Classification of Disease）\nCPT （Physician Current Procedural Terminology）\nHCPCS HCFA操作代码系统（HCFA Procedural Coding System）包括设备、诊断、治疗、管理疾病等\nCDT 现代牙医术语（Current Dental Terminology）\nNDC 国家药品代码（National Drug Codes）\n\n*\t关于独立标示（Unique Identifiers）用来表明患者、供应商、赔款人和雇主\nProviderID：给所有健康服务供应商的10位数字ID号码\nEmployerID：所有为健康医疗提供资金的雇主，由9位数字表示（EIN）\nPayerID：所有为健康医疗服务付钱的组织，由9位数字表示（EIN）\nPatientID：所有接受服务的患者，目前尚在国会（congress）讨论中，主要是Social Security Number已经存在，是不是还需要一个独立的ID存在争议。\n\n2）隐私－任何形式下的健康信息保护（PHI）\n美国卫生及公共服务部（Department of Health and Human Services）［4］在2000年颁布，2002年修改了个人医疗信息隐私政策，明确了医疗实体禁止使用和披露个人受保护健康信息（Protected Health Information）。要求受保护的内容包括电子病历，纸质病历和口头沟通。同时HIPAA作为联邦政府法案，是各州遵循但不限于的基础。各州有权利要求医疗供应商等遵循更加严格的州法案。\n\nPHI包含所有个人可辨识健康信息，包含任何形式过去、现在和可能未来的健康情况。（纸质电子病历、医疗交费记录，包括抄写副本）, 其中有一个非常重要的部分叫做个人可辨识信息（Individually Identifiable Information）\n－姓名（Name）\n－住址（Address）\n－电子邮件（E-mail）\n－日期（Dates）\n－账户号（Account Number）\n－证书号（Certification Number）\n－驾照（License Number）\n－车证（Vehicle Number）\n－社会保险号（Social Security Number）\n－病历号（Medical Record Number）\n－健康医疗保险号（Health Plan Beneficiary Number）\n－面部信息（Facial Photograph）\n－电话号码（Telephone Number）\n－网络地址（URLs）\n－网络IP地址（IP Address）\n－生物身份识别（Biometric Identification）\n－其他独立识别码\n\n了解了这个部分，我们就可以明白为什么HIPAA的一个非常有意思的规定，PHI的使用和暴露可以在两种情况下被使用。\n*\t被授权情况 \n*\t祛标示信息情况\n\n也就是说在祛标示信息（De-identity）后，其实所有的数据就可以被Freely的使用不收到太多限制。\n\n3）安全－电子信息形式下的健康信息保护（ePHI）\n{% asset_img security.jpg \"安全\" %}\n虽然安全被分成了三个级别，但是作为一个技术同学，我还是重点看了看技术安全的五个方面：\n+\t访问控制 （ACCESS Control）\n+\t审计控制（Audit Controls（R））\n+\t数据完整性（Integrity）\n+\t认证（Person and Entity Authentication）\n+\t传输安全（Transmission Security）\n\n隐私和安全的区别：\n隐私关注个人受保护健康信息（PHI）的控制和使用权力。任何形式的该信息不得在未经授权的情况下进行暴露。\n安全特指电子受保护健康信息（ePHI）的防护标准。防止该信息在未经授权情况下被暴露、破坏或丢失\n隐私信息部分依赖于安全来确保被保护。\n\n参考文献：\n［1］Portability (social security) https://en.wikipedia.org/wiki/Portability_(social_security)\n［2］HIPPA http://baike.baidu.com/link?url=FeWuYF_ezrLt1Cg1v94jfq6zb2GfplUzM4FtURhfhMRYmKn3Hmu8erdzR9m7SG0iD9jDZcTM0ARPDOQxY-1QPK\n［3］Pre Existing Conditions - Understanding Exclusions and Creditable Coverage  http://healthinsurance.about.com/od/healthinsurancebasics/a/preexisting_conditions_overview.html\n［4］美国卫生及公共服务部 https://zh.wikipedia.org/wiki/%E7%BE%8E%E5%9B%BD%E5%8D%AB%E7%94%9F%E5%8F%8A%E5%85%AC%E5%85%B1%E6%9C%8D%E5%8A%A1%E9%83%A8\n","source":"_posts/hippa-part1.md","raw":"title: 关于HIPAA\ndate: 2015-12-17 14:09:48\ncategories:\n- Technology\ntags:\n- security\n---\n最近因为工作的关系，必须系统的学习一下HIPAA，把学习结果给大家分享一下，如下：\n\n### 第一部分：什么是HIPAA\n特别词汇说明（terms）：\nPortability：这里特别说明，根据wiki在社会保险（social security）方面的解释，所谓携带型是保存（preserve）、维护（maintain）和交换（transfer）三个词的合意总称，因为很没有中文对应意思，所以特此解释，后面还是翻译成可携带［1］。\n\nHIPAA，全称Health Insurance Portability and Accountability Act. 是美国关于健康保险的携带和责任的法案。HIPAA的提出，旨在改革健康医疗产业，降低费用，简化管理过程和负担，增强隐私保护和个人信息安全保护。可以说，HIPAA是美国健康卫生领域的基础大法，也是整个相关后续政策的基石。自从1996年HIPAA被正式提出以后，法案经历了5次比较大的更新，分别对医疗卫生领域在保险和医疗管理方面的工作，进行了一系列逐渐细致化和现代化的制约。\n\n{% asset_img evolution1996.jpg \"1996年第一次正式形成法案\" %}\n{% asset_img evolution1997.jpg \"1997年引入保险改革\" %}\n{% asset_img evolution2009.jpg \"2009年引入ARRA／HITECH法案\" %}\n在美国2009年更新了HIPAA，引入了美国复苏与再投资法案ARRA（American Recovery and Reinvestment Act），第一次开始引入HITECH（全称Health Information Technology for Economic and Clinic Health Act）。该法案旨在拓展对EHR（Electronic Health Records） 的使用，并开始对ePHI进行了大量升级，详细解释了HIPAA对隐私和安全的保护，同时增加了更多强制实施的内容和相对应的不符合情况下的惩罚措施。该法案与2010年2月强制生效。\n\n{% asset_img evolution2013.jpg \"2013年由美国卫生与公共关系部下发Omnibus政策\" %}\n之后，在2013年1月，由美国卫生与公共服务部（Department of Health and Human）最终发布了Omnibus Rule，将过去的HITECH和GINA（Generic Information Nondiscrimination Act of 2008）进行了融合，并增加了更多详细的指导细节。虽然作为行政命令，Omnibus Rule并不是法案，但是它更加细则。政令与2013年9月正式施行。所有non-compilance将会收到非常严重的惩罚。\n\nHIPAA会影响到两类组群\nA. 健康覆盖实体－医疗健康提供者（如医生、护士）、医疗健康规划者（如保险公司、雇主、政府）、医疗健康数据清洗者\nB. 医疗健康业务工作组织－所有为覆盖实体提供服务的第三方合作伙伴（如律师、会计、医疗公司、咨询师等）\n\n### 第二部分，关于的组成\n\n下图描述了HIPAA的主要结果组成。\n\n{% asset_img hippa.jpg \"HIPAA的结构\" %}\n\nHIPAA与所有涉及健康相关的医疗、保险和个人都有直接关系。法案主要分为五个主题，合并为两大部分组成，分别是简化管理（administrative simplification）［2］和保险改革（insurance reform），这两个部分也分别对应了HIPAA的两个关键要求，前者是责任（Accountability），后者对应可携带（Portability）。通常意义上大家所说的HIPAA，都是指的管理责任这部分。如图所示主要包括三个组成子模块儿：\n1）事务、代码集和身份识别\n*\t关于事务，2002年启用的HIPAA关于事务（Transaction）的规定要求，完整的事物要求包含以下内容：\n提交和收费处信息\n是否任何参与“Health Plan”计划\n准入性、覆盖范围、救济金等\n需要给健康计划提供者的付费内容\n健康保险计划之外的附加费用\n状态请求和相应\n相关证书以及认证\n\n*\t关于代码集（Code Sets）部分，正如HIPAA的初衷，规定代码集大大方便了系统的交互。标准的代码，帮助系统设计之间进行有效的沟通。该部分包含5大集合，主要是为了描述疾病（desease），创伤（injury），症状（Symptoms）和操作行为（Actions）分别是：\nICD-9-CM 国际疾病分类（International Classification of Disease）\nCPT （Physician Current Procedural Terminology）\nHCPCS HCFA操作代码系统（HCFA Procedural Coding System）包括设备、诊断、治疗、管理疾病等\nCDT 现代牙医术语（Current Dental Terminology）\nNDC 国家药品代码（National Drug Codes）\n\n*\t关于独立标示（Unique Identifiers）用来表明患者、供应商、赔款人和雇主\nProviderID：给所有健康服务供应商的10位数字ID号码\nEmployerID：所有为健康医疗提供资金的雇主，由9位数字表示（EIN）\nPayerID：所有为健康医疗服务付钱的组织，由9位数字表示（EIN）\nPatientID：所有接受服务的患者，目前尚在国会（congress）讨论中，主要是Social Security Number已经存在，是不是还需要一个独立的ID存在争议。\n\n2）隐私－任何形式下的健康信息保护（PHI）\n美国卫生及公共服务部（Department of Health and Human Services）［4］在2000年颁布，2002年修改了个人医疗信息隐私政策，明确了医疗实体禁止使用和披露个人受保护健康信息（Protected Health Information）。要求受保护的内容包括电子病历，纸质病历和口头沟通。同时HIPAA作为联邦政府法案，是各州遵循但不限于的基础。各州有权利要求医疗供应商等遵循更加严格的州法案。\n\nPHI包含所有个人可辨识健康信息，包含任何形式过去、现在和可能未来的健康情况。（纸质电子病历、医疗交费记录，包括抄写副本）, 其中有一个非常重要的部分叫做个人可辨识信息（Individually Identifiable Information）\n－姓名（Name）\n－住址（Address）\n－电子邮件（E-mail）\n－日期（Dates）\n－账户号（Account Number）\n－证书号（Certification Number）\n－驾照（License Number）\n－车证（Vehicle Number）\n－社会保险号（Social Security Number）\n－病历号（Medical Record Number）\n－健康医疗保险号（Health Plan Beneficiary Number）\n－面部信息（Facial Photograph）\n－电话号码（Telephone Number）\n－网络地址（URLs）\n－网络IP地址（IP Address）\n－生物身份识别（Biometric Identification）\n－其他独立识别码\n\n了解了这个部分，我们就可以明白为什么HIPAA的一个非常有意思的规定，PHI的使用和暴露可以在两种情况下被使用。\n*\t被授权情况 \n*\t祛标示信息情况\n\n也就是说在祛标示信息（De-identity）后，其实所有的数据就可以被Freely的使用不收到太多限制。\n\n3）安全－电子信息形式下的健康信息保护（ePHI）\n{% asset_img security.jpg \"安全\" %}\n虽然安全被分成了三个级别，但是作为一个技术同学，我还是重点看了看技术安全的五个方面：\n+\t访问控制 （ACCESS Control）\n+\t审计控制（Audit Controls（R））\n+\t数据完整性（Integrity）\n+\t认证（Person and Entity Authentication）\n+\t传输安全（Transmission Security）\n\n隐私和安全的区别：\n隐私关注个人受保护健康信息（PHI）的控制和使用权力。任何形式的该信息不得在未经授权的情况下进行暴露。\n安全特指电子受保护健康信息（ePHI）的防护标准。防止该信息在未经授权情况下被暴露、破坏或丢失\n隐私信息部分依赖于安全来确保被保护。\n\n参考文献：\n［1］Portability (social security) https://en.wikipedia.org/wiki/Portability_(social_security)\n［2］HIPPA http://baike.baidu.com/link?url=FeWuYF_ezrLt1Cg1v94jfq6zb2GfplUzM4FtURhfhMRYmKn3Hmu8erdzR9m7SG0iD9jDZcTM0ARPDOQxY-1QPK\n［3］Pre Existing Conditions - Understanding Exclusions and Creditable Coverage  http://healthinsurance.about.com/od/healthinsurancebasics/a/preexisting_conditions_overview.html\n［4］美国卫生及公共服务部 https://zh.wikipedia.org/wiki/%E7%BE%8E%E5%9B%BD%E5%8D%AB%E7%94%9F%E5%8F%8A%E5%85%AC%E5%85%B1%E6%9C%8D%E5%8A%A1%E9%83%A8\n","slug":"hippa-part1","published":1,"updated":"2018-04-16T09:59:37.034Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcox000vrlfy06dgucxl","content":"<p>最近因为工作的关系，必须系统的学习一下HIPAA，把学习结果给大家分享一下，如下：</p>\n<h3 id=\"第一部分：什么是HIPAA\"><a href=\"#第一部分：什么是HIPAA\" class=\"headerlink\" title=\"第一部分：什么是HIPAA\"></a>第一部分：什么是HIPAA</h3><p>特别词汇说明（terms）：<br>Portability：这里特别说明，根据wiki在社会保险（social security）方面的解释，所谓携带型是保存（preserve）、维护（maintain）和交换（transfer）三个词的合意总称，因为很没有中文对应意思，所以特此解释，后面还是翻译成可携带［1］。</p>\n<p>HIPAA，全称Health Insurance Portability and Accountability Act. 是美国关于健康保险的携带和责任的法案。HIPAA的提出，旨在改革健康医疗产业，降低费用，简化管理过程和负担，增强隐私保护和个人信息安全保护。可以说，HIPAA是美国健康卫生领域的基础大法，也是整个相关后续政策的基石。自从1996年HIPAA被正式提出以后，法案经历了5次比较大的更新，分别对医疗卫生领域在保险和医疗管理方面的工作，进行了一系列逐渐细致化和现代化的制约。</p>\n<img src=\"/2015/12/17/hippa-part1/evolution1996.jpg\" title=\"1996年第一次正式形成法案\">\n<img src=\"/2015/12/17/hippa-part1/evolution1997.jpg\" title=\"1997年引入保险改革\">\n<img src=\"/2015/12/17/hippa-part1/evolution2009.jpg\" title=\"2009年引入ARRA／HITECH法案\">\n<p>在美国2009年更新了HIPAA，引入了美国复苏与再投资法案ARRA（American Recovery and Reinvestment Act），第一次开始引入HITECH（全称Health Information Technology for Economic and Clinic Health Act）。该法案旨在拓展对EHR（Electronic Health Records） 的使用，并开始对ePHI进行了大量升级，详细解释了HIPAA对隐私和安全的保护，同时增加了更多强制实施的内容和相对应的不符合情况下的惩罚措施。该法案与2010年2月强制生效。</p>\n<img src=\"/2015/12/17/hippa-part1/evolution2013.jpg\" title=\"2013年由美国卫生与公共关系部下发Omnibus政策\">\n<p>之后，在2013年1月，由美国卫生与公共服务部（Department of Health and Human）最终发布了Omnibus Rule，将过去的HITECH和GINA（Generic Information Nondiscrimination Act of 2008）进行了融合，并增加了更多详细的指导细节。虽然作为行政命令，Omnibus Rule并不是法案，但是它更加细则。政令与2013年9月正式施行。所有non-compilance将会收到非常严重的惩罚。</p>\n<p>HIPAA会影响到两类组群<br>A. 健康覆盖实体－医疗健康提供者（如医生、护士）、医疗健康规划者（如保险公司、雇主、政府）、医疗健康数据清洗者<br>B. 医疗健康业务工作组织－所有为覆盖实体提供服务的第三方合作伙伴（如律师、会计、医疗公司、咨询师等）</p>\n<h3 id=\"第二部分，关于的组成\"><a href=\"#第二部分，关于的组成\" class=\"headerlink\" title=\"第二部分，关于的组成\"></a>第二部分，关于的组成</h3><p>下图描述了HIPAA的主要结果组成。</p>\n<img src=\"/2015/12/17/hippa-part1/hippa.jpg\" title=\"HIPAA的结构\">\n<p>HIPAA与所有涉及健康相关的医疗、保险和个人都有直接关系。法案主要分为五个主题，合并为两大部分组成，分别是简化管理（administrative simplification）［2］和保险改革（insurance reform），这两个部分也分别对应了HIPAA的两个关键要求，前者是责任（Accountability），后者对应可携带（Portability）。通常意义上大家所说的HIPAA，都是指的管理责任这部分。如图所示主要包括三个组成子模块儿：<br>1）事务、代码集和身份识别</p>\n<ul>\n<li><p>关于事务，2002年启用的HIPAA关于事务（Transaction）的规定要求，完整的事物要求包含以下内容：<br>提交和收费处信息<br>是否任何参与“Health Plan”计划<br>准入性、覆盖范围、救济金等<br>需要给健康计划提供者的付费内容<br>健康保险计划之外的附加费用<br>状态请求和相应<br>相关证书以及认证</p>\n</li>\n<li><p>关于代码集（Code Sets）部分，正如HIPAA的初衷，规定代码集大大方便了系统的交互。标准的代码，帮助系统设计之间进行有效的沟通。该部分包含5大集合，主要是为了描述疾病（desease），创伤（injury），症状（Symptoms）和操作行为（Actions）分别是：<br>ICD-9-CM 国际疾病分类（International Classification of Disease）<br>CPT （Physician Current Procedural Terminology）<br>HCPCS HCFA操作代码系统（HCFA Procedural Coding System）包括设备、诊断、治疗、管理疾病等<br>CDT 现代牙医术语（Current Dental Terminology）<br>NDC 国家药品代码（National Drug Codes）</p>\n</li>\n<li><p>关于独立标示（Unique Identifiers）用来表明患者、供应商、赔款人和雇主<br>ProviderID：给所有健康服务供应商的10位数字ID号码<br>EmployerID：所有为健康医疗提供资金的雇主，由9位数字表示（EIN）<br>PayerID：所有为健康医疗服务付钱的组织，由9位数字表示（EIN）<br>PatientID：所有接受服务的患者，目前尚在国会（congress）讨论中，主要是Social Security Number已经存在，是不是还需要一个独立的ID存在争议。</p>\n</li>\n</ul>\n<p>2）隐私－任何形式下的健康信息保护（PHI）<br>美国卫生及公共服务部（Department of Health and Human Services）［4］在2000年颁布，2002年修改了个人医疗信息隐私政策，明确了医疗实体禁止使用和披露个人受保护健康信息（Protected Health Information）。要求受保护的内容包括电子病历，纸质病历和口头沟通。同时HIPAA作为联邦政府法案，是各州遵循但不限于的基础。各州有权利要求医疗供应商等遵循更加严格的州法案。</p>\n<p>PHI包含所有个人可辨识健康信息，包含任何形式过去、现在和可能未来的健康情况。（纸质电子病历、医疗交费记录，包括抄写副本）, 其中有一个非常重要的部分叫做个人可辨识信息（Individually Identifiable Information）<br>－姓名（Name）<br>－住址（Address）<br>－电子邮件（E-mail）<br>－日期（Dates）<br>－账户号（Account Number）<br>－证书号（Certification Number）<br>－驾照（License Number）<br>－车证（Vehicle Number）<br>－社会保险号（Social Security Number）<br>－病历号（Medical Record Number）<br>－健康医疗保险号（Health Plan Beneficiary Number）<br>－面部信息（Facial Photograph）<br>－电话号码（Telephone Number）<br>－网络地址（URLs）<br>－网络IP地址（IP Address）<br>－生物身份识别（Biometric Identification）<br>－其他独立识别码</p>\n<p>了解了这个部分，我们就可以明白为什么HIPAA的一个非常有意思的规定，PHI的使用和暴露可以在两种情况下被使用。</p>\n<ul>\n<li>被授权情况 </li>\n<li>祛标示信息情况</li>\n</ul>\n<p>也就是说在祛标示信息（De-identity）后，其实所有的数据就可以被Freely的使用不收到太多限制。</p>\n<p>3）安全－电子信息形式下的健康信息保护（ePHI）<br><img src=\"/2015/12/17/hippa-part1/security.jpg\" title=\"安全\"><br>虽然安全被分成了三个级别，但是作为一个技术同学，我还是重点看了看技术安全的五个方面：</p>\n<ul>\n<li>访问控制 （ACCESS Control）</li>\n<li>审计控制（Audit Controls（R））</li>\n<li>数据完整性（Integrity）</li>\n<li>认证（Person and Entity Authentication）</li>\n<li>传输安全（Transmission Security）</li>\n</ul>\n<p>隐私和安全的区别：<br>隐私关注个人受保护健康信息（PHI）的控制和使用权力。任何形式的该信息不得在未经授权的情况下进行暴露。<br>安全特指电子受保护健康信息（ePHI）的防护标准。防止该信息在未经授权情况下被暴露、破坏或丢失<br>隐私信息部分依赖于安全来确保被保护。</p>\n<p>参考文献：<br>［1］Portability (social security) <a href=\"https://en.wikipedia.org/wiki/Portability_(social_security\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Portability_(social_security</a>)<br>［2］HIPPA <a href=\"http://baike.baidu.com/link?url=FeWuYF_ezrLt1Cg1v94jfq6zb2GfplUzM4FtURhfhMRYmKn3Hmu8erdzR9m7SG0iD9jDZcTM0ARPDOQxY-1QPK\" target=\"_blank\" rel=\"noopener\">http://baike.baidu.com/link?url=FeWuYF_ezrLt1Cg1v94jfq6zb2GfplUzM4FtURhfhMRYmKn3Hmu8erdzR9m7SG0iD9jDZcTM0ARPDOQxY-1QPK</a><br>［3］Pre Existing Conditions - Understanding Exclusions and Creditable Coverage  <a href=\"http://healthinsurance.about.com/od/healthinsurancebasics/a/preexisting_conditions_overview.html\" target=\"_blank\" rel=\"noopener\">http://healthinsurance.about.com/od/healthinsurancebasics/a/preexisting_conditions_overview.html</a><br>［4］美国卫生及公共服务部 <a href=\"https://zh.wikipedia.org/wiki/%E7%BE%8E%E5%9B%BD%E5%8D%AB%E7%94%9F%E5%8F%8A%E5%85%AC%E5%85%B1%E6%9C%8D%E5%8A%A1%E9%83%A8\" target=\"_blank\" rel=\"noopener\">https://zh.wikipedia.org/wiki/%E7%BE%8E%E5%9B%BD%E5%8D%AB%E7%94%9F%E5%8F%8A%E5%85%AC%E5%85%B1%E6%9C%8D%E5%8A%A1%E9%83%A8</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近因为工作的关系，必须系统的学习一下HIPAA，把学习结果给大家分享一下，如下：</p>\n<h3 id=\"第一部分：什么是HIPAA\"><a href=\"#第一部分：什么是HIPAA\" class=\"headerlink\" title=\"第一部分：什么是HIPAA\"></a>第一部分：什么是HIPAA</h3><p>特别词汇说明（terms）：<br>Portability：这里特别说明，根据wiki在社会保险（social security）方面的解释，所谓携带型是保存（preserve）、维护（maintain）和交换（transfer）三个词的合意总称，因为很没有中文对应意思，所以特此解释，后面还是翻译成可携带［1］。</p>\n<p>HIPAA，全称Health Insurance Portability and Accountability Act. 是美国关于健康保险的携带和责任的法案。HIPAA的提出，旨在改革健康医疗产业，降低费用，简化管理过程和负担，增强隐私保护和个人信息安全保护。可以说，HIPAA是美国健康卫生领域的基础大法，也是整个相关后续政策的基石。自从1996年HIPAA被正式提出以后，法案经历了5次比较大的更新，分别对医疗卫生领域在保险和医疗管理方面的工作，进行了一系列逐渐细致化和现代化的制约。</p>\n<img src=\"/2015/12/17/hippa-part1/evolution1996.jpg\" title=\"1996年第一次正式形成法案\">\n<img src=\"/2015/12/17/hippa-part1/evolution1997.jpg\" title=\"1997年引入保险改革\">\n<img src=\"/2015/12/17/hippa-part1/evolution2009.jpg\" title=\"2009年引入ARRA／HITECH法案\">\n<p>在美国2009年更新了HIPAA，引入了美国复苏与再投资法案ARRA（American Recovery and Reinvestment Act），第一次开始引入HITECH（全称Health Information Technology for Economic and Clinic Health Act）。该法案旨在拓展对EHR（Electronic Health Records） 的使用，并开始对ePHI进行了大量升级，详细解释了HIPAA对隐私和安全的保护，同时增加了更多强制实施的内容和相对应的不符合情况下的惩罚措施。该法案与2010年2月强制生效。</p>\n<img src=\"/2015/12/17/hippa-part1/evolution2013.jpg\" title=\"2013年由美国卫生与公共关系部下发Omnibus政策\">\n<p>之后，在2013年1月，由美国卫生与公共服务部（Department of Health and Human）最终发布了Omnibus Rule，将过去的HITECH和GINA（Generic Information Nondiscrimination Act of 2008）进行了融合，并增加了更多详细的指导细节。虽然作为行政命令，Omnibus Rule并不是法案，但是它更加细则。政令与2013年9月正式施行。所有non-compilance将会收到非常严重的惩罚。</p>\n<p>HIPAA会影响到两类组群<br>A. 健康覆盖实体－医疗健康提供者（如医生、护士）、医疗健康规划者（如保险公司、雇主、政府）、医疗健康数据清洗者<br>B. 医疗健康业务工作组织－所有为覆盖实体提供服务的第三方合作伙伴（如律师、会计、医疗公司、咨询师等）</p>\n<h3 id=\"第二部分，关于的组成\"><a href=\"#第二部分，关于的组成\" class=\"headerlink\" title=\"第二部分，关于的组成\"></a>第二部分，关于的组成</h3><p>下图描述了HIPAA的主要结果组成。</p>\n<img src=\"/2015/12/17/hippa-part1/hippa.jpg\" title=\"HIPAA的结构\">\n<p>HIPAA与所有涉及健康相关的医疗、保险和个人都有直接关系。法案主要分为五个主题，合并为两大部分组成，分别是简化管理（administrative simplification）［2］和保险改革（insurance reform），这两个部分也分别对应了HIPAA的两个关键要求，前者是责任（Accountability），后者对应可携带（Portability）。通常意义上大家所说的HIPAA，都是指的管理责任这部分。如图所示主要包括三个组成子模块儿：<br>1）事务、代码集和身份识别</p>\n<ul>\n<li><p>关于事务，2002年启用的HIPAA关于事务（Transaction）的规定要求，完整的事物要求包含以下内容：<br>提交和收费处信息<br>是否任何参与“Health Plan”计划<br>准入性、覆盖范围、救济金等<br>需要给健康计划提供者的付费内容<br>健康保险计划之外的附加费用<br>状态请求和相应<br>相关证书以及认证</p>\n</li>\n<li><p>关于代码集（Code Sets）部分，正如HIPAA的初衷，规定代码集大大方便了系统的交互。标准的代码，帮助系统设计之间进行有效的沟通。该部分包含5大集合，主要是为了描述疾病（desease），创伤（injury），症状（Symptoms）和操作行为（Actions）分别是：<br>ICD-9-CM 国际疾病分类（International Classification of Disease）<br>CPT （Physician Current Procedural Terminology）<br>HCPCS HCFA操作代码系统（HCFA Procedural Coding System）包括设备、诊断、治疗、管理疾病等<br>CDT 现代牙医术语（Current Dental Terminology）<br>NDC 国家药品代码（National Drug Codes）</p>\n</li>\n<li><p>关于独立标示（Unique Identifiers）用来表明患者、供应商、赔款人和雇主<br>ProviderID：给所有健康服务供应商的10位数字ID号码<br>EmployerID：所有为健康医疗提供资金的雇主，由9位数字表示（EIN）<br>PayerID：所有为健康医疗服务付钱的组织，由9位数字表示（EIN）<br>PatientID：所有接受服务的患者，目前尚在国会（congress）讨论中，主要是Social Security Number已经存在，是不是还需要一个独立的ID存在争议。</p>\n</li>\n</ul>\n<p>2）隐私－任何形式下的健康信息保护（PHI）<br>美国卫生及公共服务部（Department of Health and Human Services）［4］在2000年颁布，2002年修改了个人医疗信息隐私政策，明确了医疗实体禁止使用和披露个人受保护健康信息（Protected Health Information）。要求受保护的内容包括电子病历，纸质病历和口头沟通。同时HIPAA作为联邦政府法案，是各州遵循但不限于的基础。各州有权利要求医疗供应商等遵循更加严格的州法案。</p>\n<p>PHI包含所有个人可辨识健康信息，包含任何形式过去、现在和可能未来的健康情况。（纸质电子病历、医疗交费记录，包括抄写副本）, 其中有一个非常重要的部分叫做个人可辨识信息（Individually Identifiable Information）<br>－姓名（Name）<br>－住址（Address）<br>－电子邮件（E-mail）<br>－日期（Dates）<br>－账户号（Account Number）<br>－证书号（Certification Number）<br>－驾照（License Number）<br>－车证（Vehicle Number）<br>－社会保险号（Social Security Number）<br>－病历号（Medical Record Number）<br>－健康医疗保险号（Health Plan Beneficiary Number）<br>－面部信息（Facial Photograph）<br>－电话号码（Telephone Number）<br>－网络地址（URLs）<br>－网络IP地址（IP Address）<br>－生物身份识别（Biometric Identification）<br>－其他独立识别码</p>\n<p>了解了这个部分，我们就可以明白为什么HIPAA的一个非常有意思的规定，PHI的使用和暴露可以在两种情况下被使用。</p>\n<ul>\n<li>被授权情况 </li>\n<li>祛标示信息情况</li>\n</ul>\n<p>也就是说在祛标示信息（De-identity）后，其实所有的数据就可以被Freely的使用不收到太多限制。</p>\n<p>3）安全－电子信息形式下的健康信息保护（ePHI）<br><img src=\"/2015/12/17/hippa-part1/security.jpg\" title=\"安全\"><br>虽然安全被分成了三个级别，但是作为一个技术同学，我还是重点看了看技术安全的五个方面：</p>\n<ul>\n<li>访问控制 （ACCESS Control）</li>\n<li>审计控制（Audit Controls（R））</li>\n<li>数据完整性（Integrity）</li>\n<li>认证（Person and Entity Authentication）</li>\n<li>传输安全（Transmission Security）</li>\n</ul>\n<p>隐私和安全的区别：<br>隐私关注个人受保护健康信息（PHI）的控制和使用权力。任何形式的该信息不得在未经授权的情况下进行暴露。<br>安全特指电子受保护健康信息（ePHI）的防护标准。防止该信息在未经授权情况下被暴露、破坏或丢失<br>隐私信息部分依赖于安全来确保被保护。</p>\n<p>参考文献：<br>［1］Portability (social security) <a href=\"https://en.wikipedia.org/wiki/Portability_(social_security\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Portability_(social_security</a>)<br>［2］HIPPA <a href=\"http://baike.baidu.com/link?url=FeWuYF_ezrLt1Cg1v94jfq6zb2GfplUzM4FtURhfhMRYmKn3Hmu8erdzR9m7SG0iD9jDZcTM0ARPDOQxY-1QPK\" target=\"_blank\" rel=\"noopener\">http://baike.baidu.com/link?url=FeWuYF_ezrLt1Cg1v94jfq6zb2GfplUzM4FtURhfhMRYmKn3Hmu8erdzR9m7SG0iD9jDZcTM0ARPDOQxY-1QPK</a><br>［3］Pre Existing Conditions - Understanding Exclusions and Creditable Coverage  <a href=\"http://healthinsurance.about.com/od/healthinsurancebasics/a/preexisting_conditions_overview.html\" target=\"_blank\" rel=\"noopener\">http://healthinsurance.about.com/od/healthinsurancebasics/a/preexisting_conditions_overview.html</a><br>［4］美国卫生及公共服务部 <a href=\"https://zh.wikipedia.org/wiki/%E7%BE%8E%E5%9B%BD%E5%8D%AB%E7%94%9F%E5%8F%8A%E5%85%AC%E5%85%B1%E6%9C%8D%E5%8A%A1%E9%83%A8\" target=\"_blank\" rel=\"noopener\">https://zh.wikipedia.org/wiki/%E7%BE%8E%E5%9B%BD%E5%8D%AB%E7%94%9F%E5%8F%8A%E5%85%AC%E5%85%B1%E6%9C%8D%E5%8A%A1%E9%83%A8</a></p>\n"},{"title":"一个把html生成文件转成ppt的Demo","date":"2018-04-16T10:10:24.000Z","_content":"\n最近需要搞点小东西，需要转换数据成为ppt，老样子github上找了一下。还是比较简单的\nhttps://github.com/gitbrent/PptxGenJS\n\n内容详实，联系了一下，值得参考。把内容copy出来，变成html即可\n[Demo Link](test.h)\t\n\n注意，这里好像有个问题是关于调用pptxgen.bundle.js，这个貌似要翻墙才可以，后续有空把js文件补进去","source":"_posts/html2ppt.md","raw":"title: 一个把html生成文件转成ppt的Demo\ndate: 2018-04-16 18:10:24\ncategories:\n- Technology\ntags:\n- tech\n---\n\n最近需要搞点小东西，需要转换数据成为ppt，老样子github上找了一下。还是比较简单的\nhttps://github.com/gitbrent/PptxGenJS\n\n内容详实，联系了一下，值得参考。把内容copy出来，变成html即可\n[Demo Link](test.h)\t\n\n注意，这里好像有个问题是关于调用pptxgen.bundle.js，这个貌似要翻墙才可以，后续有空把js文件补进去","slug":"html2ppt","published":1,"updated":"2018-04-23T14:46:19.588Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoy000wrlfyp3uya4nq","content":"<p>最近需要搞点小东西，需要转换数据成为ppt，老样子github上找了一下。还是比较简单的<br><a href=\"https://github.com/gitbrent/PptxGenJS\" target=\"_blank\" rel=\"noopener\">https://github.com/gitbrent/PptxGenJS</a></p>\n<p>内容详实，联系了一下，值得参考。把内容copy出来，变成html即可<br><a href=\"test.h\">Demo Link</a>    </p>\n<p>注意，这里好像有个问题是关于调用pptxgen.bundle.js，这个貌似要翻墙才可以，后续有空把js文件补进去</p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近需要搞点小东西，需要转换数据成为ppt，老样子github上找了一下。还是比较简单的<br><a href=\"https://github.com/gitbrent/PptxGenJS\" target=\"_blank\" rel=\"noopener\">https://github.com/gitbrent/PptxGenJS</a></p>\n<p>内容详实，联系了一下，值得参考。把内容copy出来，变成html即可<br><a href=\"test.h\">Demo Link</a>    </p>\n<p>注意，这里好像有个问题是关于调用pptxgen.bundle.js，这个貌似要翻墙才可以，后续有空把js文件补进去</p>\n"},{"title":"python_db_basic","date":"2015-01-16T10:58:15.000Z","_content":"有些东西就是放下来记录一下，关于python的。这两天病了，忽然觉得这个事情总是对着有一种箭在弦上不得不发的感觉，所以说什么也要把这几块儿技术上的东西完成。搞完这个，应该可以踏踏实实的看看公司下一步的内部管理要如何做了，现在开始有点疲态的感觉，可能是因为平台接二连三的事故。不过这个我倒是不担心，毕竟一直在运行，一点点小聪明还是可以搞定的，只是，眼看，需要一个系统行的计划了，新的一个半年计划应该要搞一下出来了。\n\n这个title叫python db basic，其实名字不好，主要是记录一下几个主要的命令，如何继续完成我的这个小玩具。\n\n1）Python DB，超简单的东西，Djangle框架和RoR框架一样，基本上对于简单应用就是记住几行命令\n\n在Models里面建立好class\n\n在命令行里面运行，并生成migration脚本\n\n```bash\npython manage.py makemigrations\n```\n\n在命令行里面运行，并生成数据库的内容\n\n```bash\npython manage.py syncdb\n```\n\n在view里面用class直接产生新的obj，然后obj.save()就可以了\n\n2）一个微信的小东西\nngrok用来做本地调试，却是好用截个图，剩下的自己查吧\n\n\n\n王哲，CTOin杏树林\n紧张的时候，静下来写一写条目，来看看哪些在做，哪些要做，哪些没做\n","source":"_posts/python-db-basic.md","raw":"title: python_db_basic\ndate: 2015-01-16 18:58:15\ncategories:\n- Technology\ntags:\n- tech\n- python\n---\n有些东西就是放下来记录一下，关于python的。这两天病了，忽然觉得这个事情总是对着有一种箭在弦上不得不发的感觉，所以说什么也要把这几块儿技术上的东西完成。搞完这个，应该可以踏踏实实的看看公司下一步的内部管理要如何做了，现在开始有点疲态的感觉，可能是因为平台接二连三的事故。不过这个我倒是不担心，毕竟一直在运行，一点点小聪明还是可以搞定的，只是，眼看，需要一个系统行的计划了，新的一个半年计划应该要搞一下出来了。\n\n这个title叫python db basic，其实名字不好，主要是记录一下几个主要的命令，如何继续完成我的这个小玩具。\n\n1）Python DB，超简单的东西，Djangle框架和RoR框架一样，基本上对于简单应用就是记住几行命令\n\n在Models里面建立好class\n\n在命令行里面运行，并生成migration脚本\n\n```bash\npython manage.py makemigrations\n```\n\n在命令行里面运行，并生成数据库的内容\n\n```bash\npython manage.py syncdb\n```\n\n在view里面用class直接产生新的obj，然后obj.save()就可以了\n\n2）一个微信的小东西\nngrok用来做本地调试，却是好用截个图，剩下的自己查吧\n\n\n\n王哲，CTOin杏树林\n紧张的时候，静下来写一写条目，来看看哪些在做，哪些要做，哪些没做\n","slug":"python-db-basic","published":1,"updated":"2018-04-16T09:59:37.082Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoy000xrlfyna7ldnm5","content":"<p>有些东西就是放下来记录一下，关于python的。这两天病了，忽然觉得这个事情总是对着有一种箭在弦上不得不发的感觉，所以说什么也要把这几块儿技术上的东西完成。搞完这个，应该可以踏踏实实的看看公司下一步的内部管理要如何做了，现在开始有点疲态的感觉，可能是因为平台接二连三的事故。不过这个我倒是不担心，毕竟一直在运行，一点点小聪明还是可以搞定的，只是，眼看，需要一个系统行的计划了，新的一个半年计划应该要搞一下出来了。</p>\n<p>这个title叫python db basic，其实名字不好，主要是记录一下几个主要的命令，如何继续完成我的这个小玩具。</p>\n<p>1）Python DB，超简单的东西，Djangle框架和RoR框架一样，基本上对于简单应用就是记住几行命令</p>\n<p>在Models里面建立好class</p>\n<p>在命令行里面运行，并生成migration脚本</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python manage.py makemigrations</span><br></pre></td></tr></table></figure>\n<p>在命令行里面运行，并生成数据库的内容</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python manage.py syncdb</span><br></pre></td></tr></table></figure>\n<p>在view里面用class直接产生新的obj，然后obj.save()就可以了</p>\n<p>2）一个微信的小东西<br>ngrok用来做本地调试，却是好用截个图，剩下的自己查吧</p>\n<p>王哲，CTOin杏树林<br>紧张的时候，静下来写一写条目，来看看哪些在做，哪些要做，哪些没做</p>\n","site":{"data":{}},"excerpt":"","more":"<p>有些东西就是放下来记录一下，关于python的。这两天病了，忽然觉得这个事情总是对着有一种箭在弦上不得不发的感觉，所以说什么也要把这几块儿技术上的东西完成。搞完这个，应该可以踏踏实实的看看公司下一步的内部管理要如何做了，现在开始有点疲态的感觉，可能是因为平台接二连三的事故。不过这个我倒是不担心，毕竟一直在运行，一点点小聪明还是可以搞定的，只是，眼看，需要一个系统行的计划了，新的一个半年计划应该要搞一下出来了。</p>\n<p>这个title叫python db basic，其实名字不好，主要是记录一下几个主要的命令，如何继续完成我的这个小玩具。</p>\n<p>1）Python DB，超简单的东西，Djangle框架和RoR框架一样，基本上对于简单应用就是记住几行命令</p>\n<p>在Models里面建立好class</p>\n<p>在命令行里面运行，并生成migration脚本</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python manage.py makemigrations</span><br></pre></td></tr></table></figure>\n<p>在命令行里面运行，并生成数据库的内容</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python manage.py syncdb</span><br></pre></td></tr></table></figure>\n<p>在view里面用class直接产生新的obj，然后obj.save()就可以了</p>\n<p>2）一个微信的小东西<br>ngrok用来做本地调试，却是好用截个图，剩下的自己查吧</p>\n<p>王哲，CTOin杏树林<br>紧张的时候，静下来写一写条目，来看看哪些在做，哪些要做，哪些没做</p>\n"},{"title":"python_misc","date":"2014-12-22T10:29:22.000Z","_content":"研究python，发现最大的问题就是中英文的转换，实在是太郁闷了，研究了好久。最主要的还是出在python的encode和decode上面\nhttp://www.jb51.net/article/17560.htm\n这篇文章不错，基本上把需要的内容都解释的很清楚了。\n\n在别人代码的基础上做了些修改，现在已经基本可以用了。\n\n这个是该出来的加入的内容\npython ./jira -s http://bug.xingshulin.com -u ＊＊＊＊ -p ＊＊＊＊ cat JCTP-9\npython ./jira -s http://bug.xingshulin.com -u ＊＊＊＊ -p ＊＊＊＊ comment JCTP-9 \"let do something fantastic\"\npython ./jira -s http://bug.xingshulin.com -u ＊＊＊＊ -p ＊＊＊＊ create -s \"this is summary2\" -d \"nothing special\" -p JCTP -t \"运维事故\" -a \"wangzhe\" -f \"customfield_10200:20/12/14\"\n\n有个挺tricky的地方是要把jira里面的时间传输格式进行调整，默认的是d/MMM/yy，但是因为python对中文的支持问题，所以需要被调整成dd/mm/yy\n\n\n{'priority': 3, 'description': None, 'customFieldValues': [[{'values': [['2014-12-23']], 'customfieldId': 'customfield_10200'}]], 'summary': 'Test issue using all available defaults', 'project': 'JCTP', 'assignee': 'wangzhe', 'type': 7}\n{'priority': 3, 'description': None, 'customFieldValues': [[{'values': [['2014-12-22']], 'customfieldId': 'customfield_10200'}]], 'summary': 'this is summary', 'project': 'JCTP', 'assignee': 'wangzhe', 'type': 6}","source":"_posts/python-misc.md","raw":"title: python_misc\ndate: 2014-12-22 18:29:22\ncategories:\n- Technology\ntags:\n- tech\n- python\n---\n研究python，发现最大的问题就是中英文的转换，实在是太郁闷了，研究了好久。最主要的还是出在python的encode和decode上面\nhttp://www.jb51.net/article/17560.htm\n这篇文章不错，基本上把需要的内容都解释的很清楚了。\n\n在别人代码的基础上做了些修改，现在已经基本可以用了。\n\n这个是该出来的加入的内容\npython ./jira -s http://bug.xingshulin.com -u ＊＊＊＊ -p ＊＊＊＊ cat JCTP-9\npython ./jira -s http://bug.xingshulin.com -u ＊＊＊＊ -p ＊＊＊＊ comment JCTP-9 \"let do something fantastic\"\npython ./jira -s http://bug.xingshulin.com -u ＊＊＊＊ -p ＊＊＊＊ create -s \"this is summary2\" -d \"nothing special\" -p JCTP -t \"运维事故\" -a \"wangzhe\" -f \"customfield_10200:20/12/14\"\n\n有个挺tricky的地方是要把jira里面的时间传输格式进行调整，默认的是d/MMM/yy，但是因为python对中文的支持问题，所以需要被调整成dd/mm/yy\n\n\n{'priority': 3, 'description': None, 'customFieldValues': [[{'values': [['2014-12-23']], 'customfieldId': 'customfield_10200'}]], 'summary': 'Test issue using all available defaults', 'project': 'JCTP', 'assignee': 'wangzhe', 'type': 7}\n{'priority': 3, 'description': None, 'customFieldValues': [[{'values': [['2014-12-22']], 'customfieldId': 'customfield_10200'}]], 'summary': 'this is summary', 'project': 'JCTP', 'assignee': 'wangzhe', 'type': 6}","slug":"python-misc","published":1,"updated":"2018-04-16T09:59:37.039Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoz000yrlfy9x3vfls3","content":"<p>研究python，发现最大的问题就是中英文的转换，实在是太郁闷了，研究了好久。最主要的还是出在python的encode和decode上面<br><a href=\"http://www.jb51.net/article/17560.htm\" target=\"_blank\" rel=\"noopener\">http://www.jb51.net/article/17560.htm</a><br>这篇文章不错，基本上把需要的内容都解释的很清楚了。</p>\n<p>在别人代码的基础上做了些修改，现在已经基本可以用了。</p>\n<p>这个是该出来的加入的内容<br>python ./jira -s <a href=\"http://bug.xingshulin.com\" target=\"_blank\" rel=\"noopener\">http://bug.xingshulin.com</a> -u ＊＊＊＊ -p ＊＊＊＊ cat JCTP-9<br>python ./jira -s <a href=\"http://bug.xingshulin.com\" target=\"_blank\" rel=\"noopener\">http://bug.xingshulin.com</a> -u ＊＊＊＊ -p ＊＊＊＊ comment JCTP-9 “let do something fantastic”<br>python ./jira -s <a href=\"http://bug.xingshulin.com\" target=\"_blank\" rel=\"noopener\">http://bug.xingshulin.com</a> -u ＊＊＊＊ -p ＊＊＊＊ create -s “this is summary2” -d “nothing special” -p JCTP -t “运维事故” -a “wangzhe” -f “customfield_10200:20/12/14”</p>\n<p>有个挺tricky的地方是要把jira里面的时间传输格式进行调整，默认的是d/MMM/yy，但是因为python对中文的支持问题，所以需要被调整成dd/mm/yy</p>\n<p>{‘priority’: 3, ‘description’: None, ‘customFieldValues’: [[{‘values’: [[‘2014-12-23’]], ‘customfieldId’: ‘customfield_10200’}]], ‘summary’: ‘Test issue using all available defaults’, ‘project’: ‘JCTP’, ‘assignee’: ‘wangzhe’, ‘type’: 7}<br>{‘priority’: 3, ‘description’: None, ‘customFieldValues’: [[{‘values’: [[‘2014-12-22’]], ‘customfieldId’: ‘customfield_10200’}]], ‘summary’: ‘this is summary’, ‘project’: ‘JCTP’, ‘assignee’: ‘wangzhe’, ‘type’: 6}</p>\n","site":{"data":{}},"excerpt":"","more":"<p>研究python，发现最大的问题就是中英文的转换，实在是太郁闷了，研究了好久。最主要的还是出在python的encode和decode上面<br><a href=\"http://www.jb51.net/article/17560.htm\" target=\"_blank\" rel=\"noopener\">http://www.jb51.net/article/17560.htm</a><br>这篇文章不错，基本上把需要的内容都解释的很清楚了。</p>\n<p>在别人代码的基础上做了些修改，现在已经基本可以用了。</p>\n<p>这个是该出来的加入的内容<br>python ./jira -s <a href=\"http://bug.xingshulin.com\" target=\"_blank\" rel=\"noopener\">http://bug.xingshulin.com</a> -u ＊＊＊＊ -p ＊＊＊＊ cat JCTP-9<br>python ./jira -s <a href=\"http://bug.xingshulin.com\" target=\"_blank\" rel=\"noopener\">http://bug.xingshulin.com</a> -u ＊＊＊＊ -p ＊＊＊＊ comment JCTP-9 “let do something fantastic”<br>python ./jira -s <a href=\"http://bug.xingshulin.com\" target=\"_blank\" rel=\"noopener\">http://bug.xingshulin.com</a> -u ＊＊＊＊ -p ＊＊＊＊ create -s “this is summary2” -d “nothing special” -p JCTP -t “运维事故” -a “wangzhe” -f “customfield_10200:20/12/14”</p>\n<p>有个挺tricky的地方是要把jira里面的时间传输格式进行调整，默认的是d/MMM/yy，但是因为python对中文的支持问题，所以需要被调整成dd/mm/yy</p>\n<p>{‘priority’: 3, ‘description’: None, ‘customFieldValues’: [[{‘values’: [[‘2014-12-23’]], ‘customfieldId’: ‘customfield_10200’}]], ‘summary’: ‘Test issue using all available defaults’, ‘project’: ‘JCTP’, ‘assignee’: ‘wangzhe’, ‘type’: 7}<br>{‘priority’: 3, ‘description’: None, ‘customFieldValues’: [[{‘values’: [[‘2014-12-22’]], ‘customfieldId’: ‘customfield_10200’}]], ‘summary’: ‘this is summary’, ‘project’: ‘JCTP’, ‘assignee’: ‘wangzhe’, ‘type’: 6}</p>\n"},{"title":"升级Python3几个小总结","date":"2016-05-22T01:30:31.000Z","_content":"\n最近，主要是想集中把这次的改造写完，早日上线。这次主要干了两件事情。第一是升级Python3，这里面顺带写一点学习总结。算是个misc文章吧。下一篇主要是想聊聊没有后台的系统到底是如何搭建以及为什么要做这个实验。\n\nOK，先说这次Python3升级吧。想了很久了，受制于各种阻力，总是担心升级会出问题。但是Python3既然已经嚷嚷了这么多年。而且还是有越来越多的系统在往Python3上迁移，所以还是用用看。至少知道坑在哪里，以后会遇到哪些问题。\n\n总结一下本次修改较多的几个部分：\n\n+\tPrint语法：Python3最有名的一个表征就是print的括号问题。在Python2里面是没有的，在3里加上了（）这个语法。所以导致从2向3升级的过程中print，成了修改最多的语法之一。\n\n+\tPip2（Python2）和Pip3（Python3），一般来讲，如果之前装的是2的，升级到3时候Python和Pip两个命令依然表示的是2，如果要是使用3.x的话，需要加入pip3或者python3，来作为命令开头。为了简单期间，或者用env来定义。还有一种就是在bash_profile里面增加python的alias\n\n\t```\n\talias python='python3'\n\talias pip='pip3'\n\t```\n\t\n+\tMySQL-python包的使用。这个很讨厌，因为目前这个包还不支持Python3。不过也没什么着急的，换一个呗。PyMySQL这个是Python官方对于MySQL的支持包。对于基本的增删改查操作，用法基本上是一样的。只不过要特别注意的是连接时要加入“charset=\"utf8\"”来实现对中文的支持。这个在MySQLdb里面好像是默认，至少我当时没有设置。\n\n\t```\n    db = pymysql.connect(host=host, port=int(port), user=username, passwd=password, db=database, charset=\"utf8\")\n\t```\n\n+\t关于unicode，这里说道大头了哈。因为绝大多数人对于Python2中文支持都是一个噩梦。文字，被在<class 'str'>-<class 'unicode'>-<class 'byte'>之间转换。但在Python3里面，很重要的一点是对中文做了很好的支持。消除了unicode这一层环节。所以现在就可以比较明确的说“中文.encode('utf-8')”就是解码环节，无需再顾忌其他。\n\t{% asset_img unicode.png \"Python3里的中文问题\" %}\n\n+\t也正是因为这个unicode的转变，其实造成了Python2里面很多原始代码都要进行一系列的和字符集相关的变化。比如一个buildin的函数open，在以前2的时代，是不需要对字符集进行指定的（话说，其实也是因为那个时代就没有什么字符集）。现在要特别说明打开字符集类型才可以进行有效的读写操作。\n\n\t```\n\topen(filename, 'w', newline='', encoding='utf-8')\n\t```\n\t\n+\t另外很赞的一点是，Unittest对Python的支持非常好。我自己实际用了unittest2和mock两个库。基本上都没有出现什么相关的修改。非常赞！所以说测试还是要加哦，很不错滴。\n\n这次升级，确实花了些时间，处理问题。但确实得到了不少有益的实践。总结下来是两点吧\n\n1.\t单元测试很重要。整个过程中，升级后的第一件事情就是运行单元测试，其中大量的错误，但是不可怕，一个一个的改就好了。因为我的程序设计web、邮件、数据库等多方通讯，没有单元测试，效率会大打折扣，而且还有可能遇到因为网络问题而导致的联调失败\n2.\tPython2到3的升级并不可怕，主要问题恰恰是来自于unicode这块儿的改变。只要记好了字符集utf-8这件事情。你会发现大量问题，都和这个有直接关系。","source":"_posts/python3upgrade.md","raw":"title: 升级Python3几个小总结\ndate: 2016-05-22 09:30:31\ncategories:\n- Technology\ntags:\n- tech\n- python\n---\n\n最近，主要是想集中把这次的改造写完，早日上线。这次主要干了两件事情。第一是升级Python3，这里面顺带写一点学习总结。算是个misc文章吧。下一篇主要是想聊聊没有后台的系统到底是如何搭建以及为什么要做这个实验。\n\nOK，先说这次Python3升级吧。想了很久了，受制于各种阻力，总是担心升级会出问题。但是Python3既然已经嚷嚷了这么多年。而且还是有越来越多的系统在往Python3上迁移，所以还是用用看。至少知道坑在哪里，以后会遇到哪些问题。\n\n总结一下本次修改较多的几个部分：\n\n+\tPrint语法：Python3最有名的一个表征就是print的括号问题。在Python2里面是没有的，在3里加上了（）这个语法。所以导致从2向3升级的过程中print，成了修改最多的语法之一。\n\n+\tPip2（Python2）和Pip3（Python3），一般来讲，如果之前装的是2的，升级到3时候Python和Pip两个命令依然表示的是2，如果要是使用3.x的话，需要加入pip3或者python3，来作为命令开头。为了简单期间，或者用env来定义。还有一种就是在bash_profile里面增加python的alias\n\n\t```\n\talias python='python3'\n\talias pip='pip3'\n\t```\n\t\n+\tMySQL-python包的使用。这个很讨厌，因为目前这个包还不支持Python3。不过也没什么着急的，换一个呗。PyMySQL这个是Python官方对于MySQL的支持包。对于基本的增删改查操作，用法基本上是一样的。只不过要特别注意的是连接时要加入“charset=\"utf8\"”来实现对中文的支持。这个在MySQLdb里面好像是默认，至少我当时没有设置。\n\n\t```\n    db = pymysql.connect(host=host, port=int(port), user=username, passwd=password, db=database, charset=\"utf8\")\n\t```\n\n+\t关于unicode，这里说道大头了哈。因为绝大多数人对于Python2中文支持都是一个噩梦。文字，被在<class 'str'>-<class 'unicode'>-<class 'byte'>之间转换。但在Python3里面，很重要的一点是对中文做了很好的支持。消除了unicode这一层环节。所以现在就可以比较明确的说“中文.encode('utf-8')”就是解码环节，无需再顾忌其他。\n\t{% asset_img unicode.png \"Python3里的中文问题\" %}\n\n+\t也正是因为这个unicode的转变，其实造成了Python2里面很多原始代码都要进行一系列的和字符集相关的变化。比如一个buildin的函数open，在以前2的时代，是不需要对字符集进行指定的（话说，其实也是因为那个时代就没有什么字符集）。现在要特别说明打开字符集类型才可以进行有效的读写操作。\n\n\t```\n\topen(filename, 'w', newline='', encoding='utf-8')\n\t```\n\t\n+\t另外很赞的一点是，Unittest对Python的支持非常好。我自己实际用了unittest2和mock两个库。基本上都没有出现什么相关的修改。非常赞！所以说测试还是要加哦，很不错滴。\n\n这次升级，确实花了些时间，处理问题。但确实得到了不少有益的实践。总结下来是两点吧\n\n1.\t单元测试很重要。整个过程中，升级后的第一件事情就是运行单元测试，其中大量的错误，但是不可怕，一个一个的改就好了。因为我的程序设计web、邮件、数据库等多方通讯，没有单元测试，效率会大打折扣，而且还有可能遇到因为网络问题而导致的联调失败\n2.\tPython2到3的升级并不可怕，主要问题恰恰是来自于unicode这块儿的改变。只要记好了字符集utf-8这件事情。你会发现大量问题，都和这个有直接关系。","slug":"python3upgrade","published":1,"updated":"2018-04-16T09:59:37.033Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcoz000zrlfyds494d56","content":"<p>最近，主要是想集中把这次的改造写完，早日上线。这次主要干了两件事情。第一是升级Python3，这里面顺带写一点学习总结。算是个misc文章吧。下一篇主要是想聊聊没有后台的系统到底是如何搭建以及为什么要做这个实验。</p>\n<p>OK，先说这次Python3升级吧。想了很久了，受制于各种阻力，总是担心升级会出问题。但是Python3既然已经嚷嚷了这么多年。而且还是有越来越多的系统在往Python3上迁移，所以还是用用看。至少知道坑在哪里，以后会遇到哪些问题。</p>\n<p>总结一下本次修改较多的几个部分：</p>\n<ul>\n<li><p>Print语法：Python3最有名的一个表征就是print的括号问题。在Python2里面是没有的，在3里加上了（）这个语法。所以导致从2向3升级的过程中print，成了修改最多的语法之一。</p>\n</li>\n<li><p>Pip2（Python2）和Pip3（Python3），一般来讲，如果之前装的是2的，升级到3时候Python和Pip两个命令依然表示的是2，如果要是使用3.x的话，需要加入pip3或者python3，来作为命令开头。为了简单期间，或者用env来定义。还有一种就是在bash_profile里面增加python的alias</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias python=&apos;python3&apos;</span><br><span class=\"line\">alias pip=&apos;pip3&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>MySQL-python包的使用。这个很讨厌，因为目前这个包还不支持Python3。不过也没什么着急的，换一个呗。PyMySQL这个是Python官方对于MySQL的支持包。对于基本的增删改查操作，用法基本上是一样的。只不过要特别注意的是连接时要加入“charset=”utf8””来实现对中文的支持。这个在MySQLdb里面好像是默认，至少我当时没有设置。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db = pymysql.connect(host=host, port=int(port), user=username, passwd=password, db=database, charset=&quot;utf8&quot;)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>关于unicode，这里说道大头了哈。因为绝大多数人对于Python2中文支持都是一个噩梦。文字，被在<class 'str'=\"\">-<class 'unicode'=\"\">-<class 'byte'=\"\">之间转换。但在Python3里面，很重要的一点是对中文做了很好的支持。消除了unicode这一层环节。所以现在就可以比较明确的说“中文.encode(‘utf-8’)”就是解码环节，无需再顾忌其他。</class></class></class></p>\n<img src=\"/2016/05/22/python3upgrade/unicode.png\" title=\"Python3里的中文问题\">\n</li>\n<li><p>也正是因为这个unicode的转变，其实造成了Python2里面很多原始代码都要进行一系列的和字符集相关的变化。比如一个buildin的函数open，在以前2的时代，是不需要对字符集进行指定的（话说，其实也是因为那个时代就没有什么字符集）。现在要特别说明打开字符集类型才可以进行有效的读写操作。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">open(filename, &apos;w&apos;, newline=&apos;&apos;, encoding=&apos;utf-8&apos;)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li>另外很赞的一点是，Unittest对Python的支持非常好。我自己实际用了unittest2和mock两个库。基本上都没有出现什么相关的修改。非常赞！所以说测试还是要加哦，很不错滴。</li>\n</ul>\n<p>这次升级，确实花了些时间，处理问题。但确实得到了不少有益的实践。总结下来是两点吧</p>\n<ol>\n<li>单元测试很重要。整个过程中，升级后的第一件事情就是运行单元测试，其中大量的错误，但是不可怕，一个一个的改就好了。因为我的程序设计web、邮件、数据库等多方通讯，没有单元测试，效率会大打折扣，而且还有可能遇到因为网络问题而导致的联调失败</li>\n<li>Python2到3的升级并不可怕，主要问题恰恰是来自于unicode这块儿的改变。只要记好了字符集utf-8这件事情。你会发现大量问题，都和这个有直接关系。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>最近，主要是想集中把这次的改造写完，早日上线。这次主要干了两件事情。第一是升级Python3，这里面顺带写一点学习总结。算是个misc文章吧。下一篇主要是想聊聊没有后台的系统到底是如何搭建以及为什么要做这个实验。</p>\n<p>OK，先说这次Python3升级吧。想了很久了，受制于各种阻力，总是担心升级会出问题。但是Python3既然已经嚷嚷了这么多年。而且还是有越来越多的系统在往Python3上迁移，所以还是用用看。至少知道坑在哪里，以后会遇到哪些问题。</p>\n<p>总结一下本次修改较多的几个部分：</p>\n<ul>\n<li><p>Print语法：Python3最有名的一个表征就是print的括号问题。在Python2里面是没有的，在3里加上了（）这个语法。所以导致从2向3升级的过程中print，成了修改最多的语法之一。</p>\n</li>\n<li><p>Pip2（Python2）和Pip3（Python3），一般来讲，如果之前装的是2的，升级到3时候Python和Pip两个命令依然表示的是2，如果要是使用3.x的话，需要加入pip3或者python3，来作为命令开头。为了简单期间，或者用env来定义。还有一种就是在bash_profile里面增加python的alias</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias python=&apos;python3&apos;</span><br><span class=\"line\">alias pip=&apos;pip3&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>MySQL-python包的使用。这个很讨厌，因为目前这个包还不支持Python3。不过也没什么着急的，换一个呗。PyMySQL这个是Python官方对于MySQL的支持包。对于基本的增删改查操作，用法基本上是一样的。只不过要特别注意的是连接时要加入“charset=”utf8””来实现对中文的支持。这个在MySQLdb里面好像是默认，至少我当时没有设置。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db = pymysql.connect(host=host, port=int(port), user=username, passwd=password, db=database, charset=&quot;utf8&quot;)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>关于unicode，这里说道大头了哈。因为绝大多数人对于Python2中文支持都是一个噩梦。文字，被在<class 'str'=\"\">-<class 'unicode'=\"\">-<class 'byte'=\"\">之间转换。但在Python3里面，很重要的一点是对中文做了很好的支持。消除了unicode这一层环节。所以现在就可以比较明确的说“中文.encode(‘utf-8’)”就是解码环节，无需再顾忌其他。</class></class></class></p>\n<img src=\"/2016/05/22/python3upgrade/unicode.png\" title=\"Python3里的中文问题\">\n</li>\n<li><p>也正是因为这个unicode的转变，其实造成了Python2里面很多原始代码都要进行一系列的和字符集相关的变化。比如一个buildin的函数open，在以前2的时代，是不需要对字符集进行指定的（话说，其实也是因为那个时代就没有什么字符集）。现在要特别说明打开字符集类型才可以进行有效的读写操作。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">open(filename, &apos;w&apos;, newline=&apos;&apos;, encoding=&apos;utf-8&apos;)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li>另外很赞的一点是，Unittest对Python的支持非常好。我自己实际用了unittest2和mock两个库。基本上都没有出现什么相关的修改。非常赞！所以说测试还是要加哦，很不错滴。</li>\n</ul>\n<p>这次升级，确实花了些时间，处理问题。但确实得到了不少有益的实践。总结下来是两点吧</p>\n<ol>\n<li>单元测试很重要。整个过程中，升级后的第一件事情就是运行单元测试，其中大量的错误，但是不可怕，一个一个的改就好了。因为我的程序设计web、邮件、数据库等多方通讯，没有单元测试，效率会大打折扣，而且还有可能遇到因为网络问题而导致的联调失败</li>\n<li>Python2到3的升级并不可怕，主要问题恰恰是来自于unicode这块儿的改变。只要记好了字符集utf-8这件事情。你会发现大量问题，都和这个有直接关系。</li>\n</ol>\n"},{"title":"聊聊技术驱动互联网+医疗三个争论","date":"2016-06-14T06:18:04.000Z","_content":"\n前段时间和一个医生朋友吃饭，聊到他最近几个月开始接触病历夹，觉得超级好用。我问他哪里好用，他给我讲述了他，做为多点执医的工作，要经常到处跑，使用OCR和语音录入快速整理病历，有效跟踪病人的经历。这里有个背景交代一下，作为一个技术工匠，我自己不是一个很擅长推销的人。所以身边好朋友即使知道我在做什么，也知道我的性格，如果他们自己没有发现优势，是不会推荐他们用的。这次和我这个朋友吃饭，他夸的我自己都有点不好意思。但今天回想起来，心里却是觉得美滋滋的，一种功夫不负有心人的感觉。做为工匠，我理解最重要的是对“完美”的追求。在病历夹过去的2年时间里，身为技术的负责人，在语音识别和图片识别方面投入了巨大的精力。从寻找合作商，到研习友商和国内、国际上的方法，再到如今，一步一步在公司一群兄弟的“打骂”中，一点点优化系统。从最开始大量的人工识别，全病历识别到如今，经历2.5次大规模的系统更迭，几十个迭代上线，杏树林的识别系统已经初步实现了自动化分类分拣、中转流程自动化、以病历信息安全为基础的碎片化处理、信息的预OCR处理＋分角色人工录入校验的一体化系统。这一系列的工作，不是一蹴而就的，也不是靠纸面能想出来的。它是一种工程技术和识别科学技术相结合的体系化工程。它是一种从实践到优化再到实践到优化持续不断的过程。可以说，强大的健康医疗类数据处理能力，在成立5年后的今天，已经成立杏树林站在大数据风口上的重要核心力量。因为这样的能力，我们那个月处理几千万字符的医疗健康数据。因为这样的能力，我们积累了中国可能少有的动态标准化医疗数据库，希望有朝一日成为国内最大的标准化数据库。因为这样的能力，我们为数据分析和研究打下了坚实的结构化数据基础，并且在持续的算法和人工清洗中，让数据称为进行精准化医疗基石。这里必须多说一句，感谢陪我走过这艰苦日子兄弟们，一起加班加点工程师、把我主持的一个个系统说的体无完肤数据分析师、争执争吵不停的算法工程师们。\n\n{% asset_img ocr.png \"健康医疗数据处理\" %}\n\n说了些自夸的话，主要想顺着这个话题聊聊技术驱动医疗这个事情。互联网的发展，让越来越多的人们认识到技术在现代社会发展中起到的重要作用，工程师文化在成为了越来越多企业津津乐道的话题。那么针对医疗，是否真的能再次摇动起信息技术驱动的大旗让医疗进入新阶段吗？\n\n+\t互联网+医疗的门槛－医疗信息化vs信息化医疗\n\t见过不少的HIS系统（Hospital Informatioin System)，也曾经以前作为咨询师参与过朋友HIS的系统分析工作。就我所看到的，大量的医疗信息化系统，都是基本上完全照搬的现有医疗的实际工作方法或者工作模式。更多的人把计算机信息化当作了服务于原始工作的一种手段和工具。所以，曾经有一段时间，不光是医疗领域，在很多领域都流行一个词，叫做信息化或者数字化。而HIS也是在这段时期应运而生。但HIS本身，过分强调将已有医疗进行数字化，强调还原所谓的“医疗习惯”，因此造就了一系列其实很严重的数据结构。\n\t\n\t{% asset_img report.jpg \"奇葩的骨髓检查表\" %}\n\t\n\t如上图，数据因为不符合关系型数据的存储，造成了大量医学统计上的困难。这种怪异的表格设计，从人类认知理解上并没有太大问题，但是作用到机器上就会出席比较大的麻烦。为什么会造成这种现象呢？在笔者看来，就是所谓医疗信息化在作祟。信息技术应该是一门重要的学科，它的重要作用是帮助人们重新认识这个世界。用机器的方式对世界进行组织和计算，这里面虽然会产生冗余，但是因为机器的大规模自动化应用，将是的大量数据的集体计算能力得到非常大的提升。\n\t\n\t所以笔者提出了信息化医疗的理念，这也是为什么说信息技术驱动医疗。原因在于，信息的信息化技术更加强调改造一种旧有的方法和工作模式，通过大量机器的参与，大幅度的提升旧有模式的工作效率。因此，在提到移动医疗的门槛时，笔者经常会说，这里的最重要门槛是使用新时代的信息技术方法，对医疗工作过程，方法进行重新梳理和重新定义，帮助医疗系统寻找方法进行优化。而不是遵从现有医疗方法，找一个电子化的道路，这样是不可能解决问题的。\n\n+\t互联网+医疗的创新－业务创新vs技术创新vs技术化创新\n\t以Google为首，创立了新的数据分类检索的搜索算法。以Facebook为首，创立了新的人与人的连接算法。以Apple为首，创立了新的人与机器的交互技术。这几件事情，是笔者认为具有跨时代意义的技术发展，他们可以说是真正的技术创新。当然正在到来的无人机远程图像传输技术、AR/VR技术等，也可能正在成为具有划时代意义的技术提升。\n\t\n\t但是有不少人说，不能所有公司都围绕着技术本身来进行创造性工作。是的，各行各业也都需要许多的业务创新。但是如今，在互联网的时代，任何的创新，或者说基础创新工作，还是有章可循的。这个章，就是对技术改变传统方法的总结和实践。用这套新的方法，来指导创新。因此笔者强调技术化创新，并不是认为互联网时代的引领者一定要做什么前无古人的新技术，也不是传统模式里面寻找新的业务增长点，而是从技术发展中，寻找使用革命性的技术，对领域进行创新改造。在这个过程中，实现业务价值，引导领域效能提升。因此，笔者认为，用现代化技术寻找医疗领域的增长点，是创新的关键。\n\n+\t互联网+医疗的明天－大数据vs数据分析\n\t笔者最近出席一些论坛，也看了不少文章，发现有个词现在被反复提及“大数据”。这里笔者Quote一句最近特别喜欢的话。\n\t\n\t>Big Data is like teenage sex: \n\t>Everyone talks about it, nobody really knows how to do it, \n\t>everyone thinks everyone else is doing it, so everyone claims \n\t\n\t为什么这么说？笔者还不敢说自己对“大数据”有多么深的造诣，但是就已有的技术理解而言，“大数据”不等于“大量数据”这是两码事。大数据是一种根据大量数据的统计结果进行分析的方法。举个例子来说，百度用大数据对地图两点之间距离进行统计分析，预测百度到家的送餐时间。这其中百度大数据使用了很好的大样本情况下的统计学方法。这套方法，其实在医学领域早就已经有所使用，但是这里面有一个重要的问题。现代医学有一个重要的概念，叫做循证医学，其核心思想是医疗决策（即病人的处理，治疗指南和医疗政策的制定等）应在现有的最好的临床研究依据基础上作出，同时也重视结合个人的临床经验。\n\t\n\t这个临床依据怎么来，就成了现代医学的最重要课题。于是现代医学研究，包括药理分析，开发出了一大套排除干扰，单一化的统计方法来完成数据的分析工作。这里很重要的一个词，叫排除干扰。而恰恰是这个词，和大数据的模糊化统计理念有出入。所以，有不少人说大数据是医疗的明天，这点笔者很难认同。原因恰恰是技术驱动互联网+医疗。因为如果是驱动，就要看看被驱动本体到底是什么。如果如笔者所说，大数据是一套统计方法论的话，那么除非医疗统计方法论出现了革新，否则很难说大数据是医疗明天这样的事情。\n\t\n\t那么明天到底是什么，在笔者看来要技术驱动，就要驱动信息化的医疗自身工作，那就是研究数据分析的过程，这一块儿虽说很难，但却是可以、也值得解决。从这个角度而言，也是笔者所引导杏树林努力实现的目标。\n\t\n\n总结下来，回到文章一开头写的。最近的信息技术互联网，可能由于某滴、某信的快速发展变得有点浮躁，好像今天做了投资，明天就能有几千万的用户，后天就能继续融资几亿，周而复始，大家就觉得技术驱动了。笔者眼里的技术，是一个持续话提升和解决的事情。人工智能这个词从上个世界70年代提出，经历了20年的起伏，10年的低潮，如今又慢慢随着技术的提升重新进入了“代表未来”的词语。如果一定要问，技术的发展模式，在笔者看来，是踏踏实实的，做好一个又一个优化，把现在需要10个人的事情，自动化起来，变成2个人。这就是技术的发展模式，来不得半点“大跃进”，来不得半点虚假。而是踏踏实实，一步一个脚印的去优化，去改造，不盲目遵从，也不妄自菲薄。这就是技术驱动互联网医疗的核心。只要有了这个核心，大部分争论其实都是伪命题而已。做好事，让医疗真的提升，才能让我们的兄弟姐妹父母子女更多人看到病，看好病。","source":"_posts/tech-driven.md","raw":"title: 聊聊技术驱动互联网+医疗三个争论\ndate: 2016-06-14 14:18:04\ncategories:\n- Business\ntags:\n- tech\n- med\n---\n\n前段时间和一个医生朋友吃饭，聊到他最近几个月开始接触病历夹，觉得超级好用。我问他哪里好用，他给我讲述了他，做为多点执医的工作，要经常到处跑，使用OCR和语音录入快速整理病历，有效跟踪病人的经历。这里有个背景交代一下，作为一个技术工匠，我自己不是一个很擅长推销的人。所以身边好朋友即使知道我在做什么，也知道我的性格，如果他们自己没有发现优势，是不会推荐他们用的。这次和我这个朋友吃饭，他夸的我自己都有点不好意思。但今天回想起来，心里却是觉得美滋滋的，一种功夫不负有心人的感觉。做为工匠，我理解最重要的是对“完美”的追求。在病历夹过去的2年时间里，身为技术的负责人，在语音识别和图片识别方面投入了巨大的精力。从寻找合作商，到研习友商和国内、国际上的方法，再到如今，一步一步在公司一群兄弟的“打骂”中，一点点优化系统。从最开始大量的人工识别，全病历识别到如今，经历2.5次大规模的系统更迭，几十个迭代上线，杏树林的识别系统已经初步实现了自动化分类分拣、中转流程自动化、以病历信息安全为基础的碎片化处理、信息的预OCR处理＋分角色人工录入校验的一体化系统。这一系列的工作，不是一蹴而就的，也不是靠纸面能想出来的。它是一种工程技术和识别科学技术相结合的体系化工程。它是一种从实践到优化再到实践到优化持续不断的过程。可以说，强大的健康医疗类数据处理能力，在成立5年后的今天，已经成立杏树林站在大数据风口上的重要核心力量。因为这样的能力，我们那个月处理几千万字符的医疗健康数据。因为这样的能力，我们积累了中国可能少有的动态标准化医疗数据库，希望有朝一日成为国内最大的标准化数据库。因为这样的能力，我们为数据分析和研究打下了坚实的结构化数据基础，并且在持续的算法和人工清洗中，让数据称为进行精准化医疗基石。这里必须多说一句，感谢陪我走过这艰苦日子兄弟们，一起加班加点工程师、把我主持的一个个系统说的体无完肤数据分析师、争执争吵不停的算法工程师们。\n\n{% asset_img ocr.png \"健康医疗数据处理\" %}\n\n说了些自夸的话，主要想顺着这个话题聊聊技术驱动医疗这个事情。互联网的发展，让越来越多的人们认识到技术在现代社会发展中起到的重要作用，工程师文化在成为了越来越多企业津津乐道的话题。那么针对医疗，是否真的能再次摇动起信息技术驱动的大旗让医疗进入新阶段吗？\n\n+\t互联网+医疗的门槛－医疗信息化vs信息化医疗\n\t见过不少的HIS系统（Hospital Informatioin System)，也曾经以前作为咨询师参与过朋友HIS的系统分析工作。就我所看到的，大量的医疗信息化系统，都是基本上完全照搬的现有医疗的实际工作方法或者工作模式。更多的人把计算机信息化当作了服务于原始工作的一种手段和工具。所以，曾经有一段时间，不光是医疗领域，在很多领域都流行一个词，叫做信息化或者数字化。而HIS也是在这段时期应运而生。但HIS本身，过分强调将已有医疗进行数字化，强调还原所谓的“医疗习惯”，因此造就了一系列其实很严重的数据结构。\n\t\n\t{% asset_img report.jpg \"奇葩的骨髓检查表\" %}\n\t\n\t如上图，数据因为不符合关系型数据的存储，造成了大量医学统计上的困难。这种怪异的表格设计，从人类认知理解上并没有太大问题，但是作用到机器上就会出席比较大的麻烦。为什么会造成这种现象呢？在笔者看来，就是所谓医疗信息化在作祟。信息技术应该是一门重要的学科，它的重要作用是帮助人们重新认识这个世界。用机器的方式对世界进行组织和计算，这里面虽然会产生冗余，但是因为机器的大规模自动化应用，将是的大量数据的集体计算能力得到非常大的提升。\n\t\n\t所以笔者提出了信息化医疗的理念，这也是为什么说信息技术驱动医疗。原因在于，信息的信息化技术更加强调改造一种旧有的方法和工作模式，通过大量机器的参与，大幅度的提升旧有模式的工作效率。因此，在提到移动医疗的门槛时，笔者经常会说，这里的最重要门槛是使用新时代的信息技术方法，对医疗工作过程，方法进行重新梳理和重新定义，帮助医疗系统寻找方法进行优化。而不是遵从现有医疗方法，找一个电子化的道路，这样是不可能解决问题的。\n\n+\t互联网+医疗的创新－业务创新vs技术创新vs技术化创新\n\t以Google为首，创立了新的数据分类检索的搜索算法。以Facebook为首，创立了新的人与人的连接算法。以Apple为首，创立了新的人与机器的交互技术。这几件事情，是笔者认为具有跨时代意义的技术发展，他们可以说是真正的技术创新。当然正在到来的无人机远程图像传输技术、AR/VR技术等，也可能正在成为具有划时代意义的技术提升。\n\t\n\t但是有不少人说，不能所有公司都围绕着技术本身来进行创造性工作。是的，各行各业也都需要许多的业务创新。但是如今，在互联网的时代，任何的创新，或者说基础创新工作，还是有章可循的。这个章，就是对技术改变传统方法的总结和实践。用这套新的方法，来指导创新。因此笔者强调技术化创新，并不是认为互联网时代的引领者一定要做什么前无古人的新技术，也不是传统模式里面寻找新的业务增长点，而是从技术发展中，寻找使用革命性的技术，对领域进行创新改造。在这个过程中，实现业务价值，引导领域效能提升。因此，笔者认为，用现代化技术寻找医疗领域的增长点，是创新的关键。\n\n+\t互联网+医疗的明天－大数据vs数据分析\n\t笔者最近出席一些论坛，也看了不少文章，发现有个词现在被反复提及“大数据”。这里笔者Quote一句最近特别喜欢的话。\n\t\n\t>Big Data is like teenage sex: \n\t>Everyone talks about it, nobody really knows how to do it, \n\t>everyone thinks everyone else is doing it, so everyone claims \n\t\n\t为什么这么说？笔者还不敢说自己对“大数据”有多么深的造诣，但是就已有的技术理解而言，“大数据”不等于“大量数据”这是两码事。大数据是一种根据大量数据的统计结果进行分析的方法。举个例子来说，百度用大数据对地图两点之间距离进行统计分析，预测百度到家的送餐时间。这其中百度大数据使用了很好的大样本情况下的统计学方法。这套方法，其实在医学领域早就已经有所使用，但是这里面有一个重要的问题。现代医学有一个重要的概念，叫做循证医学，其核心思想是医疗决策（即病人的处理，治疗指南和医疗政策的制定等）应在现有的最好的临床研究依据基础上作出，同时也重视结合个人的临床经验。\n\t\n\t这个临床依据怎么来，就成了现代医学的最重要课题。于是现代医学研究，包括药理分析，开发出了一大套排除干扰，单一化的统计方法来完成数据的分析工作。这里很重要的一个词，叫排除干扰。而恰恰是这个词，和大数据的模糊化统计理念有出入。所以，有不少人说大数据是医疗的明天，这点笔者很难认同。原因恰恰是技术驱动互联网+医疗。因为如果是驱动，就要看看被驱动本体到底是什么。如果如笔者所说，大数据是一套统计方法论的话，那么除非医疗统计方法论出现了革新，否则很难说大数据是医疗明天这样的事情。\n\t\n\t那么明天到底是什么，在笔者看来要技术驱动，就要驱动信息化的医疗自身工作，那就是研究数据分析的过程，这一块儿虽说很难，但却是可以、也值得解决。从这个角度而言，也是笔者所引导杏树林努力实现的目标。\n\t\n\n总结下来，回到文章一开头写的。最近的信息技术互联网，可能由于某滴、某信的快速发展变得有点浮躁，好像今天做了投资，明天就能有几千万的用户，后天就能继续融资几亿，周而复始，大家就觉得技术驱动了。笔者眼里的技术，是一个持续话提升和解决的事情。人工智能这个词从上个世界70年代提出，经历了20年的起伏，10年的低潮，如今又慢慢随着技术的提升重新进入了“代表未来”的词语。如果一定要问，技术的发展模式，在笔者看来，是踏踏实实的，做好一个又一个优化，把现在需要10个人的事情，自动化起来，变成2个人。这就是技术的发展模式，来不得半点“大跃进”，来不得半点虚假。而是踏踏实实，一步一个脚印的去优化，去改造，不盲目遵从，也不妄自菲薄。这就是技术驱动互联网医疗的核心。只要有了这个核心，大部分争论其实都是伪命题而已。做好事，让医疗真的提升，才能让我们的兄弟姐妹父母子女更多人看到病，看好病。","slug":"tech-driven","published":1,"updated":"2018-04-16T09:59:37.072Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcp00010rlfytmtobdp2","content":"<p>前段时间和一个医生朋友吃饭，聊到他最近几个月开始接触病历夹，觉得超级好用。我问他哪里好用，他给我讲述了他，做为多点执医的工作，要经常到处跑，使用OCR和语音录入快速整理病历，有效跟踪病人的经历。这里有个背景交代一下，作为一个技术工匠，我自己不是一个很擅长推销的人。所以身边好朋友即使知道我在做什么，也知道我的性格，如果他们自己没有发现优势，是不会推荐他们用的。这次和我这个朋友吃饭，他夸的我自己都有点不好意思。但今天回想起来，心里却是觉得美滋滋的，一种功夫不负有心人的感觉。做为工匠，我理解最重要的是对“完美”的追求。在病历夹过去的2年时间里，身为技术的负责人，在语音识别和图片识别方面投入了巨大的精力。从寻找合作商，到研习友商和国内、国际上的方法，再到如今，一步一步在公司一群兄弟的“打骂”中，一点点优化系统。从最开始大量的人工识别，全病历识别到如今，经历2.5次大规模的系统更迭，几十个迭代上线，杏树林的识别系统已经初步实现了自动化分类分拣、中转流程自动化、以病历信息安全为基础的碎片化处理、信息的预OCR处理＋分角色人工录入校验的一体化系统。这一系列的工作，不是一蹴而就的，也不是靠纸面能想出来的。它是一种工程技术和识别科学技术相结合的体系化工程。它是一种从实践到优化再到实践到优化持续不断的过程。可以说，强大的健康医疗类数据处理能力，在成立5年后的今天，已经成立杏树林站在大数据风口上的重要核心力量。因为这样的能力，我们那个月处理几千万字符的医疗健康数据。因为这样的能力，我们积累了中国可能少有的动态标准化医疗数据库，希望有朝一日成为国内最大的标准化数据库。因为这样的能力，我们为数据分析和研究打下了坚实的结构化数据基础，并且在持续的算法和人工清洗中，让数据称为进行精准化医疗基石。这里必须多说一句，感谢陪我走过这艰苦日子兄弟们，一起加班加点工程师、把我主持的一个个系统说的体无完肤数据分析师、争执争吵不停的算法工程师们。</p>\n<img src=\"/2016/06/14/tech-driven/ocr.png\" title=\"健康医疗数据处理\">\n<p>说了些自夸的话，主要想顺着这个话题聊聊技术驱动医疗这个事情。互联网的发展，让越来越多的人们认识到技术在现代社会发展中起到的重要作用，工程师文化在成为了越来越多企业津津乐道的话题。那么针对医疗，是否真的能再次摇动起信息技术驱动的大旗让医疗进入新阶段吗？</p>\n<ul>\n<li><p>互联网+医疗的门槛－医疗信息化vs信息化医疗<br>见过不少的HIS系统（Hospital Informatioin System)，也曾经以前作为咨询师参与过朋友HIS的系统分析工作。就我所看到的，大量的医疗信息化系统，都是基本上完全照搬的现有医疗的实际工作方法或者工作模式。更多的人把计算机信息化当作了服务于原始工作的一种手段和工具。所以，曾经有一段时间，不光是医疗领域，在很多领域都流行一个词，叫做信息化或者数字化。而HIS也是在这段时期应运而生。但HIS本身，过分强调将已有医疗进行数字化，强调还原所谓的“医疗习惯”，因此造就了一系列其实很严重的数据结构。</p>\n<img src=\"/2016/06/14/tech-driven/report.jpg\" title=\"奇葩的骨髓检查表\">\n<p>如上图，数据因为不符合关系型数据的存储，造成了大量医学统计上的困难。这种怪异的表格设计，从人类认知理解上并没有太大问题，但是作用到机器上就会出席比较大的麻烦。为什么会造成这种现象呢？在笔者看来，就是所谓医疗信息化在作祟。信息技术应该是一门重要的学科，它的重要作用是帮助人们重新认识这个世界。用机器的方式对世界进行组织和计算，这里面虽然会产生冗余，但是因为机器的大规模自动化应用，将是的大量数据的集体计算能力得到非常大的提升。</p>\n<p>所以笔者提出了信息化医疗的理念，这也是为什么说信息技术驱动医疗。原因在于，信息的信息化技术更加强调改造一种旧有的方法和工作模式，通过大量机器的参与，大幅度的提升旧有模式的工作效率。因此，在提到移动医疗的门槛时，笔者经常会说，这里的最重要门槛是使用新时代的信息技术方法，对医疗工作过程，方法进行重新梳理和重新定义，帮助医疗系统寻找方法进行优化。而不是遵从现有医疗方法，找一个电子化的道路，这样是不可能解决问题的。</p>\n</li>\n<li><p>互联网+医疗的创新－业务创新vs技术创新vs技术化创新<br>以Google为首，创立了新的数据分类检索的搜索算法。以Facebook为首，创立了新的人与人的连接算法。以Apple为首，创立了新的人与机器的交互技术。这几件事情，是笔者认为具有跨时代意义的技术发展，他们可以说是真正的技术创新。当然正在到来的无人机远程图像传输技术、AR/VR技术等，也可能正在成为具有划时代意义的技术提升。</p>\n<p>但是有不少人说，不能所有公司都围绕着技术本身来进行创造性工作。是的，各行各业也都需要许多的业务创新。但是如今，在互联网的时代，任何的创新，或者说基础创新工作，还是有章可循的。这个章，就是对技术改变传统方法的总结和实践。用这套新的方法，来指导创新。因此笔者强调技术化创新，并不是认为互联网时代的引领者一定要做什么前无古人的新技术，也不是传统模式里面寻找新的业务增长点，而是从技术发展中，寻找使用革命性的技术，对领域进行创新改造。在这个过程中，实现业务价值，引导领域效能提升。因此，笔者认为，用现代化技术寻找医疗领域的增长点，是创新的关键。</p>\n</li>\n<li><p>互联网+医疗的明天－大数据vs数据分析<br>笔者最近出席一些论坛，也看了不少文章，发现有个词现在被反复提及“大数据”。这里笔者Quote一句最近特别喜欢的话。</p>\n<blockquote>\n<p>Big Data is like teenage sex:<br>Everyone talks about it, nobody really knows how to do it,<br>everyone thinks everyone else is doing it, so everyone claims </p>\n</blockquote>\n<p>为什么这么说？笔者还不敢说自己对“大数据”有多么深的造诣，但是就已有的技术理解而言，“大数据”不等于“大量数据”这是两码事。大数据是一种根据大量数据的统计结果进行分析的方法。举个例子来说，百度用大数据对地图两点之间距离进行统计分析，预测百度到家的送餐时间。这其中百度大数据使用了很好的大样本情况下的统计学方法。这套方法，其实在医学领域早就已经有所使用，但是这里面有一个重要的问题。现代医学有一个重要的概念，叫做循证医学，其核心思想是医疗决策（即病人的处理，治疗指南和医疗政策的制定等）应在现有的最好的临床研究依据基础上作出，同时也重视结合个人的临床经验。</p>\n<p>这个临床依据怎么来，就成了现代医学的最重要课题。于是现代医学研究，包括药理分析，开发出了一大套排除干扰，单一化的统计方法来完成数据的分析工作。这里很重要的一个词，叫排除干扰。而恰恰是这个词，和大数据的模糊化统计理念有出入。所以，有不少人说大数据是医疗的明天，这点笔者很难认同。原因恰恰是技术驱动互联网+医疗。因为如果是驱动，就要看看被驱动本体到底是什么。如果如笔者所说，大数据是一套统计方法论的话，那么除非医疗统计方法论出现了革新，否则很难说大数据是医疗明天这样的事情。</p>\n<p>那么明天到底是什么，在笔者看来要技术驱动，就要驱动信息化的医疗自身工作，那就是研究数据分析的过程，这一块儿虽说很难，但却是可以、也值得解决。从这个角度而言，也是笔者所引导杏树林努力实现的目标。</p>\n</li>\n</ul>\n<p>总结下来，回到文章一开头写的。最近的信息技术互联网，可能由于某滴、某信的快速发展变得有点浮躁，好像今天做了投资，明天就能有几千万的用户，后天就能继续融资几亿，周而复始，大家就觉得技术驱动了。笔者眼里的技术，是一个持续话提升和解决的事情。人工智能这个词从上个世界70年代提出，经历了20年的起伏，10年的低潮，如今又慢慢随着技术的提升重新进入了“代表未来”的词语。如果一定要问，技术的发展模式，在笔者看来，是踏踏实实的，做好一个又一个优化，把现在需要10个人的事情，自动化起来，变成2个人。这就是技术的发展模式，来不得半点“大跃进”，来不得半点虚假。而是踏踏实实，一步一个脚印的去优化，去改造，不盲目遵从，也不妄自菲薄。这就是技术驱动互联网医疗的核心。只要有了这个核心，大部分争论其实都是伪命题而已。做好事，让医疗真的提升，才能让我们的兄弟姐妹父母子女更多人看到病，看好病。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>前段时间和一个医生朋友吃饭，聊到他最近几个月开始接触病历夹，觉得超级好用。我问他哪里好用，他给我讲述了他，做为多点执医的工作，要经常到处跑，使用OCR和语音录入快速整理病历，有效跟踪病人的经历。这里有个背景交代一下，作为一个技术工匠，我自己不是一个很擅长推销的人。所以身边好朋友即使知道我在做什么，也知道我的性格，如果他们自己没有发现优势，是不会推荐他们用的。这次和我这个朋友吃饭，他夸的我自己都有点不好意思。但今天回想起来，心里却是觉得美滋滋的，一种功夫不负有心人的感觉。做为工匠，我理解最重要的是对“完美”的追求。在病历夹过去的2年时间里，身为技术的负责人，在语音识别和图片识别方面投入了巨大的精力。从寻找合作商，到研习友商和国内、国际上的方法，再到如今，一步一步在公司一群兄弟的“打骂”中，一点点优化系统。从最开始大量的人工识别，全病历识别到如今，经历2.5次大规模的系统更迭，几十个迭代上线，杏树林的识别系统已经初步实现了自动化分类分拣、中转流程自动化、以病历信息安全为基础的碎片化处理、信息的预OCR处理＋分角色人工录入校验的一体化系统。这一系列的工作，不是一蹴而就的，也不是靠纸面能想出来的。它是一种工程技术和识别科学技术相结合的体系化工程。它是一种从实践到优化再到实践到优化持续不断的过程。可以说，强大的健康医疗类数据处理能力，在成立5年后的今天，已经成立杏树林站在大数据风口上的重要核心力量。因为这样的能力，我们那个月处理几千万字符的医疗健康数据。因为这样的能力，我们积累了中国可能少有的动态标准化医疗数据库，希望有朝一日成为国内最大的标准化数据库。因为这样的能力，我们为数据分析和研究打下了坚实的结构化数据基础，并且在持续的算法和人工清洗中，让数据称为进行精准化医疗基石。这里必须多说一句，感谢陪我走过这艰苦日子兄弟们，一起加班加点工程师、把我主持的一个个系统说的体无完肤数据分析师、争执争吵不停的算法工程师们。</p>\n<img src=\"/2016/06/14/tech-driven/ocr.png\" title=\"健康医疗数据处理\">\n<p>说了些自夸的话，主要想顺着这个话题聊聊技术驱动医疗这个事情。互联网的发展，让越来越多的人们认识到技术在现代社会发展中起到的重要作用，工程师文化在成为了越来越多企业津津乐道的话题。那么针对医疗，是否真的能再次摇动起信息技术驱动的大旗让医疗进入新阶段吗？</p>\n<ul>\n<li><p>互联网+医疗的门槛－医疗信息化vs信息化医疗<br>见过不少的HIS系统（Hospital Informatioin System)，也曾经以前作为咨询师参与过朋友HIS的系统分析工作。就我所看到的，大量的医疗信息化系统，都是基本上完全照搬的现有医疗的实际工作方法或者工作模式。更多的人把计算机信息化当作了服务于原始工作的一种手段和工具。所以，曾经有一段时间，不光是医疗领域，在很多领域都流行一个词，叫做信息化或者数字化。而HIS也是在这段时期应运而生。但HIS本身，过分强调将已有医疗进行数字化，强调还原所谓的“医疗习惯”，因此造就了一系列其实很严重的数据结构。</p>\n<img src=\"/2016/06/14/tech-driven/report.jpg\" title=\"奇葩的骨髓检查表\">\n<p>如上图，数据因为不符合关系型数据的存储，造成了大量医学统计上的困难。这种怪异的表格设计，从人类认知理解上并没有太大问题，但是作用到机器上就会出席比较大的麻烦。为什么会造成这种现象呢？在笔者看来，就是所谓医疗信息化在作祟。信息技术应该是一门重要的学科，它的重要作用是帮助人们重新认识这个世界。用机器的方式对世界进行组织和计算，这里面虽然会产生冗余，但是因为机器的大规模自动化应用，将是的大量数据的集体计算能力得到非常大的提升。</p>\n<p>所以笔者提出了信息化医疗的理念，这也是为什么说信息技术驱动医疗。原因在于，信息的信息化技术更加强调改造一种旧有的方法和工作模式，通过大量机器的参与，大幅度的提升旧有模式的工作效率。因此，在提到移动医疗的门槛时，笔者经常会说，这里的最重要门槛是使用新时代的信息技术方法，对医疗工作过程，方法进行重新梳理和重新定义，帮助医疗系统寻找方法进行优化。而不是遵从现有医疗方法，找一个电子化的道路，这样是不可能解决问题的。</p>\n</li>\n<li><p>互联网+医疗的创新－业务创新vs技术创新vs技术化创新<br>以Google为首，创立了新的数据分类检索的搜索算法。以Facebook为首，创立了新的人与人的连接算法。以Apple为首，创立了新的人与机器的交互技术。这几件事情，是笔者认为具有跨时代意义的技术发展，他们可以说是真正的技术创新。当然正在到来的无人机远程图像传输技术、AR/VR技术等，也可能正在成为具有划时代意义的技术提升。</p>\n<p>但是有不少人说，不能所有公司都围绕着技术本身来进行创造性工作。是的，各行各业也都需要许多的业务创新。但是如今，在互联网的时代，任何的创新，或者说基础创新工作，还是有章可循的。这个章，就是对技术改变传统方法的总结和实践。用这套新的方法，来指导创新。因此笔者强调技术化创新，并不是认为互联网时代的引领者一定要做什么前无古人的新技术，也不是传统模式里面寻找新的业务增长点，而是从技术发展中，寻找使用革命性的技术，对领域进行创新改造。在这个过程中，实现业务价值，引导领域效能提升。因此，笔者认为，用现代化技术寻找医疗领域的增长点，是创新的关键。</p>\n</li>\n<li><p>互联网+医疗的明天－大数据vs数据分析<br>笔者最近出席一些论坛，也看了不少文章，发现有个词现在被反复提及“大数据”。这里笔者Quote一句最近特别喜欢的话。</p>\n<blockquote>\n<p>Big Data is like teenage sex:<br>Everyone talks about it, nobody really knows how to do it,<br>everyone thinks everyone else is doing it, so everyone claims </p>\n</blockquote>\n<p>为什么这么说？笔者还不敢说自己对“大数据”有多么深的造诣，但是就已有的技术理解而言，“大数据”不等于“大量数据”这是两码事。大数据是一种根据大量数据的统计结果进行分析的方法。举个例子来说，百度用大数据对地图两点之间距离进行统计分析，预测百度到家的送餐时间。这其中百度大数据使用了很好的大样本情况下的统计学方法。这套方法，其实在医学领域早就已经有所使用，但是这里面有一个重要的问题。现代医学有一个重要的概念，叫做循证医学，其核心思想是医疗决策（即病人的处理，治疗指南和医疗政策的制定等）应在现有的最好的临床研究依据基础上作出，同时也重视结合个人的临床经验。</p>\n<p>这个临床依据怎么来，就成了现代医学的最重要课题。于是现代医学研究，包括药理分析，开发出了一大套排除干扰，单一化的统计方法来完成数据的分析工作。这里很重要的一个词，叫排除干扰。而恰恰是这个词，和大数据的模糊化统计理念有出入。所以，有不少人说大数据是医疗的明天，这点笔者很难认同。原因恰恰是技术驱动互联网+医疗。因为如果是驱动，就要看看被驱动本体到底是什么。如果如笔者所说，大数据是一套统计方法论的话，那么除非医疗统计方法论出现了革新，否则很难说大数据是医疗明天这样的事情。</p>\n<p>那么明天到底是什么，在笔者看来要技术驱动，就要驱动信息化的医疗自身工作，那就是研究数据分析的过程，这一块儿虽说很难，但却是可以、也值得解决。从这个角度而言，也是笔者所引导杏树林努力实现的目标。</p>\n</li>\n</ul>\n<p>总结下来，回到文章一开头写的。最近的信息技术互联网，可能由于某滴、某信的快速发展变得有点浮躁，好像今天做了投资，明天就能有几千万的用户，后天就能继续融资几亿，周而复始，大家就觉得技术驱动了。笔者眼里的技术，是一个持续话提升和解决的事情。人工智能这个词从上个世界70年代提出，经历了20年的起伏，10年的低潮，如今又慢慢随着技术的提升重新进入了“代表未来”的词语。如果一定要问，技术的发展模式，在笔者看来，是踏踏实实的，做好一个又一个优化，把现在需要10个人的事情，自动化起来，变成2个人。这就是技术的发展模式，来不得半点“大跃进”，来不得半点虚假。而是踏踏实实，一步一个脚印的去优化，去改造，不盲目遵从，也不妄自菲薄。这就是技术驱动互联网医疗的核心。只要有了这个核心，大部分争论其实都是伪命题而已。做好事，让医疗真的提升，才能让我们的兄弟姐妹父母子女更多人看到病，看好病。</p>\n"},{"title":"today","date":"2014-11-27T10:15:04.000Z","_content":"#I understand we need github page, but why it is not work:(\n","source":"_posts/today.md","raw":"title: today\ndate: 2014-11-27 18:15:04\ncategories:\n- Technology\ntags:\n- tech\n---\n#I understand we need github page, but why it is not work:(\n","slug":"today","published":1,"updated":"2018-04-16T09:59:37.049Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcp10011rlfy0balahsp","content":"<h1 id=\"I-understand-we-need-github-page-but-why-it-is-not-work\"><a href=\"#I-understand-we-need-github-page-but-why-it-is-not-work\" class=\"headerlink\" title=\"I understand we need github page, but why it is not work:(\"></a>I understand we need github page, but why it is not work:(</h1>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"I-understand-we-need-github-page-but-why-it-is-not-work\"><a href=\"#I-understand-we-need-github-page-but-why-it-is-not-work\" class=\"headerlink\" title=\"I understand we need github page, but why it is not work:(\"></a>I understand we need github page, but why it is not work:(</h1>"},{"title":"wechat_content","date":"2015-01-05T09:34:30.000Z","_content":"\n今天读到一片文章《{% link 腾讯年终总结：微信用户一天到晚都在干啥 http://b2b.netsun.com/detail--6222395.html %}》，搞好最近在研究微信的一些东西，于是想到还是写出来给自己留个纪念。比较在意下面这张图，其中蕴含的背后含义，我觉得是如今微信生态圈的核心\n\n{% img http://mmbiz.qpic.cn/mmbiz/ricIZzYiaUjhMibcJiauwNl1vpkaK12fuQShIq3FyzmxmMtKM1Y4wzcV3BiaCcoGe1uF3BD8y5kNibTa8XD3J6TVMZhg/0 600px 381px 文章阅读饼图 %}\n\n总结一下，作为微信的运营大概需要这么几个点：\n－ 以目前微信的生态环境而言，无论是订阅号还是服务号都无法实现大量阅读和广泛传播的目的；\n－ 微信的主要传播途径是朋友圈的转发，和因此产生的马太效应；\n－ 微信的内容可以有这么几类，“有用”，“有感情”，“有参与感”。\n\n王哲，CTOin杏树林\n作为一个公司领导者，无论是运营，产品，技术，客服，推广什么都得学","source":"_posts/wechat-content.md","raw":"title: wechat_content\ndate: 2015-01-05 17:34:30\ncategories:\n- Business Strategy\ntags:\n- operation\n---\n\n今天读到一片文章《{% link 腾讯年终总结：微信用户一天到晚都在干啥 http://b2b.netsun.com/detail--6222395.html %}》，搞好最近在研究微信的一些东西，于是想到还是写出来给自己留个纪念。比较在意下面这张图，其中蕴含的背后含义，我觉得是如今微信生态圈的核心\n\n{% img http://mmbiz.qpic.cn/mmbiz/ricIZzYiaUjhMibcJiauwNl1vpkaK12fuQShIq3FyzmxmMtKM1Y4wzcV3BiaCcoGe1uF3BD8y5kNibTa8XD3J6TVMZhg/0 600px 381px 文章阅读饼图 %}\n\n总结一下，作为微信的运营大概需要这么几个点：\n－ 以目前微信的生态环境而言，无论是订阅号还是服务号都无法实现大量阅读和广泛传播的目的；\n－ 微信的主要传播途径是朋友圈的转发，和因此产生的马太效应；\n－ 微信的内容可以有这么几类，“有用”，“有感情”，“有参与感”。\n\n王哲，CTOin杏树林\n作为一个公司领导者，无论是运营，产品，技术，客服，推广什么都得学","slug":"wechat-content","published":1,"updated":"2018-04-16T09:59:37.074Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcp10012rlfy9o4kx11v","content":"<p>今天读到一片文章《<a href=\"http://b2b.netsun.com/detail--6222395.html\" target=\"_blank\" rel=\"noopener\">腾讯年终总结：微信用户一天到晚都在干啥</a>》，搞好最近在研究微信的一些东西，于是想到还是写出来给自己留个纪念。比较在意下面这张图，其中蕴含的背后含义，我觉得是如今微信生态圈的核心</p>\n<img src=\"http://mmbiz.qpic.cn/mmbiz/ricIZzYiaUjhMibcJiauwNl1vpkaK12fuQShIq3FyzmxmMtKM1Y4wzcV3BiaCcoGe1uF3BD8y5kNibTa8XD3J6TVMZhg/0\" title=\"600px 381px 文章阅读饼图\">\n<p>总结一下，作为微信的运营大概需要这么几个点：<br>－ 以目前微信的生态环境而言，无论是订阅号还是服务号都无法实现大量阅读和广泛传播的目的；<br>－ 微信的主要传播途径是朋友圈的转发，和因此产生的马太效应；<br>－ 微信的内容可以有这么几类，“有用”，“有感情”，“有参与感”。</p>\n<p>王哲，CTOin杏树林<br>作为一个公司领导者，无论是运营，产品，技术，客服，推广什么都得学</p>\n","site":{"data":{}},"excerpt":"","more":"<p>今天读到一片文章《<a href=\"http://b2b.netsun.com/detail--6222395.html\" target=\"_blank\" rel=\"noopener\">腾讯年终总结：微信用户一天到晚都在干啥</a>》，搞好最近在研究微信的一些东西，于是想到还是写出来给自己留个纪念。比较在意下面这张图，其中蕴含的背后含义，我觉得是如今微信生态圈的核心</p>\n<img src=\"http://mmbiz.qpic.cn/mmbiz/ricIZzYiaUjhMibcJiauwNl1vpkaK12fuQShIq3FyzmxmMtKM1Y4wzcV3BiaCcoGe1uF3BD8y5kNibTa8XD3J6TVMZhg/0\" title=\"600px 381px 文章阅读饼图\">\n<p>总结一下，作为微信的运营大概需要这么几个点：<br>－ 以目前微信的生态环境而言，无论是订阅号还是服务号都无法实现大量阅读和广泛传播的目的；<br>－ 微信的主要传播途径是朋友圈的转发，和因此产生的马太效应；<br>－ 微信的内容可以有这么几类，“有用”，“有感情”，“有参与感”。</p>\n<p>王哲，CTOin杏树林<br>作为一个公司领导者，无论是运营，产品，技术，客服，推广什么都得学</p>\n"},{"title":"为什么说技术颠覆传统医疗","date":"2015-05-19T12:26:40.000Z","_content":"http://server.it168.com/a2015/0519/1729/000001729986.shtml\n\n最近一直在做移动医疗创业，作为一个创新公司的技术负责人，一方面需要努力与团队研发更有价值的产品功能，另一方面还需要关心业务的发展。毕竟，没有业务，技术只是玩具。看了许多移动医疗方面的文章，大多从纯业务角度去划分医疗的阶段（如诊前、诊中和诊后），并以此出发去考虑移动互联网能做些什么，以及如何评价一个创业公司。\n\n这种方法固然很好，但还有另一个事实被忽略了，那就是互联网从它诞生的那一刻就不是适应，而是改变，通过让人变得更为高效或者更为舒适，去改变人们过去的旧有习惯。举个例子，大约2年前，每个人还习惯于招手打车，那个时候，“有事儿”，“出门”，“找出租车多的大街”，“招手打车”。现在的习惯是什么呢？“有事儿”，“找车”，“车到”，“出门”，“上车”。所以，只要新方法足够方便，习惯并不是一成不变的。归根结底，新的技术让一切改变了。\n\n回到互联网这个话题里，我们来看看最近几年的哪些移动技术改变了人们的生活习惯。 \n\n1）\t高效移动计算能力－让计算成本变低\n自从2007年的谷歌G1，应该说是移动计算的一个里程碑，安卓系统的推出，让人们发现原来手机不仅仅是一个为个体资源服务的小盒子，而是一个可以联系从本地到远程的计算资源。\n\n其实很多人认为移动的发展是因为使用了碎片时间，这个说法没有错，但是碎片时间以前没有么？当然不是。根本问题是，计算资源提升了。这里要澄清一点的，“计算”不只是机器的事情，也需要人不断的进行输入、调整和验证输出结果，才能完整的实现真正意义上的计算。以前计算只能放在办公室里，然后下班回家，现在你可以时时刻刻的通过移动设备对需要计算的资源进行输入，实际的计算有可能是手机端，也有可能通过手机上传到远程服务器端完成，但不管怎样，任何计算都有机会时时刻刻被发起，或者经过调整被再次发起。这就是移动计算带给人类的革新。\n\n2）\t移动多媒体通讯－让沟通成本变低\n\n说到沟通，其实归根结底是个沟通两端展示的问题。展示的效率，展示的形式，决定了沟通效果的好坏。 “文字”的发明让沟通可以实现远程而非必须面对面。电信的发明远程的传播效率更加便捷。而如今，通过移动设备的多媒体通信，让人们不再需要时间和空间的约束。与此同时，多种媒体的传输使得这种沟通变得更加容易理解。一张图片、一个视频可能胜过万语千言。也正是因为展示的快捷和易于理解，成本变低的结果，我们用同样的代价，可以获得更多的沟通，那么随之带来的，包括信息更加透明化，更加海量化也就水到渠成了。\n\n3）\t多种辅助设备－让收集成本变低\n上面提到谷歌，接下来可想而知少不了苹果，笔者看来这家公司的伟大，不仅仅在于他发明多少很牛很炫酷的东西，而是他降低了各种辅助设备的成本，并把他们集中在了一台移动设备上。无论是陀螺仪、加速计、还是血氧传感器的引入，都大大增加了人们在它上面的想象空间。因为从那以后，人们有机会利用更加低廉的成本，收集更加广泛的信息。\n\n正是因为上述所言的变化，使得人们有机会寻求新的改变，用这样的技术去创造新的生活习惯。如果我们意识到移动技术在过去八年时间里成功改变了人们的许多生活习惯；如果我们认为医疗群体也是一个有生活的人群，那么移动技术也一定会类似的，在这三个维度上改变传统医疗，使医生改变旧有习惯。更为高效或更舒服地从事工作。\n\n从公司医疗业务同事那里，笔者不断尝试了解尽可能多的信息，寻找可能带来的改变。医疗会第一个想到“看病”，看病是什么，要“收集信息”，“制定方案”，“随访结果”，“调整方案”。只要是医疗，一定涉及这几个话题，这是循证医学的根基，无论诊前，诊中还是诊后。这也是几乎绝大部分技术型工种的工作方法（比如开发“收集需求”，“架构设计”，“结果测试”，“重构”）。于是乎，我们这回倒着说，从最后一条开始：\n\n3）\t多种辅助设备－让收集成本变低\n\n现在不少人在这方面做努力，其中最典型的创新，就是大量硬件、软件的简易医疗设备的。过去医院、医疗单位、药厂研究机构等收集病历，简直是一项无比繁冗复杂的工作，如今只需要用收集拍摄一下，就会收集起来，自动识别成病历数据。\n\n硬件方面，一个皮肤科常用的探测仪，动辄几千、上万，还操作极其复杂。笔者自己从“华强北”花36块钱买了一个高倍放大镜，配搭上自己的手机摄像头，把照出来的效果问了几个医生，还真不错。\n\n再比如对帕金森的诊治和判断，一直是医疗界的难题，尤其是如何形容，让许多医生非常痛苦。后来开始有专注病例采集的应用，使用视频来进行数据采集，大大方便了对于帕金森病患阶段的描述。前不久，苹果又利用ResearchKit的工具包，开发了通过记录患者手握手机的抖动幅度，来进行临床描述的方法。\n\n可以说，收集信息成本的降低，让越来越多的医生可以不再需要购买昂贵的医疗设备进行初级临床数据收集。这里笔者特别强调初级的概念，因为高级医疗是一个非常需要精准的专业领域。降低的成本主要用来帮助更广泛的个人、地县级医院或社区医院的医生从事医疗判断，完成初级筛查，这是现在在中国存在的很严重的社会问题。\n\n2）\t移动多媒体通讯－让沟通成本变低\n机缘巧合，前两天和一位医生朋友聊到一个很有趣的话题，emoji，就是我们常见微信里的那种表情图标。他说因为这个事情，让他避免了一个险些出现的医患关系问题。当时的情况是两个人通过医患沟通的软件，谈话有点僵，大概是病人没有遵医嘱，所以出现了些问题。在郁闷之时，他误点击了一个女儿不知何时装进软件的一个很欢快的表情，发了出去，此后神奇的事情发生了，患者也回了一个表情，一来二去，两个人的谈话竟然转入了互相理解的环节。最近很流行一句话，“你懂的”。其实图像在许多时候可以表达出语言或者话语无法描述的内心含义，让许多的沟通解释变成了浮云。\n\n再举一个例子，笔者和产品经理一起看到用户反馈，说到患者教育有多么痛苦，几个小护士不停给患者打电话，累得半死不说，因为频繁打扰到患者的生活，患者既记不住，也不领情。于是我们尝试在移动设备里添加了一个被津津乐道的功能，医生可在设备上进行简单的配置，剩下的交给系统，然后自动发送到患者的移动设备上。患者可以在任何时间看到这些精心制作的文字、图片、影音信息，并填写反馈信息，从而得到了非常好的效果。\n\n1）\t高效移动计算能力－让计算成本变低\n说到移动计算能力，在互联网这个时代，更多的人会想到大数据分析和数据整合。是的，医疗的大数据分析需求是非常大的，而且广泛的，这也是笔者目前专注的重点。但其实现实世界远不仅于此。以我们近期主导的一次搜索引擎改造为例，最传统的医生若想提升自己的研究水平，需要到病案室去，利用各种检索方法，搜寻到自己想找的“诊断”，“药典”或者“病历”资料。后来图书馆里有了电脑，查询变得稍微方便了一点。不过今天，技术的发展，让医生可以随时随地在手机上查找想要的东西，高效的搜索引擎，通过对海量数据的索引，完成目标的获取，并返回给医生的移动终端，这个过程可以在家里、地铁上、回家的路上、或者某个餐馆里。\n\n最后，如果一定要个总结，那就以笔者做博士研究时导师说过的话作为结语：创新无非两种，一种是跳跃性的，突破进展，第二种是延续性的，遵循规律踏实下来探索。前者虽然伟大，创立了诸如相对论等的伟大理论，而后者才是世界上绝大多数成果的来由，不断促进着社会前进。同理我想说，所谓颠覆是一点一滴的创新的汇集；所谓创新本身，尤其是移动互联网的创新，也没有那么难，还是可以从一些规律里面寻找创新点，结合业务去探寻。作为一家创新互联网公司的技术团队，我们继续努力了解医疗与医生的每一滴渴求，从技术出发降低他们的工作难度，这就是我们的技术颠覆传统医疗之路。\n","source":"_posts/tech-destruction-of-medicine.md","raw":"title: 为什么说技术颠覆传统医疗\ndate: 2015-05-19 20:26:40\ncategories:\n- Business Strategy\ntags:\n- tech\n---\nhttp://server.it168.com/a2015/0519/1729/000001729986.shtml\n\n最近一直在做移动医疗创业，作为一个创新公司的技术负责人，一方面需要努力与团队研发更有价值的产品功能，另一方面还需要关心业务的发展。毕竟，没有业务，技术只是玩具。看了许多移动医疗方面的文章，大多从纯业务角度去划分医疗的阶段（如诊前、诊中和诊后），并以此出发去考虑移动互联网能做些什么，以及如何评价一个创业公司。\n\n这种方法固然很好，但还有另一个事实被忽略了，那就是互联网从它诞生的那一刻就不是适应，而是改变，通过让人变得更为高效或者更为舒适，去改变人们过去的旧有习惯。举个例子，大约2年前，每个人还习惯于招手打车，那个时候，“有事儿”，“出门”，“找出租车多的大街”，“招手打车”。现在的习惯是什么呢？“有事儿”，“找车”，“车到”，“出门”，“上车”。所以，只要新方法足够方便，习惯并不是一成不变的。归根结底，新的技术让一切改变了。\n\n回到互联网这个话题里，我们来看看最近几年的哪些移动技术改变了人们的生活习惯。 \n\n1）\t高效移动计算能力－让计算成本变低\n自从2007年的谷歌G1，应该说是移动计算的一个里程碑，安卓系统的推出，让人们发现原来手机不仅仅是一个为个体资源服务的小盒子，而是一个可以联系从本地到远程的计算资源。\n\n其实很多人认为移动的发展是因为使用了碎片时间，这个说法没有错，但是碎片时间以前没有么？当然不是。根本问题是，计算资源提升了。这里要澄清一点的，“计算”不只是机器的事情，也需要人不断的进行输入、调整和验证输出结果，才能完整的实现真正意义上的计算。以前计算只能放在办公室里，然后下班回家，现在你可以时时刻刻的通过移动设备对需要计算的资源进行输入，实际的计算有可能是手机端，也有可能通过手机上传到远程服务器端完成，但不管怎样，任何计算都有机会时时刻刻被发起，或者经过调整被再次发起。这就是移动计算带给人类的革新。\n\n2）\t移动多媒体通讯－让沟通成本变低\n\n说到沟通，其实归根结底是个沟通两端展示的问题。展示的效率，展示的形式，决定了沟通效果的好坏。 “文字”的发明让沟通可以实现远程而非必须面对面。电信的发明远程的传播效率更加便捷。而如今，通过移动设备的多媒体通信，让人们不再需要时间和空间的约束。与此同时，多种媒体的传输使得这种沟通变得更加容易理解。一张图片、一个视频可能胜过万语千言。也正是因为展示的快捷和易于理解，成本变低的结果，我们用同样的代价，可以获得更多的沟通，那么随之带来的，包括信息更加透明化，更加海量化也就水到渠成了。\n\n3）\t多种辅助设备－让收集成本变低\n上面提到谷歌，接下来可想而知少不了苹果，笔者看来这家公司的伟大，不仅仅在于他发明多少很牛很炫酷的东西，而是他降低了各种辅助设备的成本，并把他们集中在了一台移动设备上。无论是陀螺仪、加速计、还是血氧传感器的引入，都大大增加了人们在它上面的想象空间。因为从那以后，人们有机会利用更加低廉的成本，收集更加广泛的信息。\n\n正是因为上述所言的变化，使得人们有机会寻求新的改变，用这样的技术去创造新的生活习惯。如果我们意识到移动技术在过去八年时间里成功改变了人们的许多生活习惯；如果我们认为医疗群体也是一个有生活的人群，那么移动技术也一定会类似的，在这三个维度上改变传统医疗，使医生改变旧有习惯。更为高效或更舒服地从事工作。\n\n从公司医疗业务同事那里，笔者不断尝试了解尽可能多的信息，寻找可能带来的改变。医疗会第一个想到“看病”，看病是什么，要“收集信息”，“制定方案”，“随访结果”，“调整方案”。只要是医疗，一定涉及这几个话题，这是循证医学的根基，无论诊前，诊中还是诊后。这也是几乎绝大部分技术型工种的工作方法（比如开发“收集需求”，“架构设计”，“结果测试”，“重构”）。于是乎，我们这回倒着说，从最后一条开始：\n\n3）\t多种辅助设备－让收集成本变低\n\n现在不少人在这方面做努力，其中最典型的创新，就是大量硬件、软件的简易医疗设备的。过去医院、医疗单位、药厂研究机构等收集病历，简直是一项无比繁冗复杂的工作，如今只需要用收集拍摄一下，就会收集起来，自动识别成病历数据。\n\n硬件方面，一个皮肤科常用的探测仪，动辄几千、上万，还操作极其复杂。笔者自己从“华强北”花36块钱买了一个高倍放大镜，配搭上自己的手机摄像头，把照出来的效果问了几个医生，还真不错。\n\n再比如对帕金森的诊治和判断，一直是医疗界的难题，尤其是如何形容，让许多医生非常痛苦。后来开始有专注病例采集的应用，使用视频来进行数据采集，大大方便了对于帕金森病患阶段的描述。前不久，苹果又利用ResearchKit的工具包，开发了通过记录患者手握手机的抖动幅度，来进行临床描述的方法。\n\n可以说，收集信息成本的降低，让越来越多的医生可以不再需要购买昂贵的医疗设备进行初级临床数据收集。这里笔者特别强调初级的概念，因为高级医疗是一个非常需要精准的专业领域。降低的成本主要用来帮助更广泛的个人、地县级医院或社区医院的医生从事医疗判断，完成初级筛查，这是现在在中国存在的很严重的社会问题。\n\n2）\t移动多媒体通讯－让沟通成本变低\n机缘巧合，前两天和一位医生朋友聊到一个很有趣的话题，emoji，就是我们常见微信里的那种表情图标。他说因为这个事情，让他避免了一个险些出现的医患关系问题。当时的情况是两个人通过医患沟通的软件，谈话有点僵，大概是病人没有遵医嘱，所以出现了些问题。在郁闷之时，他误点击了一个女儿不知何时装进软件的一个很欢快的表情，发了出去，此后神奇的事情发生了，患者也回了一个表情，一来二去，两个人的谈话竟然转入了互相理解的环节。最近很流行一句话，“你懂的”。其实图像在许多时候可以表达出语言或者话语无法描述的内心含义，让许多的沟通解释变成了浮云。\n\n再举一个例子，笔者和产品经理一起看到用户反馈，说到患者教育有多么痛苦，几个小护士不停给患者打电话，累得半死不说，因为频繁打扰到患者的生活，患者既记不住，也不领情。于是我们尝试在移动设备里添加了一个被津津乐道的功能，医生可在设备上进行简单的配置，剩下的交给系统，然后自动发送到患者的移动设备上。患者可以在任何时间看到这些精心制作的文字、图片、影音信息，并填写反馈信息，从而得到了非常好的效果。\n\n1）\t高效移动计算能力－让计算成本变低\n说到移动计算能力，在互联网这个时代，更多的人会想到大数据分析和数据整合。是的，医疗的大数据分析需求是非常大的，而且广泛的，这也是笔者目前专注的重点。但其实现实世界远不仅于此。以我们近期主导的一次搜索引擎改造为例，最传统的医生若想提升自己的研究水平，需要到病案室去，利用各种检索方法，搜寻到自己想找的“诊断”，“药典”或者“病历”资料。后来图书馆里有了电脑，查询变得稍微方便了一点。不过今天，技术的发展，让医生可以随时随地在手机上查找想要的东西，高效的搜索引擎，通过对海量数据的索引，完成目标的获取，并返回给医生的移动终端，这个过程可以在家里、地铁上、回家的路上、或者某个餐馆里。\n\n最后，如果一定要个总结，那就以笔者做博士研究时导师说过的话作为结语：创新无非两种，一种是跳跃性的，突破进展，第二种是延续性的，遵循规律踏实下来探索。前者虽然伟大，创立了诸如相对论等的伟大理论，而后者才是世界上绝大多数成果的来由，不断促进着社会前进。同理我想说，所谓颠覆是一点一滴的创新的汇集；所谓创新本身，尤其是移动互联网的创新，也没有那么难，还是可以从一些规律里面寻找创新点，结合业务去探寻。作为一家创新互联网公司的技术团队，我们继续努力了解医疗与医生的每一滴渴求，从技术出发降低他们的工作难度，这就是我们的技术颠覆传统医疗之路。\n","slug":"tech-destruction-of-medicine","published":1,"updated":"2018-04-16T09:59:37.039Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcp30013rlfy6dbvvpcc","content":"<p><a href=\"http://server.it168.com/a2015/0519/1729/000001729986.shtml\" target=\"_blank\" rel=\"noopener\">http://server.it168.com/a2015/0519/1729/000001729986.shtml</a></p>\n<p>最近一直在做移动医疗创业，作为一个创新公司的技术负责人，一方面需要努力与团队研发更有价值的产品功能，另一方面还需要关心业务的发展。毕竟，没有业务，技术只是玩具。看了许多移动医疗方面的文章，大多从纯业务角度去划分医疗的阶段（如诊前、诊中和诊后），并以此出发去考虑移动互联网能做些什么，以及如何评价一个创业公司。</p>\n<p>这种方法固然很好，但还有另一个事实被忽略了，那就是互联网从它诞生的那一刻就不是适应，而是改变，通过让人变得更为高效或者更为舒适，去改变人们过去的旧有习惯。举个例子，大约2年前，每个人还习惯于招手打车，那个时候，“有事儿”，“出门”，“找出租车多的大街”，“招手打车”。现在的习惯是什么呢？“有事儿”，“找车”，“车到”，“出门”，“上车”。所以，只要新方法足够方便，习惯并不是一成不变的。归根结底，新的技术让一切改变了。</p>\n<p>回到互联网这个话题里，我们来看看最近几年的哪些移动技术改变了人们的生活习惯。 </p>\n<p>1）    高效移动计算能力－让计算成本变低<br>自从2007年的谷歌G1，应该说是移动计算的一个里程碑，安卓系统的推出，让人们发现原来手机不仅仅是一个为个体资源服务的小盒子，而是一个可以联系从本地到远程的计算资源。</p>\n<p>其实很多人认为移动的发展是因为使用了碎片时间，这个说法没有错，但是碎片时间以前没有么？当然不是。根本问题是，计算资源提升了。这里要澄清一点的，“计算”不只是机器的事情，也需要人不断的进行输入、调整和验证输出结果，才能完整的实现真正意义上的计算。以前计算只能放在办公室里，然后下班回家，现在你可以时时刻刻的通过移动设备对需要计算的资源进行输入，实际的计算有可能是手机端，也有可能通过手机上传到远程服务器端完成，但不管怎样，任何计算都有机会时时刻刻被发起，或者经过调整被再次发起。这就是移动计算带给人类的革新。</p>\n<p>2）    移动多媒体通讯－让沟通成本变低</p>\n<p>说到沟通，其实归根结底是个沟通两端展示的问题。展示的效率，展示的形式，决定了沟通效果的好坏。 “文字”的发明让沟通可以实现远程而非必须面对面。电信的发明远程的传播效率更加便捷。而如今，通过移动设备的多媒体通信，让人们不再需要时间和空间的约束。与此同时，多种媒体的传输使得这种沟通变得更加容易理解。一张图片、一个视频可能胜过万语千言。也正是因为展示的快捷和易于理解，成本变低的结果，我们用同样的代价，可以获得更多的沟通，那么随之带来的，包括信息更加透明化，更加海量化也就水到渠成了。</p>\n<p>3）    多种辅助设备－让收集成本变低<br>上面提到谷歌，接下来可想而知少不了苹果，笔者看来这家公司的伟大，不仅仅在于他发明多少很牛很炫酷的东西，而是他降低了各种辅助设备的成本，并把他们集中在了一台移动设备上。无论是陀螺仪、加速计、还是血氧传感器的引入，都大大增加了人们在它上面的想象空间。因为从那以后，人们有机会利用更加低廉的成本，收集更加广泛的信息。</p>\n<p>正是因为上述所言的变化，使得人们有机会寻求新的改变，用这样的技术去创造新的生活习惯。如果我们意识到移动技术在过去八年时间里成功改变了人们的许多生活习惯；如果我们认为医疗群体也是一个有生活的人群，那么移动技术也一定会类似的，在这三个维度上改变传统医疗，使医生改变旧有习惯。更为高效或更舒服地从事工作。</p>\n<p>从公司医疗业务同事那里，笔者不断尝试了解尽可能多的信息，寻找可能带来的改变。医疗会第一个想到“看病”，看病是什么，要“收集信息”，“制定方案”，“随访结果”，“调整方案”。只要是医疗，一定涉及这几个话题，这是循证医学的根基，无论诊前，诊中还是诊后。这也是几乎绝大部分技术型工种的工作方法（比如开发“收集需求”，“架构设计”，“结果测试”，“重构”）。于是乎，我们这回倒着说，从最后一条开始：</p>\n<p>3）    多种辅助设备－让收集成本变低</p>\n<p>现在不少人在这方面做努力，其中最典型的创新，就是大量硬件、软件的简易医疗设备的。过去医院、医疗单位、药厂研究机构等收集病历，简直是一项无比繁冗复杂的工作，如今只需要用收集拍摄一下，就会收集起来，自动识别成病历数据。</p>\n<p>硬件方面，一个皮肤科常用的探测仪，动辄几千、上万，还操作极其复杂。笔者自己从“华强北”花36块钱买了一个高倍放大镜，配搭上自己的手机摄像头，把照出来的效果问了几个医生，还真不错。</p>\n<p>再比如对帕金森的诊治和判断，一直是医疗界的难题，尤其是如何形容，让许多医生非常痛苦。后来开始有专注病例采集的应用，使用视频来进行数据采集，大大方便了对于帕金森病患阶段的描述。前不久，苹果又利用ResearchKit的工具包，开发了通过记录患者手握手机的抖动幅度，来进行临床描述的方法。</p>\n<p>可以说，收集信息成本的降低，让越来越多的医生可以不再需要购买昂贵的医疗设备进行初级临床数据收集。这里笔者特别强调初级的概念，因为高级医疗是一个非常需要精准的专业领域。降低的成本主要用来帮助更广泛的个人、地县级医院或社区医院的医生从事医疗判断，完成初级筛查，这是现在在中国存在的很严重的社会问题。</p>\n<p>2）    移动多媒体通讯－让沟通成本变低<br>机缘巧合，前两天和一位医生朋友聊到一个很有趣的话题，emoji，就是我们常见微信里的那种表情图标。他说因为这个事情，让他避免了一个险些出现的医患关系问题。当时的情况是两个人通过医患沟通的软件，谈话有点僵，大概是病人没有遵医嘱，所以出现了些问题。在郁闷之时，他误点击了一个女儿不知何时装进软件的一个很欢快的表情，发了出去，此后神奇的事情发生了，患者也回了一个表情，一来二去，两个人的谈话竟然转入了互相理解的环节。最近很流行一句话，“你懂的”。其实图像在许多时候可以表达出语言或者话语无法描述的内心含义，让许多的沟通解释变成了浮云。</p>\n<p>再举一个例子，笔者和产品经理一起看到用户反馈，说到患者教育有多么痛苦，几个小护士不停给患者打电话，累得半死不说，因为频繁打扰到患者的生活，患者既记不住，也不领情。于是我们尝试在移动设备里添加了一个被津津乐道的功能，医生可在设备上进行简单的配置，剩下的交给系统，然后自动发送到患者的移动设备上。患者可以在任何时间看到这些精心制作的文字、图片、影音信息，并填写反馈信息，从而得到了非常好的效果。</p>\n<p>1）    高效移动计算能力－让计算成本变低<br>说到移动计算能力，在互联网这个时代，更多的人会想到大数据分析和数据整合。是的，医疗的大数据分析需求是非常大的，而且广泛的，这也是笔者目前专注的重点。但其实现实世界远不仅于此。以我们近期主导的一次搜索引擎改造为例，最传统的医生若想提升自己的研究水平，需要到病案室去，利用各种检索方法，搜寻到自己想找的“诊断”，“药典”或者“病历”资料。后来图书馆里有了电脑，查询变得稍微方便了一点。不过今天，技术的发展，让医生可以随时随地在手机上查找想要的东西，高效的搜索引擎，通过对海量数据的索引，完成目标的获取，并返回给医生的移动终端，这个过程可以在家里、地铁上、回家的路上、或者某个餐馆里。</p>\n<p>最后，如果一定要个总结，那就以笔者做博士研究时导师说过的话作为结语：创新无非两种，一种是跳跃性的，突破进展，第二种是延续性的，遵循规律踏实下来探索。前者虽然伟大，创立了诸如相对论等的伟大理论，而后者才是世界上绝大多数成果的来由，不断促进着社会前进。同理我想说，所谓颠覆是一点一滴的创新的汇集；所谓创新本身，尤其是移动互联网的创新，也没有那么难，还是可以从一些规律里面寻找创新点，结合业务去探寻。作为一家创新互联网公司的技术团队，我们继续努力了解医疗与医生的每一滴渴求，从技术出发降低他们的工作难度，这就是我们的技术颠覆传统医疗之路。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://server.it168.com/a2015/0519/1729/000001729986.shtml\" target=\"_blank\" rel=\"noopener\">http://server.it168.com/a2015/0519/1729/000001729986.shtml</a></p>\n<p>最近一直在做移动医疗创业，作为一个创新公司的技术负责人，一方面需要努力与团队研发更有价值的产品功能，另一方面还需要关心业务的发展。毕竟，没有业务，技术只是玩具。看了许多移动医疗方面的文章，大多从纯业务角度去划分医疗的阶段（如诊前、诊中和诊后），并以此出发去考虑移动互联网能做些什么，以及如何评价一个创业公司。</p>\n<p>这种方法固然很好，但还有另一个事实被忽略了，那就是互联网从它诞生的那一刻就不是适应，而是改变，通过让人变得更为高效或者更为舒适，去改变人们过去的旧有习惯。举个例子，大约2年前，每个人还习惯于招手打车，那个时候，“有事儿”，“出门”，“找出租车多的大街”，“招手打车”。现在的习惯是什么呢？“有事儿”，“找车”，“车到”，“出门”，“上车”。所以，只要新方法足够方便，习惯并不是一成不变的。归根结底，新的技术让一切改变了。</p>\n<p>回到互联网这个话题里，我们来看看最近几年的哪些移动技术改变了人们的生活习惯。 </p>\n<p>1）    高效移动计算能力－让计算成本变低<br>自从2007年的谷歌G1，应该说是移动计算的一个里程碑，安卓系统的推出，让人们发现原来手机不仅仅是一个为个体资源服务的小盒子，而是一个可以联系从本地到远程的计算资源。</p>\n<p>其实很多人认为移动的发展是因为使用了碎片时间，这个说法没有错，但是碎片时间以前没有么？当然不是。根本问题是，计算资源提升了。这里要澄清一点的，“计算”不只是机器的事情，也需要人不断的进行输入、调整和验证输出结果，才能完整的实现真正意义上的计算。以前计算只能放在办公室里，然后下班回家，现在你可以时时刻刻的通过移动设备对需要计算的资源进行输入，实际的计算有可能是手机端，也有可能通过手机上传到远程服务器端完成，但不管怎样，任何计算都有机会时时刻刻被发起，或者经过调整被再次发起。这就是移动计算带给人类的革新。</p>\n<p>2）    移动多媒体通讯－让沟通成本变低</p>\n<p>说到沟通，其实归根结底是个沟通两端展示的问题。展示的效率，展示的形式，决定了沟通效果的好坏。 “文字”的发明让沟通可以实现远程而非必须面对面。电信的发明远程的传播效率更加便捷。而如今，通过移动设备的多媒体通信，让人们不再需要时间和空间的约束。与此同时，多种媒体的传输使得这种沟通变得更加容易理解。一张图片、一个视频可能胜过万语千言。也正是因为展示的快捷和易于理解，成本变低的结果，我们用同样的代价，可以获得更多的沟通，那么随之带来的，包括信息更加透明化，更加海量化也就水到渠成了。</p>\n<p>3）    多种辅助设备－让收集成本变低<br>上面提到谷歌，接下来可想而知少不了苹果，笔者看来这家公司的伟大，不仅仅在于他发明多少很牛很炫酷的东西，而是他降低了各种辅助设备的成本，并把他们集中在了一台移动设备上。无论是陀螺仪、加速计、还是血氧传感器的引入，都大大增加了人们在它上面的想象空间。因为从那以后，人们有机会利用更加低廉的成本，收集更加广泛的信息。</p>\n<p>正是因为上述所言的变化，使得人们有机会寻求新的改变，用这样的技术去创造新的生活习惯。如果我们意识到移动技术在过去八年时间里成功改变了人们的许多生活习惯；如果我们认为医疗群体也是一个有生活的人群，那么移动技术也一定会类似的，在这三个维度上改变传统医疗，使医生改变旧有习惯。更为高效或更舒服地从事工作。</p>\n<p>从公司医疗业务同事那里，笔者不断尝试了解尽可能多的信息，寻找可能带来的改变。医疗会第一个想到“看病”，看病是什么，要“收集信息”，“制定方案”，“随访结果”，“调整方案”。只要是医疗，一定涉及这几个话题，这是循证医学的根基，无论诊前，诊中还是诊后。这也是几乎绝大部分技术型工种的工作方法（比如开发“收集需求”，“架构设计”，“结果测试”，“重构”）。于是乎，我们这回倒着说，从最后一条开始：</p>\n<p>3）    多种辅助设备－让收集成本变低</p>\n<p>现在不少人在这方面做努力，其中最典型的创新，就是大量硬件、软件的简易医疗设备的。过去医院、医疗单位、药厂研究机构等收集病历，简直是一项无比繁冗复杂的工作，如今只需要用收集拍摄一下，就会收集起来，自动识别成病历数据。</p>\n<p>硬件方面，一个皮肤科常用的探测仪，动辄几千、上万，还操作极其复杂。笔者自己从“华强北”花36块钱买了一个高倍放大镜，配搭上自己的手机摄像头，把照出来的效果问了几个医生，还真不错。</p>\n<p>再比如对帕金森的诊治和判断，一直是医疗界的难题，尤其是如何形容，让许多医生非常痛苦。后来开始有专注病例采集的应用，使用视频来进行数据采集，大大方便了对于帕金森病患阶段的描述。前不久，苹果又利用ResearchKit的工具包，开发了通过记录患者手握手机的抖动幅度，来进行临床描述的方法。</p>\n<p>可以说，收集信息成本的降低，让越来越多的医生可以不再需要购买昂贵的医疗设备进行初级临床数据收集。这里笔者特别强调初级的概念，因为高级医疗是一个非常需要精准的专业领域。降低的成本主要用来帮助更广泛的个人、地县级医院或社区医院的医生从事医疗判断，完成初级筛查，这是现在在中国存在的很严重的社会问题。</p>\n<p>2）    移动多媒体通讯－让沟通成本变低<br>机缘巧合，前两天和一位医生朋友聊到一个很有趣的话题，emoji，就是我们常见微信里的那种表情图标。他说因为这个事情，让他避免了一个险些出现的医患关系问题。当时的情况是两个人通过医患沟通的软件，谈话有点僵，大概是病人没有遵医嘱，所以出现了些问题。在郁闷之时，他误点击了一个女儿不知何时装进软件的一个很欢快的表情，发了出去，此后神奇的事情发生了，患者也回了一个表情，一来二去，两个人的谈话竟然转入了互相理解的环节。最近很流行一句话，“你懂的”。其实图像在许多时候可以表达出语言或者话语无法描述的内心含义，让许多的沟通解释变成了浮云。</p>\n<p>再举一个例子，笔者和产品经理一起看到用户反馈，说到患者教育有多么痛苦，几个小护士不停给患者打电话，累得半死不说，因为频繁打扰到患者的生活，患者既记不住，也不领情。于是我们尝试在移动设备里添加了一个被津津乐道的功能，医生可在设备上进行简单的配置，剩下的交给系统，然后自动发送到患者的移动设备上。患者可以在任何时间看到这些精心制作的文字、图片、影音信息，并填写反馈信息，从而得到了非常好的效果。</p>\n<p>1）    高效移动计算能力－让计算成本变低<br>说到移动计算能力，在互联网这个时代，更多的人会想到大数据分析和数据整合。是的，医疗的大数据分析需求是非常大的，而且广泛的，这也是笔者目前专注的重点。但其实现实世界远不仅于此。以我们近期主导的一次搜索引擎改造为例，最传统的医生若想提升自己的研究水平，需要到病案室去，利用各种检索方法，搜寻到自己想找的“诊断”，“药典”或者“病历”资料。后来图书馆里有了电脑，查询变得稍微方便了一点。不过今天，技术的发展，让医生可以随时随地在手机上查找想要的东西，高效的搜索引擎，通过对海量数据的索引，完成目标的获取，并返回给医生的移动终端，这个过程可以在家里、地铁上、回家的路上、或者某个餐馆里。</p>\n<p>最后，如果一定要个总结，那就以笔者做博士研究时导师说过的话作为结语：创新无非两种，一种是跳跃性的，突破进展，第二种是延续性的，遵循规律踏实下来探索。前者虽然伟大，创立了诸如相对论等的伟大理论，而后者才是世界上绝大多数成果的来由，不断促进着社会前进。同理我想说，所谓颠覆是一点一滴的创新的汇集；所谓创新本身，尤其是移动互联网的创新，也没有那么难，还是可以从一些规律里面寻找创新点，结合业务去探寻。作为一家创新互联网公司的技术团队，我们继续努力了解医疗与医生的每一滴渴求，从技术出发降低他们的工作难度，这就是我们的技术颠覆传统医疗之路。</p>\n"},{"title":"wechat","date":"2014-12-31T10:46:41.000Z","_content":"开始一些关于微信方面的研究。其实很早就做过，只是这次又捡起来。对这部分的研究起源于那个需求，就是大事件记。作为数据方面的尝试，这是我第一次搞大事件。我希望以此为模型开始不断推演未来数据分析所需要的更方面东西。无论是业务上的，算法上的，应用上的，包括组织形式上的。ok，闲话少说，关于那个的总结以后再写，先说微信。\n\n微信公共平台确实是个对于小商家和小需求型交互非常好的地方。在我看来，他的最大特点就是包含了自然语言交互，和基本的按钮交互，两种功能。一般认为，手机移动端的设计难点就是小小的屏幕如何放很多的交互是一个难题。对于微信公共平台来说，3个按钮（包括附属菜单）组成的基本交互，解决了最急切交互的需求。剩下的交给了自然语言或者有特定标示的自然语言，是非常好的方式，符合人们对于数据需求的认知。\n\n微信包括了企业号，服务号，订阅号。企业号就不说了，服务号理论上比订阅号要高级一些，因为可以支持更多的高级API接口，最最关键的是，可以支持微信支付。更适用于企业用户，而订阅号则没有这个功能。订阅号更多的是适应于自媒体，因为他最大的优势是每天可以发送1条群发消息，这点远好于服务号，每月4条。我的这个查询大事件，想来每次出现大事件的时候，也会群发给我的所有订阅者，因此，我考虑订阅号对我来讲更加有意义，虽然我不能说每天都发生大事件，但一个月有超过4次大事件，在这个快速发展的互联网时代，还是非常有可能。\n\n说到服务API开发，我觉得这方面以前拿ruby写过代码，逻辑上没什么难的。现在就是我的python水平有点垃圾，之前jira－soap的服务，是看别人的python库，而且公司用的con和jira版本是在太低，soap的协议，不支持rest，新的nodejs都没有，哎没办法，上吧。写一个python微信的服务","source":"_posts/wechat.md","raw":"title: wechat\ndate: 2014-12-31 18:46:41\ncategories:\n- Technology\ntags:\n- tech\n---\n开始一些关于微信方面的研究。其实很早就做过，只是这次又捡起来。对这部分的研究起源于那个需求，就是大事件记。作为数据方面的尝试，这是我第一次搞大事件。我希望以此为模型开始不断推演未来数据分析所需要的更方面东西。无论是业务上的，算法上的，应用上的，包括组织形式上的。ok，闲话少说，关于那个的总结以后再写，先说微信。\n\n微信公共平台确实是个对于小商家和小需求型交互非常好的地方。在我看来，他的最大特点就是包含了自然语言交互，和基本的按钮交互，两种功能。一般认为，手机移动端的设计难点就是小小的屏幕如何放很多的交互是一个难题。对于微信公共平台来说，3个按钮（包括附属菜单）组成的基本交互，解决了最急切交互的需求。剩下的交给了自然语言或者有特定标示的自然语言，是非常好的方式，符合人们对于数据需求的认知。\n\n微信包括了企业号，服务号，订阅号。企业号就不说了，服务号理论上比订阅号要高级一些，因为可以支持更多的高级API接口，最最关键的是，可以支持微信支付。更适用于企业用户，而订阅号则没有这个功能。订阅号更多的是适应于自媒体，因为他最大的优势是每天可以发送1条群发消息，这点远好于服务号，每月4条。我的这个查询大事件，想来每次出现大事件的时候，也会群发给我的所有订阅者，因此，我考虑订阅号对我来讲更加有意义，虽然我不能说每天都发生大事件，但一个月有超过4次大事件，在这个快速发展的互联网时代，还是非常有可能。\n\n说到服务API开发，我觉得这方面以前拿ruby写过代码，逻辑上没什么难的。现在就是我的python水平有点垃圾，之前jira－soap的服务，是看别人的python库，而且公司用的con和jira版本是在太低，soap的协议，不支持rest，新的nodejs都没有，哎没办法，上吧。写一个python微信的服务","slug":"wechat","published":1,"updated":"2018-04-16T09:59:37.040Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcp40014rlfyxr1gwe18","content":"<p>开始一些关于微信方面的研究。其实很早就做过，只是这次又捡起来。对这部分的研究起源于那个需求，就是大事件记。作为数据方面的尝试，这是我第一次搞大事件。我希望以此为模型开始不断推演未来数据分析所需要的更方面东西。无论是业务上的，算法上的，应用上的，包括组织形式上的。ok，闲话少说，关于那个的总结以后再写，先说微信。</p>\n<p>微信公共平台确实是个对于小商家和小需求型交互非常好的地方。在我看来，他的最大特点就是包含了自然语言交互，和基本的按钮交互，两种功能。一般认为，手机移动端的设计难点就是小小的屏幕如何放很多的交互是一个难题。对于微信公共平台来说，3个按钮（包括附属菜单）组成的基本交互，解决了最急切交互的需求。剩下的交给了自然语言或者有特定标示的自然语言，是非常好的方式，符合人们对于数据需求的认知。</p>\n<p>微信包括了企业号，服务号，订阅号。企业号就不说了，服务号理论上比订阅号要高级一些，因为可以支持更多的高级API接口，最最关键的是，可以支持微信支付。更适用于企业用户，而订阅号则没有这个功能。订阅号更多的是适应于自媒体，因为他最大的优势是每天可以发送1条群发消息，这点远好于服务号，每月4条。我的这个查询大事件，想来每次出现大事件的时候，也会群发给我的所有订阅者，因此，我考虑订阅号对我来讲更加有意义，虽然我不能说每天都发生大事件，但一个月有超过4次大事件，在这个快速发展的互联网时代，还是非常有可能。</p>\n<p>说到服务API开发，我觉得这方面以前拿ruby写过代码，逻辑上没什么难的。现在就是我的python水平有点垃圾，之前jira－soap的服务，是看别人的python库，而且公司用的con和jira版本是在太低，soap的协议，不支持rest，新的nodejs都没有，哎没办法，上吧。写一个python微信的服务</p>\n","site":{"data":{}},"excerpt":"","more":"<p>开始一些关于微信方面的研究。其实很早就做过，只是这次又捡起来。对这部分的研究起源于那个需求，就是大事件记。作为数据方面的尝试，这是我第一次搞大事件。我希望以此为模型开始不断推演未来数据分析所需要的更方面东西。无论是业务上的，算法上的，应用上的，包括组织形式上的。ok，闲话少说，关于那个的总结以后再写，先说微信。</p>\n<p>微信公共平台确实是个对于小商家和小需求型交互非常好的地方。在我看来，他的最大特点就是包含了自然语言交互，和基本的按钮交互，两种功能。一般认为，手机移动端的设计难点就是小小的屏幕如何放很多的交互是一个难题。对于微信公共平台来说，3个按钮（包括附属菜单）组成的基本交互，解决了最急切交互的需求。剩下的交给了自然语言或者有特定标示的自然语言，是非常好的方式，符合人们对于数据需求的认知。</p>\n<p>微信包括了企业号，服务号，订阅号。企业号就不说了，服务号理论上比订阅号要高级一些，因为可以支持更多的高级API接口，最最关键的是，可以支持微信支付。更适用于企业用户，而订阅号则没有这个功能。订阅号更多的是适应于自媒体，因为他最大的优势是每天可以发送1条群发消息，这点远好于服务号，每月4条。我的这个查询大事件，想来每次出现大事件的时候，也会群发给我的所有订阅者，因此，我考虑订阅号对我来讲更加有意义，虽然我不能说每天都发生大事件，但一个月有超过4次大事件，在这个快速发展的互联网时代，还是非常有可能。</p>\n<p>说到服务API开发，我觉得这方面以前拿ruby写过代码，逻辑上没什么难的。现在就是我的python水平有点垃圾，之前jira－soap的服务，是看别人的python库，而且公司用的con和jira版本是在太低，soap的协议，不支持rest，新的nodejs都没有，哎没办法，上吧。写一个python微信的服务</p>\n"},{"title":"对于tensorflow基本用法的一些记录","date":"2018-08-12T12:11:27.000Z","mathjax":true,"_content":"\n最近已经学到了机器学习的第四课CNN的部分。这个部分里面还是用到了一些Tensorflow的基本内容。这里把一些简单的方法做个总结，以做备忘，也许之后用得上。\n\n```Python\n # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep\n    nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes, iou_threshold)\n ```\n\n对于TF的Run始终觉得需要系统的理解一下\n```Python\n# Run the session with the correct tensors and choose the correct placeholders in the feed_dict.\n    # You'll need to use feed_dict={yolo_model.input: ... , K.learning_phase(): 0})\n    out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes],\n                                                  feed_dict={yolo_model.input: image_data, K.learning_phase(): 0})\n```\n\n\n这里除了TensorFlow还得多提一个Keras，一个构建在TF上面更加丰富函数的第三方包\n\n```Python\nkeras.backend.argmax(x, axis=-1)\nkeras.backend.max(x, axis=None, keepdims=False)\n```\n\nnp也有一些特殊的方法，比较不常见和不容易理解，下面np.eye就是把一个Y变成C个为一组的one-hot\n```Python\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)]\n    return Y\n```","source":"_posts/tensorflow-notes.md","raw":"title: 对于tensorflow基本用法的一些记录\ndate: 2018-08-12 20:11:27\ncategories:\n- Technology\ntags:\n- tech\n- deeplearning\nmathjax: true\n---\n\n最近已经学到了机器学习的第四课CNN的部分。这个部分里面还是用到了一些Tensorflow的基本内容。这里把一些简单的方法做个总结，以做备忘，也许之后用得上。\n\n```Python\n # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep\n    nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes, iou_threshold)\n ```\n\n对于TF的Run始终觉得需要系统的理解一下\n```Python\n# Run the session with the correct tensors and choose the correct placeholders in the feed_dict.\n    # You'll need to use feed_dict={yolo_model.input: ... , K.learning_phase(): 0})\n    out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes],\n                                                  feed_dict={yolo_model.input: image_data, K.learning_phase(): 0})\n```\n\n\n这里除了TensorFlow还得多提一个Keras，一个构建在TF上面更加丰富函数的第三方包\n\n```Python\nkeras.backend.argmax(x, axis=-1)\nkeras.backend.max(x, axis=None, keepdims=False)\n```\n\nnp也有一些特殊的方法，比较不常见和不容易理解，下面np.eye就是把一个Y变成C个为一组的one-hot\n```Python\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)]\n    return Y\n```","slug":"tensorflow-notes","published":1,"updated":"2018-09-09T10:50:07.301Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcp40015rlfy57llaje3","content":"<p>最近已经学到了机器学习的第四课CNN的部分。这个部分里面还是用到了一些Tensorflow的基本内容。这里把一些简单的方法做个总结，以做备忘，也许之后用得上。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep</span></span><br><span class=\"line\">   nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes, iou_threshold)</span><br></pre></td></tr></table></figure>\n<p>对于TF的Run始终觉得需要系统的理解一下<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Run the session with the correct tensors and choose the correct placeholders in the feed_dict.</span></span><br><span class=\"line\">    <span class=\"comment\"># You'll need to use feed_dict=&#123;yolo_model.input: ... , K.learning_phase(): 0&#125;)</span></span><br><span class=\"line\">    out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes],</span><br><span class=\"line\">                                                  feed_dict=&#123;yolo_model.input: image_data, K.learning_phase(): <span class=\"number\">0</span>&#125;)</span><br></pre></td></tr></table></figure></p>\n<p>这里除了TensorFlow还得多提一个Keras，一个构建在TF上面更加丰富函数的第三方包</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">keras.backend.argmax(x, axis=<span class=\"number\">-1</span>)</span><br><span class=\"line\">keras.backend.max(x, axis=<span class=\"keyword\">None</span>, keepdims=<span class=\"keyword\">False</span>)</span><br></pre></td></tr></table></figure>\n<p>np也有一些特殊的方法，比较不常见和不容易理解，下面np.eye就是把一个Y变成C个为一组的one-hot<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convert_to_one_hot</span><span class=\"params\">(Y, C)</span>:</span></span><br><span class=\"line\">    Y = np.eye(C)[Y.reshape(<span class=\"number\">-1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近已经学到了机器学习的第四课CNN的部分。这个部分里面还是用到了一些Tensorflow的基本内容。这里把一些简单的方法做个总结，以做备忘，也许之后用得上。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep</span></span><br><span class=\"line\">   nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes, iou_threshold)</span><br></pre></td></tr></table></figure>\n<p>对于TF的Run始终觉得需要系统的理解一下<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Run the session with the correct tensors and choose the correct placeholders in the feed_dict.</span></span><br><span class=\"line\">    <span class=\"comment\"># You'll need to use feed_dict=&#123;yolo_model.input: ... , K.learning_phase(): 0&#125;)</span></span><br><span class=\"line\">    out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes],</span><br><span class=\"line\">                                                  feed_dict=&#123;yolo_model.input: image_data, K.learning_phase(): <span class=\"number\">0</span>&#125;)</span><br></pre></td></tr></table></figure></p>\n<p>这里除了TensorFlow还得多提一个Keras，一个构建在TF上面更加丰富函数的第三方包</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">keras.backend.argmax(x, axis=<span class=\"number\">-1</span>)</span><br><span class=\"line\">keras.backend.max(x, axis=<span class=\"keyword\">None</span>, keepdims=<span class=\"keyword\">False</span>)</span><br></pre></td></tr></table></figure>\n<p>np也有一些特殊的方法，比较不常见和不容易理解，下面np.eye就是把一个Y变成C个为一组的one-hot<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convert_to_one_hot</span><span class=\"params\">(Y, C)</span>:</span></span><br><span class=\"line\">    Y = np.eye(C)[Y.reshape(<span class=\"number\">-1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y</span><br></pre></td></tr></table></figure></p>\n"},{"title":"关于文字集与编码","date":"2016-06-04T05:54:32.000Z","_content":"\n写Python2的人，很多人都见过下面这行Error\n```\nTraceback (most recent call last):\n  File \"/Users/Jack/Documents/ApricotForestDoc/2_product_rnd/redspiderlily/mailfetcher.py\", line 35, in <module>\n    if is_reply_mail(subject):\n  File \"/Users/Jack/Documents/ApricotForestDoc/2_product_rnd/redspiderlily/module/mailutil.py\", line 46, in is_reply_mail\n    return (\"回复\" in subject_.lower()) or (\"re:\" in subject_.lower())\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe5 in position 0: ordinal not in range(128)\n```\n\n上一次聊到Python3升级的一个重头就是unicode编码。所以这次想重点就聊两句字符编码。说实话就是在此之前自己也有些算不清楚。所以还是深入的跑到wiki上研究了一番，分享供参考。\n\n\nASCII，全称American Standard Code for Information Interchange，是一个字符编码标准。使用0-127（2^7)的数字代表不同的英文字符，这里包括大小写字母，空格，特殊字符等。这是一个大家都比较熟悉的字符集标准，可以看看下面图中的内容。这里不做赘述。\n\n{% asset_img ascii.png \"ASCII字符表\" %}\n\n但是，ASCII因为标准内含有的有限，带有音标的字符‘é’就无法很好的表述，更别提汉字了。到了1980年代，计算机技术发展，那时候大家已经开始使用字节（byte）来作为计算机基本计数单位。1byte＝8bit（2^8=256）,所以可以表述的字符变多了。那个时候开始出现了实用128-255这些数字表示发音单词。所以直到今天，你去看word里面的字符表，Latin依然可以看到这个顺序关系，大体就是这个原因。\n\n{% asset_img character_in_word.png \"MS Word的英文字符集\" %}\n\n再之后，当世界各地的语言发展出了各自不同的字符集体系，比如中国简体（GBK，GB18030），中文繁体（Big5，以前有个特别扯的名字叫做大五码），法语（Latin1），日语等等。本来各种语言字符集各自写互不干扰，倒也相安无事。但是，世界大融合嘛，于是问题来了。有人需要在中文里写上一段日语，就像这样 ++“日本語にほんご”++ 。于是问题就来了，怎么才能在同一个文件里现实不同的字符集的字符内容呢？1980年，人来开始尝试解决这个问题，并制定了一个新的计算机工业标准用以规范的处理、表示和编码全世界主要文字。这个标准叫做Unicode（全称是The Unicode Standard）。目前，Unicode标准是8.0，已经包含了全世界已经有超过12万个字符，覆盖129种现代和历史上的语言种类。在这里面需要说明一个额外的概念叫做Universal Coded Character Set (UCS，也叫做ISO 10646)。按照Wiki对于Unicode和UCS的说法，目前两套字符集应该是完全相同的。同一个数字在两个字符集里所代表的字符应该是相同的。但是Unicode的外延要多一些。USC仅仅是一个字符集，而对应的Unicode同时还规范了校对、标准化、表示顺序的算法等。就如同本文提及对Unicode的定义一样，Unicode出来包括了字符集，还有表现和处理方法的部分。因此Unicode应该说是一个更加广泛含义上的标准。\n\n理论上说，Unicode字符集或者UCS有110万字符点数可以被分配，目前时机分配成16个Plane，其中Plane0，被叫做BMP（Basic Multilingual Plane），一共65536个，其中绝大部分是中文（Chinese），日文（Japanese）和韩文（Korean），三者合称CJK。\n\n{% asset_img CJK.png \"Unicode中CJK所包含的字符浮点内容\" %}\n\n这里需要特别提一下Chinese Simplified（GBK，GB18030），自从2000年以后，中国政府规定，所有在中国售卖的软件产品必须支持Chinese Simplified（GB18030）字符集。因此在我国，就出现了一个神奇的事情，就是Unicode和Chinese Simplified双字符集并行的问题。\n\n有趣的是，笔者大概调研了几大中文网站的编码如下：\n\n| URI  | 字符编码 |\n| ------ | --------- |\n| https://www.taobao.com/ | Unicode(UTF-8)|\n| http://www.jd.com/ | Chinese Simplified (GBK)|\n| http://bj.meituan.com/ | Unicode(UTF-8)|\n| https://www.baidu.com/ | Unicode(UTF-8)|\n| http://cn.bing.com/ | Unicode(UTF-8)|\n| http://www.sohu.com/ | Chinese Simplified (GBK)|\n| http://www.qq.com/ | Chinese Simplified (GBK)|\n| http://www.sina.com.cn/ | Unicode(UTF-8)|\n\n感觉上各个大厂也是自说自话，不是不是很一致要遵守政府规定或者不遵守规定。这么说来，国家对这块儿在申请xxx备案的管理也不是很严格。\n\n话有点扯远了，咱们再回来。目前Unicode字符集共设定16个Plane（数字从0x000000-0x10FFFF）对应(2^16+2^20)对应（1,114,112）。刚刚说的Plane0，基础语言集定义是从0x000000-0x00FFFF(2^16)。其他的大家可以查，Plane1和2用了一些，其他基本上用的很少。因此总共来说目前分配的字符大约是12万。\n\n但是这样庞大的数字和计算机的比特（byte）之间并不统一，把一个Unicode字符串转换成Byte的过程，这里引入了一个叫做Encoding，编码的概念。1992年，为了兼容不同处理器和C语言，人们引入了一个编码标准，这就是大家广泛知道的UTF-8。截至2016年5月份，在WWW上的统计，UTF-8的使用率已经达到86.9%，对比GB2312（0.8%）。同时W3C也把UTF-8作为XML和HTML的推荐编码。\n\n下面我们来阐述一下UTF-8的实现原理（同时可以结合下图来看）：\n\n如果是7位以内表述的字符表数字，就只占用一个自己，表现为（0xxxxxxx），这样刚好和ASCII码的描述相一致，这样就不会造成原有ASCII的识别错乱，特别是针对C语言的strlen()和strcopy()。从第8位，也就是十位数的256开始，采用两字节表述模式（110xxxxx 10xxxxxx),最多可以表示11个bit位，也就是2^8(256)到2^12-1(4095)。以此类推。是不是一种很有趣的编码模式 :)\n\n{% asset_img utf8.png \"UTF-8在Wiki上的例子截图\" %}\n\n下面看一个栗子。以杏树林的“杏”字为题。\n\n{% asset_img example.png \"杏字的uft-8编码翻译\" %}\n\n所以总结一下，Unicode和ASCII是一个字符集的概念，他们是随着电信发展而产生的编码本。只不过Unicode有多包涵了表示和处理部分，范围会更广泛。为了让计算机能读懂编码，适应计算机的计算，我们有了诸如UTF-8类的编码方法。\n\n值得参考的一系列词条出处\nhttps://docs.python.org/3/howto/unicode.html\nhttps://en.wikipedia.org/wiki/Character_encoding\nhttps://en.wikipedia.org/wiki/Unicode\nhttps://en.wikipedia.org/wiki/Universal_Coded_Character_Set\nhttps://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane\nhttps://en.wikipedia.org/wiki/UTF-8","source":"_posts/unicode.md","raw":"title: 关于文字集与编码\ndate: 2016-06-04 13:54:32\ncategories:\n- Technology\ntags:\n- tech\n- python\n---\n\n写Python2的人，很多人都见过下面这行Error\n```\nTraceback (most recent call last):\n  File \"/Users/Jack/Documents/ApricotForestDoc/2_product_rnd/redspiderlily/mailfetcher.py\", line 35, in <module>\n    if is_reply_mail(subject):\n  File \"/Users/Jack/Documents/ApricotForestDoc/2_product_rnd/redspiderlily/module/mailutil.py\", line 46, in is_reply_mail\n    return (\"回复\" in subject_.lower()) or (\"re:\" in subject_.lower())\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe5 in position 0: ordinal not in range(128)\n```\n\n上一次聊到Python3升级的一个重头就是unicode编码。所以这次想重点就聊两句字符编码。说实话就是在此之前自己也有些算不清楚。所以还是深入的跑到wiki上研究了一番，分享供参考。\n\n\nASCII，全称American Standard Code for Information Interchange，是一个字符编码标准。使用0-127（2^7)的数字代表不同的英文字符，这里包括大小写字母，空格，特殊字符等。这是一个大家都比较熟悉的字符集标准，可以看看下面图中的内容。这里不做赘述。\n\n{% asset_img ascii.png \"ASCII字符表\" %}\n\n但是，ASCII因为标准内含有的有限，带有音标的字符‘é’就无法很好的表述，更别提汉字了。到了1980年代，计算机技术发展，那时候大家已经开始使用字节（byte）来作为计算机基本计数单位。1byte＝8bit（2^8=256）,所以可以表述的字符变多了。那个时候开始出现了实用128-255这些数字表示发音单词。所以直到今天，你去看word里面的字符表，Latin依然可以看到这个顺序关系，大体就是这个原因。\n\n{% asset_img character_in_word.png \"MS Word的英文字符集\" %}\n\n再之后，当世界各地的语言发展出了各自不同的字符集体系，比如中国简体（GBK，GB18030），中文繁体（Big5，以前有个特别扯的名字叫做大五码），法语（Latin1），日语等等。本来各种语言字符集各自写互不干扰，倒也相安无事。但是，世界大融合嘛，于是问题来了。有人需要在中文里写上一段日语，就像这样 ++“日本語にほんご”++ 。于是问题就来了，怎么才能在同一个文件里现实不同的字符集的字符内容呢？1980年，人来开始尝试解决这个问题，并制定了一个新的计算机工业标准用以规范的处理、表示和编码全世界主要文字。这个标准叫做Unicode（全称是The Unicode Standard）。目前，Unicode标准是8.0，已经包含了全世界已经有超过12万个字符，覆盖129种现代和历史上的语言种类。在这里面需要说明一个额外的概念叫做Universal Coded Character Set (UCS，也叫做ISO 10646)。按照Wiki对于Unicode和UCS的说法，目前两套字符集应该是完全相同的。同一个数字在两个字符集里所代表的字符应该是相同的。但是Unicode的外延要多一些。USC仅仅是一个字符集，而对应的Unicode同时还规范了校对、标准化、表示顺序的算法等。就如同本文提及对Unicode的定义一样，Unicode出来包括了字符集，还有表现和处理方法的部分。因此Unicode应该说是一个更加广泛含义上的标准。\n\n理论上说，Unicode字符集或者UCS有110万字符点数可以被分配，目前时机分配成16个Plane，其中Plane0，被叫做BMP（Basic Multilingual Plane），一共65536个，其中绝大部分是中文（Chinese），日文（Japanese）和韩文（Korean），三者合称CJK。\n\n{% asset_img CJK.png \"Unicode中CJK所包含的字符浮点内容\" %}\n\n这里需要特别提一下Chinese Simplified（GBK，GB18030），自从2000年以后，中国政府规定，所有在中国售卖的软件产品必须支持Chinese Simplified（GB18030）字符集。因此在我国，就出现了一个神奇的事情，就是Unicode和Chinese Simplified双字符集并行的问题。\n\n有趣的是，笔者大概调研了几大中文网站的编码如下：\n\n| URI  | 字符编码 |\n| ------ | --------- |\n| https://www.taobao.com/ | Unicode(UTF-8)|\n| http://www.jd.com/ | Chinese Simplified (GBK)|\n| http://bj.meituan.com/ | Unicode(UTF-8)|\n| https://www.baidu.com/ | Unicode(UTF-8)|\n| http://cn.bing.com/ | Unicode(UTF-8)|\n| http://www.sohu.com/ | Chinese Simplified (GBK)|\n| http://www.qq.com/ | Chinese Simplified (GBK)|\n| http://www.sina.com.cn/ | Unicode(UTF-8)|\n\n感觉上各个大厂也是自说自话，不是不是很一致要遵守政府规定或者不遵守规定。这么说来，国家对这块儿在申请xxx备案的管理也不是很严格。\n\n话有点扯远了，咱们再回来。目前Unicode字符集共设定16个Plane（数字从0x000000-0x10FFFF）对应(2^16+2^20)对应（1,114,112）。刚刚说的Plane0，基础语言集定义是从0x000000-0x00FFFF(2^16)。其他的大家可以查，Plane1和2用了一些，其他基本上用的很少。因此总共来说目前分配的字符大约是12万。\n\n但是这样庞大的数字和计算机的比特（byte）之间并不统一，把一个Unicode字符串转换成Byte的过程，这里引入了一个叫做Encoding，编码的概念。1992年，为了兼容不同处理器和C语言，人们引入了一个编码标准，这就是大家广泛知道的UTF-8。截至2016年5月份，在WWW上的统计，UTF-8的使用率已经达到86.9%，对比GB2312（0.8%）。同时W3C也把UTF-8作为XML和HTML的推荐编码。\n\n下面我们来阐述一下UTF-8的实现原理（同时可以结合下图来看）：\n\n如果是7位以内表述的字符表数字，就只占用一个自己，表现为（0xxxxxxx），这样刚好和ASCII码的描述相一致，这样就不会造成原有ASCII的识别错乱，特别是针对C语言的strlen()和strcopy()。从第8位，也就是十位数的256开始，采用两字节表述模式（110xxxxx 10xxxxxx),最多可以表示11个bit位，也就是2^8(256)到2^12-1(4095)。以此类推。是不是一种很有趣的编码模式 :)\n\n{% asset_img utf8.png \"UTF-8在Wiki上的例子截图\" %}\n\n下面看一个栗子。以杏树林的“杏”字为题。\n\n{% asset_img example.png \"杏字的uft-8编码翻译\" %}\n\n所以总结一下，Unicode和ASCII是一个字符集的概念，他们是随着电信发展而产生的编码本。只不过Unicode有多包涵了表示和处理部分，范围会更广泛。为了让计算机能读懂编码，适应计算机的计算，我们有了诸如UTF-8类的编码方法。\n\n值得参考的一系列词条出处\nhttps://docs.python.org/3/howto/unicode.html\nhttps://en.wikipedia.org/wiki/Character_encoding\nhttps://en.wikipedia.org/wiki/Unicode\nhttps://en.wikipedia.org/wiki/Universal_Coded_Character_Set\nhttps://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane\nhttps://en.wikipedia.org/wiki/UTF-8","slug":"unicode","published":1,"updated":"2018-04-16T09:59:37.073Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcp50016rlfyoa93cvwt","content":"<p>写Python2的人，很多人都见过下面这行Error<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File &quot;/Users/Jack/Documents/ApricotForestDoc/2_product_rnd/redspiderlily/mailfetcher.py&quot;, line 35, in &lt;module&gt;</span><br><span class=\"line\">    if is_reply_mail(subject):</span><br><span class=\"line\">  File &quot;/Users/Jack/Documents/ApricotForestDoc/2_product_rnd/redspiderlily/module/mailutil.py&quot;, line 46, in is_reply_mail</span><br><span class=\"line\">    return (&quot;回复&quot; in subject_.lower()) or (&quot;re:&quot; in subject_.lower())</span><br><span class=\"line\">UnicodeDecodeError: &apos;ascii&apos; codec can&apos;t decode byte 0xe5 in position 0: ordinal not in range(128)</span><br></pre></td></tr></table></figure></p>\n<p>上一次聊到Python3升级的一个重头就是unicode编码。所以这次想重点就聊两句字符编码。说实话就是在此之前自己也有些算不清楚。所以还是深入的跑到wiki上研究了一番，分享供参考。</p>\n<p>ASCII，全称American Standard Code for Information Interchange，是一个字符编码标准。使用0-127（2^7)的数字代表不同的英文字符，这里包括大小写字母，空格，特殊字符等。这是一个大家都比较熟悉的字符集标准，可以看看下面图中的内容。这里不做赘述。</p>\n<img src=\"/2016/06/04/unicode/ascii.png\" title=\"ASCII字符表\">\n<p>但是，ASCII因为标准内含有的有限，带有音标的字符‘é’就无法很好的表述，更别提汉字了。到了1980年代，计算机技术发展，那时候大家已经开始使用字节（byte）来作为计算机基本计数单位。1byte＝8bit（2^8=256）,所以可以表述的字符变多了。那个时候开始出现了实用128-255这些数字表示发音单词。所以直到今天，你去看word里面的字符表，Latin依然可以看到这个顺序关系，大体就是这个原因。</p>\n<img src=\"/2016/06/04/unicode/character_in_word.png\" title=\"MS Word的英文字符集\">\n<p>再之后，当世界各地的语言发展出了各自不同的字符集体系，比如中国简体（GBK，GB18030），中文繁体（Big5，以前有个特别扯的名字叫做大五码），法语（Latin1），日语等等。本来各种语言字符集各自写互不干扰，倒也相安无事。但是，世界大融合嘛，于是问题来了。有人需要在中文里写上一段日语，就像这样 ++“日本語にほんご”++ 。于是问题就来了，怎么才能在同一个文件里现实不同的字符集的字符内容呢？1980年，人来开始尝试解决这个问题，并制定了一个新的计算机工业标准用以规范的处理、表示和编码全世界主要文字。这个标准叫做Unicode（全称是The Unicode Standard）。目前，Unicode标准是8.0，已经包含了全世界已经有超过12万个字符，覆盖129种现代和历史上的语言种类。在这里面需要说明一个额外的概念叫做Universal Coded Character Set (UCS，也叫做ISO 10646)。按照Wiki对于Unicode和UCS的说法，目前两套字符集应该是完全相同的。同一个数字在两个字符集里所代表的字符应该是相同的。但是Unicode的外延要多一些。USC仅仅是一个字符集，而对应的Unicode同时还规范了校对、标准化、表示顺序的算法等。就如同本文提及对Unicode的定义一样，Unicode出来包括了字符集，还有表现和处理方法的部分。因此Unicode应该说是一个更加广泛含义上的标准。</p>\n<p>理论上说，Unicode字符集或者UCS有110万字符点数可以被分配，目前时机分配成16个Plane，其中Plane0，被叫做BMP（Basic Multilingual Plane），一共65536个，其中绝大部分是中文（Chinese），日文（Japanese）和韩文（Korean），三者合称CJK。</p>\n<img src=\"/2016/06/04/unicode/CJK.png\" title=\"Unicode中CJK所包含的字符浮点内容\">\n<p>这里需要特别提一下Chinese Simplified（GBK，GB18030），自从2000年以后，中国政府规定，所有在中国售卖的软件产品必须支持Chinese Simplified（GB18030）字符集。因此在我国，就出现了一个神奇的事情，就是Unicode和Chinese Simplified双字符集并行的问题。</p>\n<p>有趣的是，笔者大概调研了几大中文网站的编码如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>URI</th>\n<th>字符编码</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://www.taobao.com/\" target=\"_blank\" rel=\"noopener\">https://www.taobao.com/</a></td>\n<td>Unicode(UTF-8)</td>\n</tr>\n<tr>\n<td><a href=\"http://www.jd.com/\" target=\"_blank\" rel=\"noopener\">http://www.jd.com/</a></td>\n<td>Chinese Simplified (GBK)</td>\n</tr>\n<tr>\n<td><a href=\"http://bj.meituan.com/\" target=\"_blank\" rel=\"noopener\">http://bj.meituan.com/</a></td>\n<td>Unicode(UTF-8)</td>\n</tr>\n<tr>\n<td><a href=\"https://www.baidu.com/\" target=\"_blank\" rel=\"noopener\">https://www.baidu.com/</a></td>\n<td>Unicode(UTF-8)</td>\n</tr>\n<tr>\n<td><a href=\"http://cn.bing.com/\" target=\"_blank\" rel=\"noopener\">http://cn.bing.com/</a></td>\n<td>Unicode(UTF-8)</td>\n</tr>\n<tr>\n<td><a href=\"http://www.sohu.com/\" target=\"_blank\" rel=\"noopener\">http://www.sohu.com/</a></td>\n<td>Chinese Simplified (GBK)</td>\n</tr>\n<tr>\n<td><a href=\"http://www.qq.com/\" target=\"_blank\" rel=\"noopener\">http://www.qq.com/</a></td>\n<td>Chinese Simplified (GBK)</td>\n</tr>\n<tr>\n<td><a href=\"http://www.sina.com.cn/\" target=\"_blank\" rel=\"noopener\">http://www.sina.com.cn/</a></td>\n<td>Unicode(UTF-8)</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>感觉上各个大厂也是自说自话，不是不是很一致要遵守政府规定或者不遵守规定。这么说来，国家对这块儿在申请xxx备案的管理也不是很严格。</p>\n<p>话有点扯远了，咱们再回来。目前Unicode字符集共设定16个Plane（数字从0x000000-0x10FFFF）对应(2^16+2^20)对应（1,114,112）。刚刚说的Plane0，基础语言集定义是从0x000000-0x00FFFF(2^16)。其他的大家可以查，Plane1和2用了一些，其他基本上用的很少。因此总共来说目前分配的字符大约是12万。</p>\n<p>但是这样庞大的数字和计算机的比特（byte）之间并不统一，把一个Unicode字符串转换成Byte的过程，这里引入了一个叫做Encoding，编码的概念。1992年，为了兼容不同处理器和C语言，人们引入了一个编码标准，这就是大家广泛知道的UTF-8。截至2016年5月份，在WWW上的统计，UTF-8的使用率已经达到86.9%，对比GB2312（0.8%）。同时W3C也把UTF-8作为XML和HTML的推荐编码。</p>\n<p>下面我们来阐述一下UTF-8的实现原理（同时可以结合下图来看）：</p>\n<p>如果是7位以内表述的字符表数字，就只占用一个自己，表现为（0xxxxxxx），这样刚好和ASCII码的描述相一致，这样就不会造成原有ASCII的识别错乱，特别是针对C语言的strlen()和strcopy()。从第8位，也就是十位数的256开始，采用两字节表述模式（110xxxxx 10xxxxxx),最多可以表示11个bit位，也就是2^8(256)到2^12-1(4095)。以此类推。是不是一种很有趣的编码模式 :)</p>\n<img src=\"/2016/06/04/unicode/utf8.png\" title=\"UTF-8在Wiki上的例子截图\">\n<p>下面看一个栗子。以杏树林的“杏”字为题。</p>\n<img src=\"/2016/06/04/unicode/example.png\" title=\"杏字的uft-8编码翻译\">\n<p>所以总结一下，Unicode和ASCII是一个字符集的概念，他们是随着电信发展而产生的编码本。只不过Unicode有多包涵了表示和处理部分，范围会更广泛。为了让计算机能读懂编码，适应计算机的计算，我们有了诸如UTF-8类的编码方法。</p>\n<p>值得参考的一系列词条出处<br><a href=\"https://docs.python.org/3/howto/unicode.html\" target=\"_blank\" rel=\"noopener\">https://docs.python.org/3/howto/unicode.html</a><br><a href=\"https://en.wikipedia.org/wiki/Character_encoding\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Character_encoding</a><br><a href=\"https://en.wikipedia.org/wiki/Unicode\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Unicode</a><br><a href=\"https://en.wikipedia.org/wiki/Universal_Coded_Character_Set\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Universal_Coded_Character_Set</a><br><a href=\"https://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane</a><br><a href=\"https://en.wikipedia.org/wiki/UTF-8\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/UTF-8</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>写Python2的人，很多人都见过下面这行Error<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File &quot;/Users/Jack/Documents/ApricotForestDoc/2_product_rnd/redspiderlily/mailfetcher.py&quot;, line 35, in &lt;module&gt;</span><br><span class=\"line\">    if is_reply_mail(subject):</span><br><span class=\"line\">  File &quot;/Users/Jack/Documents/ApricotForestDoc/2_product_rnd/redspiderlily/module/mailutil.py&quot;, line 46, in is_reply_mail</span><br><span class=\"line\">    return (&quot;回复&quot; in subject_.lower()) or (&quot;re:&quot; in subject_.lower())</span><br><span class=\"line\">UnicodeDecodeError: &apos;ascii&apos; codec can&apos;t decode byte 0xe5 in position 0: ordinal not in range(128)</span><br></pre></td></tr></table></figure></p>\n<p>上一次聊到Python3升级的一个重头就是unicode编码。所以这次想重点就聊两句字符编码。说实话就是在此之前自己也有些算不清楚。所以还是深入的跑到wiki上研究了一番，分享供参考。</p>\n<p>ASCII，全称American Standard Code for Information Interchange，是一个字符编码标准。使用0-127（2^7)的数字代表不同的英文字符，这里包括大小写字母，空格，特殊字符等。这是一个大家都比较熟悉的字符集标准，可以看看下面图中的内容。这里不做赘述。</p>\n<img src=\"/2016/06/04/unicode/ascii.png\" title=\"ASCII字符表\">\n<p>但是，ASCII因为标准内含有的有限，带有音标的字符‘é’就无法很好的表述，更别提汉字了。到了1980年代，计算机技术发展，那时候大家已经开始使用字节（byte）来作为计算机基本计数单位。1byte＝8bit（2^8=256）,所以可以表述的字符变多了。那个时候开始出现了实用128-255这些数字表示发音单词。所以直到今天，你去看word里面的字符表，Latin依然可以看到这个顺序关系，大体就是这个原因。</p>\n<img src=\"/2016/06/04/unicode/character_in_word.png\" title=\"MS Word的英文字符集\">\n<p>再之后，当世界各地的语言发展出了各自不同的字符集体系，比如中国简体（GBK，GB18030），中文繁体（Big5，以前有个特别扯的名字叫做大五码），法语（Latin1），日语等等。本来各种语言字符集各自写互不干扰，倒也相安无事。但是，世界大融合嘛，于是问题来了。有人需要在中文里写上一段日语，就像这样 ++“日本語にほんご”++ 。于是问题就来了，怎么才能在同一个文件里现实不同的字符集的字符内容呢？1980年，人来开始尝试解决这个问题，并制定了一个新的计算机工业标准用以规范的处理、表示和编码全世界主要文字。这个标准叫做Unicode（全称是The Unicode Standard）。目前，Unicode标准是8.0，已经包含了全世界已经有超过12万个字符，覆盖129种现代和历史上的语言种类。在这里面需要说明一个额外的概念叫做Universal Coded Character Set (UCS，也叫做ISO 10646)。按照Wiki对于Unicode和UCS的说法，目前两套字符集应该是完全相同的。同一个数字在两个字符集里所代表的字符应该是相同的。但是Unicode的外延要多一些。USC仅仅是一个字符集，而对应的Unicode同时还规范了校对、标准化、表示顺序的算法等。就如同本文提及对Unicode的定义一样，Unicode出来包括了字符集，还有表现和处理方法的部分。因此Unicode应该说是一个更加广泛含义上的标准。</p>\n<p>理论上说，Unicode字符集或者UCS有110万字符点数可以被分配，目前时机分配成16个Plane，其中Plane0，被叫做BMP（Basic Multilingual Plane），一共65536个，其中绝大部分是中文（Chinese），日文（Japanese）和韩文（Korean），三者合称CJK。</p>\n<img src=\"/2016/06/04/unicode/CJK.png\" title=\"Unicode中CJK所包含的字符浮点内容\">\n<p>这里需要特别提一下Chinese Simplified（GBK，GB18030），自从2000年以后，中国政府规定，所有在中国售卖的软件产品必须支持Chinese Simplified（GB18030）字符集。因此在我国，就出现了一个神奇的事情，就是Unicode和Chinese Simplified双字符集并行的问题。</p>\n<p>有趣的是，笔者大概调研了几大中文网站的编码如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>URI</th>\n<th>字符编码</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://www.taobao.com/\" target=\"_blank\" rel=\"noopener\">https://www.taobao.com/</a></td>\n<td>Unicode(UTF-8)</td>\n</tr>\n<tr>\n<td><a href=\"http://www.jd.com/\" target=\"_blank\" rel=\"noopener\">http://www.jd.com/</a></td>\n<td>Chinese Simplified (GBK)</td>\n</tr>\n<tr>\n<td><a href=\"http://bj.meituan.com/\" target=\"_blank\" rel=\"noopener\">http://bj.meituan.com/</a></td>\n<td>Unicode(UTF-8)</td>\n</tr>\n<tr>\n<td><a href=\"https://www.baidu.com/\" target=\"_blank\" rel=\"noopener\">https://www.baidu.com/</a></td>\n<td>Unicode(UTF-8)</td>\n</tr>\n<tr>\n<td><a href=\"http://cn.bing.com/\" target=\"_blank\" rel=\"noopener\">http://cn.bing.com/</a></td>\n<td>Unicode(UTF-8)</td>\n</tr>\n<tr>\n<td><a href=\"http://www.sohu.com/\" target=\"_blank\" rel=\"noopener\">http://www.sohu.com/</a></td>\n<td>Chinese Simplified (GBK)</td>\n</tr>\n<tr>\n<td><a href=\"http://www.qq.com/\" target=\"_blank\" rel=\"noopener\">http://www.qq.com/</a></td>\n<td>Chinese Simplified (GBK)</td>\n</tr>\n<tr>\n<td><a href=\"http://www.sina.com.cn/\" target=\"_blank\" rel=\"noopener\">http://www.sina.com.cn/</a></td>\n<td>Unicode(UTF-8)</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>感觉上各个大厂也是自说自话，不是不是很一致要遵守政府规定或者不遵守规定。这么说来，国家对这块儿在申请xxx备案的管理也不是很严格。</p>\n<p>话有点扯远了，咱们再回来。目前Unicode字符集共设定16个Plane（数字从0x000000-0x10FFFF）对应(2^16+2^20)对应（1,114,112）。刚刚说的Plane0，基础语言集定义是从0x000000-0x00FFFF(2^16)。其他的大家可以查，Plane1和2用了一些，其他基本上用的很少。因此总共来说目前分配的字符大约是12万。</p>\n<p>但是这样庞大的数字和计算机的比特（byte）之间并不统一，把一个Unicode字符串转换成Byte的过程，这里引入了一个叫做Encoding，编码的概念。1992年，为了兼容不同处理器和C语言，人们引入了一个编码标准，这就是大家广泛知道的UTF-8。截至2016年5月份，在WWW上的统计，UTF-8的使用率已经达到86.9%，对比GB2312（0.8%）。同时W3C也把UTF-8作为XML和HTML的推荐编码。</p>\n<p>下面我们来阐述一下UTF-8的实现原理（同时可以结合下图来看）：</p>\n<p>如果是7位以内表述的字符表数字，就只占用一个自己，表现为（0xxxxxxx），这样刚好和ASCII码的描述相一致，这样就不会造成原有ASCII的识别错乱，特别是针对C语言的strlen()和strcopy()。从第8位，也就是十位数的256开始，采用两字节表述模式（110xxxxx 10xxxxxx),最多可以表示11个bit位，也就是2^8(256)到2^12-1(4095)。以此类推。是不是一种很有趣的编码模式 :)</p>\n<img src=\"/2016/06/04/unicode/utf8.png\" title=\"UTF-8在Wiki上的例子截图\">\n<p>下面看一个栗子。以杏树林的“杏”字为题。</p>\n<img src=\"/2016/06/04/unicode/example.png\" title=\"杏字的uft-8编码翻译\">\n<p>所以总结一下，Unicode和ASCII是一个字符集的概念，他们是随着电信发展而产生的编码本。只不过Unicode有多包涵了表示和处理部分，范围会更广泛。为了让计算机能读懂编码，适应计算机的计算，我们有了诸如UTF-8类的编码方法。</p>\n<p>值得参考的一系列词条出处<br><a href=\"https://docs.python.org/3/howto/unicode.html\" target=\"_blank\" rel=\"noopener\">https://docs.python.org/3/howto/unicode.html</a><br><a href=\"https://en.wikipedia.org/wiki/Character_encoding\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Character_encoding</a><br><a href=\"https://en.wikipedia.org/wiki/Unicode\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Unicode</a><br><a href=\"https://en.wikipedia.org/wiki/Universal_Coded_Character_Set\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Universal_Coded_Character_Set</a><br><a href=\"https://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane</a><br><a href=\"https://en.wikipedia.org/wiki/UTF-8\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/UTF-8</a></p>\n"},{"title":"品味苹果的盛宴，移动医疗能尝到什么？","date":"2015-07-03T03:40:28.000Z","_content":"WWDC，全称“苹果世界开发者大会”，是苹果公司面向全球开发者的一场盛会。今年的WWDC’15，吸引了来自全球5000人参与，为期一周时间。大会从开发者功能出发，讨论在多媒体、图像与游戏处理、系统架构、应用架构、设计和部署五方面内容。不得不说，每年6月的WWDC，是开发者的一次饕餮盛宴，也是苹果展现最新技术的巨大舞台。\n\n本年WWDC’15的特别之处在于，苹果在健康领域的两大功能再次被提及——就是它的HealthKit和ResearchKit。那么对于移动医疗而言，我们能做点什么？苹果创造了新的机遇还是毁灭了什么？通过Keynote，HealthKit，ResearchKit以及Watch的几个session，笔者做了如下的一些小结：\n\nKeynote－智能再生:\n\n从2010年开始看WWDC的感觉，笔者认为本届最大的亮点是——智能。无论是Craig在OSX上高调的搜索“上周处理的文档”，还是Siri的日程提醒，一直到后来的苹果音乐推出的个性化推荐，都在这一点上给人比较深刻的印象。那么对于医疗健康而言呢？IBM在Watson上的“失败”并不是一无是处，至少它证明了在医疗领域计算机不能替代人类完成系统性决策任务，但是可以很好的给人帮助辅助决策。在笔者的走访中，关于计算智能化，目前主要还是海外的一些公司在做，比如FlatironHealth，被谷歌投资致力于癌症数据标准化与数据分析，实现辅助癌症诊断。当然，国内互联网公司许多也在从事这样的事情，但是目前成果还非常有限。以2014年的中国互联网融资报告中所提及的数据为例，在所有的不完全统计中，总数达14亿美金的医疗投资中，仅有一家是大数据分析的公司。当然这并不是说其他类别的公司不做智能，但是根据笔者所闻，确实很少。更多的公司还只是玩玩概念，说说而已。\n\n\nHealthKit与ResearchKit－更加完善的数据和医疗研究方法论：\n\n关于本次WWDC，苹果在HealthKit和ResearchKit两个演讲可谓可圈可点。作为两个以医疗为突破点的iOS生态模块，前者更加偏向大众健康管理部分，后者偏向医疗研究。\n\n关于大众健康部分，iOS希望打造一整套与个人健康信息相关的数据服务系统，从各个层面为人们展现出更丰富的数据资料。过去的HealthKit数据指标项涵盖了身体量度，健身，医学指标，营养睡眠，核心生命体征五大模块。本次除了饮水量，UV紫外线外，还增加了一个重要的生殖模块，重点关注女性健康方面，包括基础体温、白带、排卵等。这确实又是一个大领域，在中国，过去“大姨吗”，“美柚”都算在此耕耘过。可见以数据类型（数据指标项）为基础，HealthKit通过数据链描述人的方面做得越来越趋近与完整。\n\n\n\n不过，HealthKit的发展对于医疗互联网从业者来说并不十分有利。其中的重要原因在于，苹果对HealthKit的使用场景做了严格的限制。尤其在数据的使用上，iOS系统基本上就是将数据占为己有。这对以数据为中心的应用而言完全是不可接受的。让很多应用服务的发展少了不小的想象空间。可以说，没有了数据，上文中提到的智能化就很难建立。当然换个角度说，这也是互联网向物联网转型的一个发展模式。HealthKit最大的收益者莫过于智能硬件类生产厂商，无需再花费繁重的软件研发费用，把自己工作更多的集中在硬件制造领域，创造更加符合人使用习惯的外部设备。其实也不光是苹果的iOS，医疗作为全世界最大的资本市场，每年的硬件成本是相当高昂的。软硬结合的发展，让这个领域的公司有了更多合作与互补。例如，在心脏病领域，心电的实时监控已经是被广泛认定合理的诊断方法。将数据合理、快速的回传到医生的信息端，会大大方便医生的诊疗工作。\n\n关于医疗研究部分，笔者深以为，这是移动互联完为医疗创造的最大价值之一。将医疗研究，从有限的人数，拓展到了互联网更加广泛的人群；将以问卷为中心的主观描述，变为可以持续收集的客观数据；使参加者得到更多反馈。这几句总结很精妙的描述了移动医疗的价值所在。举个随访的例子，很多人都觉得随访就是聊天，是医生和患者间的即时通讯。其实不然，随访是医生对患者进行跟踪诊断的最主要手段。医生通过对患者的不断随访，通过数据指标，调整用药和药量。其重要作用是明确写在医疗教科书里的。那么移动医疗要解决的，当然不是怎么让医患聊天的问题，而是如何让医生以最有效的方式，帮助患者进行诊疗调整。这其实也是一种研究。苹果的iOS系统确实在这方面做了很系统化的努力，包括主动式活动（如问卷调查），被动式活动（传感器采集数据），知情同意书，展示面板。这方面非常值得同业移动医疗人学习的。同时，苹果也在“医药与软件合作”、“表单处理”、“数据安全”、“参与者权利”四个方向提出了相当明确的参考意见。\n\nWatch－依然值得期待的玩具\n\n关于AppleWatch，其实是本届WWDC’15的重头戏，在所有267个演讲中，17个明确以Watch为主讲内容，还有若干把Watch作为重点工作进行描述，这对于以讲代码研发为主的WWDC而言是非常少见的。本次苹果在手表方面大幅增加其自身的运算能力，新版本的WatchOS也终于摆脱了对iPhone的完全从属地位。这给了软件研发着更多的空间。目前绝大多数关于Watch的研发，虽然许多在说医疗健康领域的话题，但大体还是集中在一些和个人健康指标相关的收集与展示上，没有太大突破。笔者认为，这主要与AppleWatch目前的传感器数量和计算能力有关。纵观所有Watch类应用，无外乎提醒（日程提醒）、提醒（消息通知），还是提醒。在纯展现的大背景下，确实能思考的东西是有限。所以，在医疗健康市场上，除了数据收集，唯一能看到的产品就是用药提醒类的应用。这就回到了最开始的话题，关于智能，其实苹果已经说的很明确了，以Siri为代表的语音计算，已经开始帮助人们解决传统的输入和输出问题。那么接下来的医疗市场，围绕此而展开的智能计算，应该会成为新的热点。但至于具体是什么，笔者也在思考，同时也在等待苹果给出更好的输入和输出解决方案，甚至可能不是语音，而是生物电传感或其他什么，谷歌不是很好的解决了听骨传音这个问题么，也许下一个创新就在这里。但不管怎么说，目前的手表市场还依然算是玩具，我们都相信这里巨大的商业价值，但还需踏踏实实的创新脚步。\n\n苹果的盛宴还在继续，每年的这个月总是会让人感觉很兴奋。希望能从中获取到一些灵感。技术在一次次颠覆着我们的思考模式。在使用全新的方法，去完成过去耗费庞大人力物力才能完成的事情。而如果认为所谓全新的方式，是把一些过去的东西电子化，以及搬移到移动设备上，那便是大错特错了。无论是Keynote诠释的智能化计算，HealthKit描述的关于医疗健康指标的定义，还是ResearchKit里对于数据采集收集方法的突破创新，抑或是AppleWatch这个有可能颠覆人类“文字依赖”的革命，所有的一切都在告诉我们，移动医疗改变的必然对传统“计算”、“沟通方式”、“输入”和“输出”方面的革新。而这四个方面，恰恰就是每一个从业人可以思考的。\n\n王哲，2015-6-20\n\n欢迎关注我的微信\n","source":"_posts/wwdc.md","raw":"title: 品味苹果的盛宴，移动医疗能尝到什么？\ndate: 2015-07-03 11:40:28\ncategories:\n- Business Strategy\ntags:\n- tech\n- med\n---\nWWDC，全称“苹果世界开发者大会”，是苹果公司面向全球开发者的一场盛会。今年的WWDC’15，吸引了来自全球5000人参与，为期一周时间。大会从开发者功能出发，讨论在多媒体、图像与游戏处理、系统架构、应用架构、设计和部署五方面内容。不得不说，每年6月的WWDC，是开发者的一次饕餮盛宴，也是苹果展现最新技术的巨大舞台。\n\n本年WWDC’15的特别之处在于，苹果在健康领域的两大功能再次被提及——就是它的HealthKit和ResearchKit。那么对于移动医疗而言，我们能做点什么？苹果创造了新的机遇还是毁灭了什么？通过Keynote，HealthKit，ResearchKit以及Watch的几个session，笔者做了如下的一些小结：\n\nKeynote－智能再生:\n\n从2010年开始看WWDC的感觉，笔者认为本届最大的亮点是——智能。无论是Craig在OSX上高调的搜索“上周处理的文档”，还是Siri的日程提醒，一直到后来的苹果音乐推出的个性化推荐，都在这一点上给人比较深刻的印象。那么对于医疗健康而言呢？IBM在Watson上的“失败”并不是一无是处，至少它证明了在医疗领域计算机不能替代人类完成系统性决策任务，但是可以很好的给人帮助辅助决策。在笔者的走访中，关于计算智能化，目前主要还是海外的一些公司在做，比如FlatironHealth，被谷歌投资致力于癌症数据标准化与数据分析，实现辅助癌症诊断。当然，国内互联网公司许多也在从事这样的事情，但是目前成果还非常有限。以2014年的中国互联网融资报告中所提及的数据为例，在所有的不完全统计中，总数达14亿美金的医疗投资中，仅有一家是大数据分析的公司。当然这并不是说其他类别的公司不做智能，但是根据笔者所闻，确实很少。更多的公司还只是玩玩概念，说说而已。\n\n\nHealthKit与ResearchKit－更加完善的数据和医疗研究方法论：\n\n关于本次WWDC，苹果在HealthKit和ResearchKit两个演讲可谓可圈可点。作为两个以医疗为突破点的iOS生态模块，前者更加偏向大众健康管理部分，后者偏向医疗研究。\n\n关于大众健康部分，iOS希望打造一整套与个人健康信息相关的数据服务系统，从各个层面为人们展现出更丰富的数据资料。过去的HealthKit数据指标项涵盖了身体量度，健身，医学指标，营养睡眠，核心生命体征五大模块。本次除了饮水量，UV紫外线外，还增加了一个重要的生殖模块，重点关注女性健康方面，包括基础体温、白带、排卵等。这确实又是一个大领域，在中国，过去“大姨吗”，“美柚”都算在此耕耘过。可见以数据类型（数据指标项）为基础，HealthKit通过数据链描述人的方面做得越来越趋近与完整。\n\n\n\n不过，HealthKit的发展对于医疗互联网从业者来说并不十分有利。其中的重要原因在于，苹果对HealthKit的使用场景做了严格的限制。尤其在数据的使用上，iOS系统基本上就是将数据占为己有。这对以数据为中心的应用而言完全是不可接受的。让很多应用服务的发展少了不小的想象空间。可以说，没有了数据，上文中提到的智能化就很难建立。当然换个角度说，这也是互联网向物联网转型的一个发展模式。HealthKit最大的收益者莫过于智能硬件类生产厂商，无需再花费繁重的软件研发费用，把自己工作更多的集中在硬件制造领域，创造更加符合人使用习惯的外部设备。其实也不光是苹果的iOS，医疗作为全世界最大的资本市场，每年的硬件成本是相当高昂的。软硬结合的发展，让这个领域的公司有了更多合作与互补。例如，在心脏病领域，心电的实时监控已经是被广泛认定合理的诊断方法。将数据合理、快速的回传到医生的信息端，会大大方便医生的诊疗工作。\n\n关于医疗研究部分，笔者深以为，这是移动互联完为医疗创造的最大价值之一。将医疗研究，从有限的人数，拓展到了互联网更加广泛的人群；将以问卷为中心的主观描述，变为可以持续收集的客观数据；使参加者得到更多反馈。这几句总结很精妙的描述了移动医疗的价值所在。举个随访的例子，很多人都觉得随访就是聊天，是医生和患者间的即时通讯。其实不然，随访是医生对患者进行跟踪诊断的最主要手段。医生通过对患者的不断随访，通过数据指标，调整用药和药量。其重要作用是明确写在医疗教科书里的。那么移动医疗要解决的，当然不是怎么让医患聊天的问题，而是如何让医生以最有效的方式，帮助患者进行诊疗调整。这其实也是一种研究。苹果的iOS系统确实在这方面做了很系统化的努力，包括主动式活动（如问卷调查），被动式活动（传感器采集数据），知情同意书，展示面板。这方面非常值得同业移动医疗人学习的。同时，苹果也在“医药与软件合作”、“表单处理”、“数据安全”、“参与者权利”四个方向提出了相当明确的参考意见。\n\nWatch－依然值得期待的玩具\n\n关于AppleWatch，其实是本届WWDC’15的重头戏，在所有267个演讲中，17个明确以Watch为主讲内容，还有若干把Watch作为重点工作进行描述，这对于以讲代码研发为主的WWDC而言是非常少见的。本次苹果在手表方面大幅增加其自身的运算能力，新版本的WatchOS也终于摆脱了对iPhone的完全从属地位。这给了软件研发着更多的空间。目前绝大多数关于Watch的研发，虽然许多在说医疗健康领域的话题，但大体还是集中在一些和个人健康指标相关的收集与展示上，没有太大突破。笔者认为，这主要与AppleWatch目前的传感器数量和计算能力有关。纵观所有Watch类应用，无外乎提醒（日程提醒）、提醒（消息通知），还是提醒。在纯展现的大背景下，确实能思考的东西是有限。所以，在医疗健康市场上，除了数据收集，唯一能看到的产品就是用药提醒类的应用。这就回到了最开始的话题，关于智能，其实苹果已经说的很明确了，以Siri为代表的语音计算，已经开始帮助人们解决传统的输入和输出问题。那么接下来的医疗市场，围绕此而展开的智能计算，应该会成为新的热点。但至于具体是什么，笔者也在思考，同时也在等待苹果给出更好的输入和输出解决方案，甚至可能不是语音，而是生物电传感或其他什么，谷歌不是很好的解决了听骨传音这个问题么，也许下一个创新就在这里。但不管怎么说，目前的手表市场还依然算是玩具，我们都相信这里巨大的商业价值，但还需踏踏实实的创新脚步。\n\n苹果的盛宴还在继续，每年的这个月总是会让人感觉很兴奋。希望能从中获取到一些灵感。技术在一次次颠覆着我们的思考模式。在使用全新的方法，去完成过去耗费庞大人力物力才能完成的事情。而如果认为所谓全新的方式，是把一些过去的东西电子化，以及搬移到移动设备上，那便是大错特错了。无论是Keynote诠释的智能化计算，HealthKit描述的关于医疗健康指标的定义，还是ResearchKit里对于数据采集收集方法的突破创新，抑或是AppleWatch这个有可能颠覆人类“文字依赖”的革命，所有的一切都在告诉我们，移动医疗改变的必然对传统“计算”、“沟通方式”、“输入”和“输出”方面的革新。而这四个方面，恰恰就是每一个从业人可以思考的。\n\n王哲，2015-6-20\n\n欢迎关注我的微信\n","slug":"wwdc","published":1,"updated":"2018-04-16T09:59:37.074Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcp50017rlfy8jxau9o9","content":"<p>WWDC，全称“苹果世界开发者大会”，是苹果公司面向全球开发者的一场盛会。今年的WWDC’15，吸引了来自全球5000人参与，为期一周时间。大会从开发者功能出发，讨论在多媒体、图像与游戏处理、系统架构、应用架构、设计和部署五方面内容。不得不说，每年6月的WWDC，是开发者的一次饕餮盛宴，也是苹果展现最新技术的巨大舞台。</p>\n<p>本年WWDC’15的特别之处在于，苹果在健康领域的两大功能再次被提及——就是它的HealthKit和ResearchKit。那么对于移动医疗而言，我们能做点什么？苹果创造了新的机遇还是毁灭了什么？通过Keynote，HealthKit，ResearchKit以及Watch的几个session，笔者做了如下的一些小结：</p>\n<p>Keynote－智能再生:</p>\n<p>从2010年开始看WWDC的感觉，笔者认为本届最大的亮点是——智能。无论是Craig在OSX上高调的搜索“上周处理的文档”，还是Siri的日程提醒，一直到后来的苹果音乐推出的个性化推荐，都在这一点上给人比较深刻的印象。那么对于医疗健康而言呢？IBM在Watson上的“失败”并不是一无是处，至少它证明了在医疗领域计算机不能替代人类完成系统性决策任务，但是可以很好的给人帮助辅助决策。在笔者的走访中，关于计算智能化，目前主要还是海外的一些公司在做，比如FlatironHealth，被谷歌投资致力于癌症数据标准化与数据分析，实现辅助癌症诊断。当然，国内互联网公司许多也在从事这样的事情，但是目前成果还非常有限。以2014年的中国互联网融资报告中所提及的数据为例，在所有的不完全统计中，总数达14亿美金的医疗投资中，仅有一家是大数据分析的公司。当然这并不是说其他类别的公司不做智能，但是根据笔者所闻，确实很少。更多的公司还只是玩玩概念，说说而已。</p>\n<p>HealthKit与ResearchKit－更加完善的数据和医疗研究方法论：</p>\n<p>关于本次WWDC，苹果在HealthKit和ResearchKit两个演讲可谓可圈可点。作为两个以医疗为突破点的iOS生态模块，前者更加偏向大众健康管理部分，后者偏向医疗研究。</p>\n<p>关于大众健康部分，iOS希望打造一整套与个人健康信息相关的数据服务系统，从各个层面为人们展现出更丰富的数据资料。过去的HealthKit数据指标项涵盖了身体量度，健身，医学指标，营养睡眠，核心生命体征五大模块。本次除了饮水量，UV紫外线外，还增加了一个重要的生殖模块，重点关注女性健康方面，包括基础体温、白带、排卵等。这确实又是一个大领域，在中国，过去“大姨吗”，“美柚”都算在此耕耘过。可见以数据类型（数据指标项）为基础，HealthKit通过数据链描述人的方面做得越来越趋近与完整。</p>\n<p>不过，HealthKit的发展对于医疗互联网从业者来说并不十分有利。其中的重要原因在于，苹果对HealthKit的使用场景做了严格的限制。尤其在数据的使用上，iOS系统基本上就是将数据占为己有。这对以数据为中心的应用而言完全是不可接受的。让很多应用服务的发展少了不小的想象空间。可以说，没有了数据，上文中提到的智能化就很难建立。当然换个角度说，这也是互联网向物联网转型的一个发展模式。HealthKit最大的收益者莫过于智能硬件类生产厂商，无需再花费繁重的软件研发费用，把自己工作更多的集中在硬件制造领域，创造更加符合人使用习惯的外部设备。其实也不光是苹果的iOS，医疗作为全世界最大的资本市场，每年的硬件成本是相当高昂的。软硬结合的发展，让这个领域的公司有了更多合作与互补。例如，在心脏病领域，心电的实时监控已经是被广泛认定合理的诊断方法。将数据合理、快速的回传到医生的信息端，会大大方便医生的诊疗工作。</p>\n<p>关于医疗研究部分，笔者深以为，这是移动互联完为医疗创造的最大价值之一。将医疗研究，从有限的人数，拓展到了互联网更加广泛的人群；将以问卷为中心的主观描述，变为可以持续收集的客观数据；使参加者得到更多反馈。这几句总结很精妙的描述了移动医疗的价值所在。举个随访的例子，很多人都觉得随访就是聊天，是医生和患者间的即时通讯。其实不然，随访是医生对患者进行跟踪诊断的最主要手段。医生通过对患者的不断随访，通过数据指标，调整用药和药量。其重要作用是明确写在医疗教科书里的。那么移动医疗要解决的，当然不是怎么让医患聊天的问题，而是如何让医生以最有效的方式，帮助患者进行诊疗调整。这其实也是一种研究。苹果的iOS系统确实在这方面做了很系统化的努力，包括主动式活动（如问卷调查），被动式活动（传感器采集数据），知情同意书，展示面板。这方面非常值得同业移动医疗人学习的。同时，苹果也在“医药与软件合作”、“表单处理”、“数据安全”、“参与者权利”四个方向提出了相当明确的参考意见。</p>\n<p>Watch－依然值得期待的玩具</p>\n<p>关于AppleWatch，其实是本届WWDC’15的重头戏，在所有267个演讲中，17个明确以Watch为主讲内容，还有若干把Watch作为重点工作进行描述，这对于以讲代码研发为主的WWDC而言是非常少见的。本次苹果在手表方面大幅增加其自身的运算能力，新版本的WatchOS也终于摆脱了对iPhone的完全从属地位。这给了软件研发着更多的空间。目前绝大多数关于Watch的研发，虽然许多在说医疗健康领域的话题，但大体还是集中在一些和个人健康指标相关的收集与展示上，没有太大突破。笔者认为，这主要与AppleWatch目前的传感器数量和计算能力有关。纵观所有Watch类应用，无外乎提醒（日程提醒）、提醒（消息通知），还是提醒。在纯展现的大背景下，确实能思考的东西是有限。所以，在医疗健康市场上，除了数据收集，唯一能看到的产品就是用药提醒类的应用。这就回到了最开始的话题，关于智能，其实苹果已经说的很明确了，以Siri为代表的语音计算，已经开始帮助人们解决传统的输入和输出问题。那么接下来的医疗市场，围绕此而展开的智能计算，应该会成为新的热点。但至于具体是什么，笔者也在思考，同时也在等待苹果给出更好的输入和输出解决方案，甚至可能不是语音，而是生物电传感或其他什么，谷歌不是很好的解决了听骨传音这个问题么，也许下一个创新就在这里。但不管怎么说，目前的手表市场还依然算是玩具，我们都相信这里巨大的商业价值，但还需踏踏实实的创新脚步。</p>\n<p>苹果的盛宴还在继续，每年的这个月总是会让人感觉很兴奋。希望能从中获取到一些灵感。技术在一次次颠覆着我们的思考模式。在使用全新的方法，去完成过去耗费庞大人力物力才能完成的事情。而如果认为所谓全新的方式，是把一些过去的东西电子化，以及搬移到移动设备上，那便是大错特错了。无论是Keynote诠释的智能化计算，HealthKit描述的关于医疗健康指标的定义，还是ResearchKit里对于数据采集收集方法的突破创新，抑或是AppleWatch这个有可能颠覆人类“文字依赖”的革命，所有的一切都在告诉我们，移动医疗改变的必然对传统“计算”、“沟通方式”、“输入”和“输出”方面的革新。而这四个方面，恰恰就是每一个从业人可以思考的。</p>\n<p>王哲，2015-6-20</p>\n<p>欢迎关注我的微信</p>\n","site":{"data":{}},"excerpt":"","more":"<p>WWDC，全称“苹果世界开发者大会”，是苹果公司面向全球开发者的一场盛会。今年的WWDC’15，吸引了来自全球5000人参与，为期一周时间。大会从开发者功能出发，讨论在多媒体、图像与游戏处理、系统架构、应用架构、设计和部署五方面内容。不得不说，每年6月的WWDC，是开发者的一次饕餮盛宴，也是苹果展现最新技术的巨大舞台。</p>\n<p>本年WWDC’15的特别之处在于，苹果在健康领域的两大功能再次被提及——就是它的HealthKit和ResearchKit。那么对于移动医疗而言，我们能做点什么？苹果创造了新的机遇还是毁灭了什么？通过Keynote，HealthKit，ResearchKit以及Watch的几个session，笔者做了如下的一些小结：</p>\n<p>Keynote－智能再生:</p>\n<p>从2010年开始看WWDC的感觉，笔者认为本届最大的亮点是——智能。无论是Craig在OSX上高调的搜索“上周处理的文档”，还是Siri的日程提醒，一直到后来的苹果音乐推出的个性化推荐，都在这一点上给人比较深刻的印象。那么对于医疗健康而言呢？IBM在Watson上的“失败”并不是一无是处，至少它证明了在医疗领域计算机不能替代人类完成系统性决策任务，但是可以很好的给人帮助辅助决策。在笔者的走访中，关于计算智能化，目前主要还是海外的一些公司在做，比如FlatironHealth，被谷歌投资致力于癌症数据标准化与数据分析，实现辅助癌症诊断。当然，国内互联网公司许多也在从事这样的事情，但是目前成果还非常有限。以2014年的中国互联网融资报告中所提及的数据为例，在所有的不完全统计中，总数达14亿美金的医疗投资中，仅有一家是大数据分析的公司。当然这并不是说其他类别的公司不做智能，但是根据笔者所闻，确实很少。更多的公司还只是玩玩概念，说说而已。</p>\n<p>HealthKit与ResearchKit－更加完善的数据和医疗研究方法论：</p>\n<p>关于本次WWDC，苹果在HealthKit和ResearchKit两个演讲可谓可圈可点。作为两个以医疗为突破点的iOS生态模块，前者更加偏向大众健康管理部分，后者偏向医疗研究。</p>\n<p>关于大众健康部分，iOS希望打造一整套与个人健康信息相关的数据服务系统，从各个层面为人们展现出更丰富的数据资料。过去的HealthKit数据指标项涵盖了身体量度，健身，医学指标，营养睡眠，核心生命体征五大模块。本次除了饮水量，UV紫外线外，还增加了一个重要的生殖模块，重点关注女性健康方面，包括基础体温、白带、排卵等。这确实又是一个大领域，在中国，过去“大姨吗”，“美柚”都算在此耕耘过。可见以数据类型（数据指标项）为基础，HealthKit通过数据链描述人的方面做得越来越趋近与完整。</p>\n<p>不过，HealthKit的发展对于医疗互联网从业者来说并不十分有利。其中的重要原因在于，苹果对HealthKit的使用场景做了严格的限制。尤其在数据的使用上，iOS系统基本上就是将数据占为己有。这对以数据为中心的应用而言完全是不可接受的。让很多应用服务的发展少了不小的想象空间。可以说，没有了数据，上文中提到的智能化就很难建立。当然换个角度说，这也是互联网向物联网转型的一个发展模式。HealthKit最大的收益者莫过于智能硬件类生产厂商，无需再花费繁重的软件研发费用，把自己工作更多的集中在硬件制造领域，创造更加符合人使用习惯的外部设备。其实也不光是苹果的iOS，医疗作为全世界最大的资本市场，每年的硬件成本是相当高昂的。软硬结合的发展，让这个领域的公司有了更多合作与互补。例如，在心脏病领域，心电的实时监控已经是被广泛认定合理的诊断方法。将数据合理、快速的回传到医生的信息端，会大大方便医生的诊疗工作。</p>\n<p>关于医疗研究部分，笔者深以为，这是移动互联完为医疗创造的最大价值之一。将医疗研究，从有限的人数，拓展到了互联网更加广泛的人群；将以问卷为中心的主观描述，变为可以持续收集的客观数据；使参加者得到更多反馈。这几句总结很精妙的描述了移动医疗的价值所在。举个随访的例子，很多人都觉得随访就是聊天，是医生和患者间的即时通讯。其实不然，随访是医生对患者进行跟踪诊断的最主要手段。医生通过对患者的不断随访，通过数据指标，调整用药和药量。其重要作用是明确写在医疗教科书里的。那么移动医疗要解决的，当然不是怎么让医患聊天的问题，而是如何让医生以最有效的方式，帮助患者进行诊疗调整。这其实也是一种研究。苹果的iOS系统确实在这方面做了很系统化的努力，包括主动式活动（如问卷调查），被动式活动（传感器采集数据），知情同意书，展示面板。这方面非常值得同业移动医疗人学习的。同时，苹果也在“医药与软件合作”、“表单处理”、“数据安全”、“参与者权利”四个方向提出了相当明确的参考意见。</p>\n<p>Watch－依然值得期待的玩具</p>\n<p>关于AppleWatch，其实是本届WWDC’15的重头戏，在所有267个演讲中，17个明确以Watch为主讲内容，还有若干把Watch作为重点工作进行描述，这对于以讲代码研发为主的WWDC而言是非常少见的。本次苹果在手表方面大幅增加其自身的运算能力，新版本的WatchOS也终于摆脱了对iPhone的完全从属地位。这给了软件研发着更多的空间。目前绝大多数关于Watch的研发，虽然许多在说医疗健康领域的话题，但大体还是集中在一些和个人健康指标相关的收集与展示上，没有太大突破。笔者认为，这主要与AppleWatch目前的传感器数量和计算能力有关。纵观所有Watch类应用，无外乎提醒（日程提醒）、提醒（消息通知），还是提醒。在纯展现的大背景下，确实能思考的东西是有限。所以，在医疗健康市场上，除了数据收集，唯一能看到的产品就是用药提醒类的应用。这就回到了最开始的话题，关于智能，其实苹果已经说的很明确了，以Siri为代表的语音计算，已经开始帮助人们解决传统的输入和输出问题。那么接下来的医疗市场，围绕此而展开的智能计算，应该会成为新的热点。但至于具体是什么，笔者也在思考，同时也在等待苹果给出更好的输入和输出解决方案，甚至可能不是语音，而是生物电传感或其他什么，谷歌不是很好的解决了听骨传音这个问题么，也许下一个创新就在这里。但不管怎么说，目前的手表市场还依然算是玩具，我们都相信这里巨大的商业价值，但还需踏踏实实的创新脚步。</p>\n<p>苹果的盛宴还在继续，每年的这个月总是会让人感觉很兴奋。希望能从中获取到一些灵感。技术在一次次颠覆着我们的思考模式。在使用全新的方法，去完成过去耗费庞大人力物力才能完成的事情。而如果认为所谓全新的方式，是把一些过去的东西电子化，以及搬移到移动设备上，那便是大错特错了。无论是Keynote诠释的智能化计算，HealthKit描述的关于医疗健康指标的定义，还是ResearchKit里对于数据采集收集方法的突破创新，抑或是AppleWatch这个有可能颠覆人类“文字依赖”的革命，所有的一切都在告诉我们，移动医疗改变的必然对传统“计算”、“沟通方式”、“输入”和“输出”方面的革新。而这四个方面，恰恰就是每一个从业人可以思考的。</p>\n<p>王哲，2015-6-20</p>\n<p>欢迎关注我的微信</p>\n"},{"title":"无UI系统设计之再建图灵","date":"2016-07-02T09:54:37.000Z","_content":"\n在过去的十年时间里，从微软到ThoughtWorks，再到后来做了杏树林，历经了Symbian，Windows Mobile，然后的Blackberry和Android，直到后来的iOS，WP，设计了一个又一个从前到后的系统架构，差不多经历了大半个移动时代的发展。还算完整的工程师生涯让我对用户体验和人机交互有着格外的感情。直到有一天，一场争论让我陷入了沉思。\n\n事情缘起于对后台运维系统的设计。在最开始季度初的时候，我雄心勃勃给DevOps团队定义了一个目标，让运维同学学会使用Axure作出产品设计原型，推演操作路径。这在我们这种搞移动产品设计起家的人眼里是顺理成章的事情。然而，技术专家的反对，却给了我一闷棍。后来想来，UI设计对于运维系统而言简直是极大的浪费。原因如下：\n\n+\t运维系统的核心任务，是提升运维人员的工作效率，提升运维质量（降低运维差错率）\n+\t运维系统的面向对象，熟练的操作人员（研发、运维）\n+\t运维系统的研发目的，以最快速度，解决现有问题\n\n第一个很重要的原因就是，正是因为如上所述，要快、要高效的面对熟练人员。如果是工作效率在运维系统中可以使用机器来尽量多的完成任务的话。那么“快”在运维系统的研发中就显得格外的重要。其实，当今世界上最简单的，也是最早出现的交互见面便是Terminal。在Terminal的情况下，大量的UI工作和交互工作都转化成了基本的命令，不再需要写表单，调整在不同窗口大小情况下的适配，也不用做繁琐的IE，Chrome等浏览器适配工作。这让DevOps研发更多的关注于“核心任务”，就是用机器方法提升工作效率本身，让出了核心功能外的代码降低到最大程度。\n\n{% asset_img terminal.png \"用户终端\" %}\n\n第二个原因，其实，随着后移动时代的到来，越来越多的人已经开始从2007年手机触屏所带来的一股极大的UI交互体验旋风中走了出来。渐渐的，一种更加方便简易的形式在兴起。Cortana, Google Now和Siri，人机对话。其实，早在Symbian时代，就有一家著名的公司Nuance公司，从事语音识别，而且效果已经很高了。不过微软、Google和苹果在识别的背后加入了更加强大的语音分析引擎，让识别结果更加语意化，可以基于场景来从事更多的工作。因此，如果说过去我们在交互这个领域，还在苦苦追求更加简单、易于理解和上手的人机交互的时候。新的机遇自然语言或者简易自然语言的交互，很可能会在不远的未来改变我们的生活，而这一切，一定会从技术本身开始。\n\n{% asset_img siri_cort_now.jpg \"Cortana, Google Now & Siri\" %}\n\nGithub去年发布了非常有意思的办公自动化软件Hubot，他通过添加adaptor的方法，想Hubot注入更多语言交互指令，让Hubot可以通过一系列在Chat过程中的指令模式完成操作。其实本质上说Hubot和Terminal的工作原理并没有多大不同，但是这一次，对话不再仅仅限制在Terminal上，更加无需了解很复杂的ip地址和命令。如今的交互在IM软件上，通过一些预先设定的语言类指令来完成。\n\n{% asset_img hubot.jpg \"Github Hubot\" %}\n\n前段时间，Google发布了他们的语言搜索系统，WWDC在新版的macOS上面加入了语义搜索功能。可以想见，ChatOps，ChatOffice，ChatXXX为代表的一系列人工语音交互模式，将会实现。那么如今，我们要做的，就是把业务内核准备好，剩下的就是不断探索更新的技术所带来的便捷。\n\n最后，写到这里的时候，我忽然想到了一个事情－－图灵实验。1950年，图灵发表了具有里程碑意义的论文《电脑能思考吗？》，在里面第一次提到了人工智能的概念。图灵对人工智能的描述就是，有一堵墙，墙上有一个小窗口，窗口的两侧，一边是人，一边是具有人工智能的机器。如果两方都认为对方是人的话，则实验成功。现在看起来，这样的日子真的不远了。:)\n\n\n","source":"_posts/no-back-end-sys.md","raw":"title: 无UI系统设计之再建图灵\ndate: 2016-07-02 17:54:37\ncategories:\n- Technology\ntags:\n- tech\n---\n\n在过去的十年时间里，从微软到ThoughtWorks，再到后来做了杏树林，历经了Symbian，Windows Mobile，然后的Blackberry和Android，直到后来的iOS，WP，设计了一个又一个从前到后的系统架构，差不多经历了大半个移动时代的发展。还算完整的工程师生涯让我对用户体验和人机交互有着格外的感情。直到有一天，一场争论让我陷入了沉思。\n\n事情缘起于对后台运维系统的设计。在最开始季度初的时候，我雄心勃勃给DevOps团队定义了一个目标，让运维同学学会使用Axure作出产品设计原型，推演操作路径。这在我们这种搞移动产品设计起家的人眼里是顺理成章的事情。然而，技术专家的反对，却给了我一闷棍。后来想来，UI设计对于运维系统而言简直是极大的浪费。原因如下：\n\n+\t运维系统的核心任务，是提升运维人员的工作效率，提升运维质量（降低运维差错率）\n+\t运维系统的面向对象，熟练的操作人员（研发、运维）\n+\t运维系统的研发目的，以最快速度，解决现有问题\n\n第一个很重要的原因就是，正是因为如上所述，要快、要高效的面对熟练人员。如果是工作效率在运维系统中可以使用机器来尽量多的完成任务的话。那么“快”在运维系统的研发中就显得格外的重要。其实，当今世界上最简单的，也是最早出现的交互见面便是Terminal。在Terminal的情况下，大量的UI工作和交互工作都转化成了基本的命令，不再需要写表单，调整在不同窗口大小情况下的适配，也不用做繁琐的IE，Chrome等浏览器适配工作。这让DevOps研发更多的关注于“核心任务”，就是用机器方法提升工作效率本身，让出了核心功能外的代码降低到最大程度。\n\n{% asset_img terminal.png \"用户终端\" %}\n\n第二个原因，其实，随着后移动时代的到来，越来越多的人已经开始从2007年手机触屏所带来的一股极大的UI交互体验旋风中走了出来。渐渐的，一种更加方便简易的形式在兴起。Cortana, Google Now和Siri，人机对话。其实，早在Symbian时代，就有一家著名的公司Nuance公司，从事语音识别，而且效果已经很高了。不过微软、Google和苹果在识别的背后加入了更加强大的语音分析引擎，让识别结果更加语意化，可以基于场景来从事更多的工作。因此，如果说过去我们在交互这个领域，还在苦苦追求更加简单、易于理解和上手的人机交互的时候。新的机遇自然语言或者简易自然语言的交互，很可能会在不远的未来改变我们的生活，而这一切，一定会从技术本身开始。\n\n{% asset_img siri_cort_now.jpg \"Cortana, Google Now & Siri\" %}\n\nGithub去年发布了非常有意思的办公自动化软件Hubot，他通过添加adaptor的方法，想Hubot注入更多语言交互指令，让Hubot可以通过一系列在Chat过程中的指令模式完成操作。其实本质上说Hubot和Terminal的工作原理并没有多大不同，但是这一次，对话不再仅仅限制在Terminal上，更加无需了解很复杂的ip地址和命令。如今的交互在IM软件上，通过一些预先设定的语言类指令来完成。\n\n{% asset_img hubot.jpg \"Github Hubot\" %}\n\n前段时间，Google发布了他们的语言搜索系统，WWDC在新版的macOS上面加入了语义搜索功能。可以想见，ChatOps，ChatOffice，ChatXXX为代表的一系列人工语音交互模式，将会实现。那么如今，我们要做的，就是把业务内核准备好，剩下的就是不断探索更新的技术所带来的便捷。\n\n最后，写到这里的时候，我忽然想到了一个事情－－图灵实验。1950年，图灵发表了具有里程碑意义的论文《电脑能思考吗？》，在里面第一次提到了人工智能的概念。图灵对人工智能的描述就是，有一堵墙，墙上有一个小窗口，窗口的两侧，一边是人，一边是具有人工智能的机器。如果两方都认为对方是人的话，则实验成功。现在看起来，这样的日子真的不远了。:)\n\n\n","slug":"no-back-end-sys","published":1,"updated":"2018-04-16T09:59:37.083Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcp60018rlfyrp5okrhf","content":"<p>在过去的十年时间里，从微软到ThoughtWorks，再到后来做了杏树林，历经了Symbian，Windows Mobile，然后的Blackberry和Android，直到后来的iOS，WP，设计了一个又一个从前到后的系统架构，差不多经历了大半个移动时代的发展。还算完整的工程师生涯让我对用户体验和人机交互有着格外的感情。直到有一天，一场争论让我陷入了沉思。</p>\n<p>事情缘起于对后台运维系统的设计。在最开始季度初的时候，我雄心勃勃给DevOps团队定义了一个目标，让运维同学学会使用Axure作出产品设计原型，推演操作路径。这在我们这种搞移动产品设计起家的人眼里是顺理成章的事情。然而，技术专家的反对，却给了我一闷棍。后来想来，UI设计对于运维系统而言简直是极大的浪费。原因如下：</p>\n<ul>\n<li>运维系统的核心任务，是提升运维人员的工作效率，提升运维质量（降低运维差错率）</li>\n<li>运维系统的面向对象，熟练的操作人员（研发、运维）</li>\n<li>运维系统的研发目的，以最快速度，解决现有问题</li>\n</ul>\n<p>第一个很重要的原因就是，正是因为如上所述，要快、要高效的面对熟练人员。如果是工作效率在运维系统中可以使用机器来尽量多的完成任务的话。那么“快”在运维系统的研发中就显得格外的重要。其实，当今世界上最简单的，也是最早出现的交互见面便是Terminal。在Terminal的情况下，大量的UI工作和交互工作都转化成了基本的命令，不再需要写表单，调整在不同窗口大小情况下的适配，也不用做繁琐的IE，Chrome等浏览器适配工作。这让DevOps研发更多的关注于“核心任务”，就是用机器方法提升工作效率本身，让出了核心功能外的代码降低到最大程度。</p>\n<img src=\"/2016/07/02/no-back-end-sys/terminal.png\" title=\"用户终端\">\n<p>第二个原因，其实，随着后移动时代的到来，越来越多的人已经开始从2007年手机触屏所带来的一股极大的UI交互体验旋风中走了出来。渐渐的，一种更加方便简易的形式在兴起。Cortana, Google Now和Siri，人机对话。其实，早在Symbian时代，就有一家著名的公司Nuance公司，从事语音识别，而且效果已经很高了。不过微软、Google和苹果在识别的背后加入了更加强大的语音分析引擎，让识别结果更加语意化，可以基于场景来从事更多的工作。因此，如果说过去我们在交互这个领域，还在苦苦追求更加简单、易于理解和上手的人机交互的时候。新的机遇自然语言或者简易自然语言的交互，很可能会在不远的未来改变我们的生活，而这一切，一定会从技术本身开始。</p>\n<img src=\"/2016/07/02/no-back-end-sys/siri_cort_now.jpg\" title=\"Cortana, Google Now & Siri\">\n<p>Github去年发布了非常有意思的办公自动化软件Hubot，他通过添加adaptor的方法，想Hubot注入更多语言交互指令，让Hubot可以通过一系列在Chat过程中的指令模式完成操作。其实本质上说Hubot和Terminal的工作原理并没有多大不同，但是这一次，对话不再仅仅限制在Terminal上，更加无需了解很复杂的ip地址和命令。如今的交互在IM软件上，通过一些预先设定的语言类指令来完成。</p>\n<img src=\"/2016/07/02/no-back-end-sys/hubot.jpg\" title=\"Github Hubot\">\n<p>前段时间，Google发布了他们的语言搜索系统，WWDC在新版的macOS上面加入了语义搜索功能。可以想见，ChatOps，ChatOffice，ChatXXX为代表的一系列人工语音交互模式，将会实现。那么如今，我们要做的，就是把业务内核准备好，剩下的就是不断探索更新的技术所带来的便捷。</p>\n<p>最后，写到这里的时候，我忽然想到了一个事情－－图灵实验。1950年，图灵发表了具有里程碑意义的论文《电脑能思考吗？》，在里面第一次提到了人工智能的概念。图灵对人工智能的描述就是，有一堵墙，墙上有一个小窗口，窗口的两侧，一边是人，一边是具有人工智能的机器。如果两方都认为对方是人的话，则实验成功。现在看起来，这样的日子真的不远了。:)</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在过去的十年时间里，从微软到ThoughtWorks，再到后来做了杏树林，历经了Symbian，Windows Mobile，然后的Blackberry和Android，直到后来的iOS，WP，设计了一个又一个从前到后的系统架构，差不多经历了大半个移动时代的发展。还算完整的工程师生涯让我对用户体验和人机交互有着格外的感情。直到有一天，一场争论让我陷入了沉思。</p>\n<p>事情缘起于对后台运维系统的设计。在最开始季度初的时候，我雄心勃勃给DevOps团队定义了一个目标，让运维同学学会使用Axure作出产品设计原型，推演操作路径。这在我们这种搞移动产品设计起家的人眼里是顺理成章的事情。然而，技术专家的反对，却给了我一闷棍。后来想来，UI设计对于运维系统而言简直是极大的浪费。原因如下：</p>\n<ul>\n<li>运维系统的核心任务，是提升运维人员的工作效率，提升运维质量（降低运维差错率）</li>\n<li>运维系统的面向对象，熟练的操作人员（研发、运维）</li>\n<li>运维系统的研发目的，以最快速度，解决现有问题</li>\n</ul>\n<p>第一个很重要的原因就是，正是因为如上所述，要快、要高效的面对熟练人员。如果是工作效率在运维系统中可以使用机器来尽量多的完成任务的话。那么“快”在运维系统的研发中就显得格外的重要。其实，当今世界上最简单的，也是最早出现的交互见面便是Terminal。在Terminal的情况下，大量的UI工作和交互工作都转化成了基本的命令，不再需要写表单，调整在不同窗口大小情况下的适配，也不用做繁琐的IE，Chrome等浏览器适配工作。这让DevOps研发更多的关注于“核心任务”，就是用机器方法提升工作效率本身，让出了核心功能外的代码降低到最大程度。</p>\n<img src=\"/2016/07/02/no-back-end-sys/terminal.png\" title=\"用户终端\">\n<p>第二个原因，其实，随着后移动时代的到来，越来越多的人已经开始从2007年手机触屏所带来的一股极大的UI交互体验旋风中走了出来。渐渐的，一种更加方便简易的形式在兴起。Cortana, Google Now和Siri，人机对话。其实，早在Symbian时代，就有一家著名的公司Nuance公司，从事语音识别，而且效果已经很高了。不过微软、Google和苹果在识别的背后加入了更加强大的语音分析引擎，让识别结果更加语意化，可以基于场景来从事更多的工作。因此，如果说过去我们在交互这个领域，还在苦苦追求更加简单、易于理解和上手的人机交互的时候。新的机遇自然语言或者简易自然语言的交互，很可能会在不远的未来改变我们的生活，而这一切，一定会从技术本身开始。</p>\n<img src=\"/2016/07/02/no-back-end-sys/siri_cort_now.jpg\" title=\"Cortana, Google Now & Siri\">\n<p>Github去年发布了非常有意思的办公自动化软件Hubot，他通过添加adaptor的方法，想Hubot注入更多语言交互指令，让Hubot可以通过一系列在Chat过程中的指令模式完成操作。其实本质上说Hubot和Terminal的工作原理并没有多大不同，但是这一次，对话不再仅仅限制在Terminal上，更加无需了解很复杂的ip地址和命令。如今的交互在IM软件上，通过一些预先设定的语言类指令来完成。</p>\n<img src=\"/2016/07/02/no-back-end-sys/hubot.jpg\" title=\"Github Hubot\">\n<p>前段时间，Google发布了他们的语言搜索系统，WWDC在新版的macOS上面加入了语义搜索功能。可以想见，ChatOps，ChatOffice，ChatXXX为代表的一系列人工语音交互模式，将会实现。那么如今，我们要做的，就是把业务内核准备好，剩下的就是不断探索更新的技术所带来的便捷。</p>\n<p>最后，写到这里的时候，我忽然想到了一个事情－－图灵实验。1950年，图灵发表了具有里程碑意义的论文《电脑能思考吗？》，在里面第一次提到了人工智能的概念。图灵对人工智能的描述就是，有一堵墙，墙上有一个小窗口，窗口的两侧，一边是人，一边是具有人工智能的机器。如果两方都认为对方是人的话，则实验成功。现在看起来，这样的日子真的不远了。:)</p>\n"},{"title":"代码也是一种诗","date":"2015-06-01T10:39:25.000Z","_content":"只是为了记住那些你曾经非常容易忽略的。这里也没有什么不一样，只是安静的土地，再来喧嚣 \n \n```bash\npython manage.py syncdb\npython manage.py test\npython manage.py migrate\npython manage.py runserver\npython manage.py inspectdb\npython manage.py startapp service\n``` \n","source":"_posts/some-python-cmd.md","raw":"title: 代码也是一种诗\ndate: 2015-06-01 18:39:25\ncategories:\n- Technology\ntags:\n- tech\n- python\n---\n只是为了记住那些你曾经非常容易忽略的。这里也没有什么不一样，只是安静的土地，再来喧嚣 \n \n```bash\npython manage.py syncdb\npython manage.py test\npython manage.py migrate\npython manage.py runserver\npython manage.py inspectdb\npython manage.py startapp service\n``` \n","slug":"some-python-cmd","published":1,"updated":"2018-04-16T09:59:37.049Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck3offcp70019rlfy9r24rtcl","content":"<p>只是为了记住那些你曾经非常容易忽略的。这里也没有什么不一样，只是安静的土地，再来喧嚣 </p>\n<pre><code class=\"lang-bash\">python manage.py syncdb\npython manage.py test\npython manage.py migrate\npython manage.py runserver\npython manage.py inspectdb\npython manage.py startapp service\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>只是为了记住那些你曾经非常容易忽略的。这里也没有什么不一样，只是安静的土地，再来喧嚣 </p>\n<pre><code class=\"lang-bash\">python manage.py syncdb\npython manage.py test\npython manage.py migrate\npython manage.py runserver\npython manage.py inspectdb\npython manage.py startapp service\n</code></pre>\n"},{"title":"留给2019","date":"2020-01-06T12:46:22.000Z","_content":"\n转眼一年过去了，总结一下。确实发现了不少问题。也想清楚了不少问题。花了不少时间在纠结和思考，乐观一点想，这算是一个蜕变的过程吧。\n\n##方向篇：\n\n之前也在和不少人谈，觉得这些年，做了业务，做了技术，但是自己的定位到底在哪儿呢？年轻时候，一直想着做寄一个技术的领导者，去改变世界。这之后可以说一直在沿着这个轨迹走，一个技术的BU head，两任CTO都是技术领导者，但这一年，很明显的感到瓶颈来了。这个瓶颈就在于前半句做到了，但是后半句一直没有，怎么也做不到。我到底改变了什么？在这件事情上，就很纠结。团队一直在50人上下徘徊，似乎这也成了自己一个无法逾越的坎儿。从某种意义上讲，我的公司挑选是正确的，地域的选择也没有问题。这是薪酬一路上涨的原因。也确实随之能力也在不断提升。不过忽略了业务与行业的选择问题。也就是后面一件事，去“改变”。从业务也就是公司（或者个人），到行业，自己是否应该把更多精力放到寻求改变里面呢？回到了一个老生常谈的问题，一到业务发展瓶颈期，就退缩了。在TW当时面临的是创建的业务，因为边界不清，始终得不到认可。然后在杏树林，吸取了经验教训，但其实问题并没有得到改善。刚开始做数据业务的时候，其实更多还是波哥在做，要不是他后来离开了，可能也轮不到我来主导业务。但问题存在于销售端的控制上，缺了这个部分，总觉得不踏实。后来药企这摊事儿，也是努力了好久，做了好多，但是最后因为认可问题，一拍两散。准确的说并不能说一拍两散，遇升还是在最后妥协了的。说明有些事情，力争发展是有可能。然而退缩的是自己，一方面担心搞不定。一方面对财富的增值太看重了。我觉得这可能导致了自己长期上的发展僵局。所以目前自己最重要的是对于方向的决心和信心。\n\n2020年，有两件事情是一定要做的。第一个就是作为技术领导者本身，在AI上一定要取得突破。我觉得自己选择的这个技术数据驱动，提升技术团队效率这个方向蛮有意思的，确实很喜欢。应用Cloud Native + AI技术实现真正的效率提升，代码和测试等一系列自动化编写。我觉得这个的确有可能改变世界些什么。说真的，我是笃信的。然而这个会遇上另一个矛盾，那就是作为业务领导者，因为改变技术本身似乎并不能改造社会，创造多么大的社会价值。或者，我知道机器人的引用，的确是各个公司需要的，因为他们的确在改造传统行业。可技术人员是个相对高级的工种，改造他们，作为技术领导者，他们愿意吗？当然这里也有一个分支方向，就是通过AI赋能Ops，不断寻找和提升Ops的效率，提升系统自动操作性（而不是restaurants，我不觉得restaurants的owner会为更多的功能买账，他们不是搞IT的！！！\n\n2020年，另一件事情，就是纯业务，比如医生经纪人这样的纯业务领域。很有趣，自己之前做了不少，但想想，可能和数据项目当时的状态类似。我有一个很信任我的人，很愿意和我工作的好朋友在。然而我好像也贡献不了太多的东西。哦，也许运营是我可以做的。不过最关键的医院销售端，似乎还没太搞定。这么说来，这个的确有点意思，这不就又是当年的搭配么，我（用户运营），金尹（医生），华丰（私立医院）。不过这个东西和技术差异真的蛮大的，也是一直下不了决心的原因。2020年，需要考虑考虑。最后还有一件和业务相关的事情，就是我的关于美容美发行业的技术化改造。again，这事儿因为没有初期的伙伴。可能更是八字没一撇。\n\n这两个想法，四个子想法，接下来要做个计划，留给2020年！否则肯定达不成结果！\n\n##方法论：\n\n+\t提升管理OKR方法论：对于国内还摸不着头脑的OKR来说，可能OKR的掌握自己越来越有效了。运用OKR来指导团队管理，还是很明显的。\n+\t提升质量的方法论：自己开始能够运用Tech Solution的四个基础方法，来规范技术解决方案。2020年，有望通过这一些列的技术解决方案提出技术的质量体系。我一直认为，研发的质量体系，应该是可以穷尽的。如果能够积累出一整套tech solution的方面，以后就可以不用这种bug一点一点出的笨办法，就可以把数据和案例驱动，变成最佳实践驱动，这样会大大加快速度。所以这也是2020年，有可能实现的一个重要目标\n+\t提升的纯技术管理方法论：Cloud Native：这个应该是最近要重点攻读的部分，因为感觉做了这么久的技术敏捷，一直没给自己找到个合理的原理来解释。Cloud Native因为继承了大量技术方法和管理方法。觉得应该会有点突破。\n\n##技术篇：\n\n+\t云端函数计算：2019年做了两个函数计算的项目，都有始有终了。虽然都不大，但是让自己对于函数计算有了比较好的初始实践。一个是部署在Aliyun上的团队index，一个是部署在GCP上的Jenkins每天总结和Wunderlist总结。Serverless的场景在我看来比较适合DevOps和数据团队做定期数据ETL和数据计算。至少这是目前我认为很合适的领域。至于其他的目前不打算探索。\n+\t从Code Function到Excel再到Data Studio的数据提取、计算到展现闭环打通。未来很多自动化都可以在这个基础上搞了。这回事OKR的重要基础，让OKR的数据收集工作变得相当简单。我认为这个以被写进OKR方法论。\n+\t开始对Python做了一些系统性的总结，这两年用python也比较多了，每次总是要去查，很烦。当然一方面说明还是用得少。训练太少。另一方面，觉得有可能形成一些自己的机械记忆会更好些。\n\n\n##展望篇：\n\n最后展望一下2020年的几个可能的成果\n\n###方向上要思考的事情：\n\n+\tCloud Native + AI完善的管理方法（可以自己不是大公司的管理者，或者没有被大公司管理者认可，在这方面也许可以明确一下方向）\n+\tAI帮助Ops（算了，这个方向是在没精力就算了，虽然有趣，但还是在给别人做嫁衣，最后的结果还是要依靠于业务的发展）\n+\t医生经纪人，这个先写，等等华丰\n+\t美容美发（算了，在有医生经纪人之前，暂时不考虑）\n\n###技术方法论要提高的：\n\n+\t解决质量问题，有没有可能出现最佳实践\n+\tCloud Native方法\n+\tPython进阶语法和编程的进一步总结，需要做些题\n+\t对AWS有更深入的使用，（估计是有八九没戏，虽然是公司使用的主要AWS平台，但是交集太少）\n+\t对GCP有更深入的使用，这个如果AI能做的话，就比较有可能。\n\n所以综合来说，2020年，属于自己的AI项目势在必行。这个会让自己在2020的技术上进一大步。加油吧2020！\n\n**附录本年度的主要工作结果**\n\n{% asset_img stability_1.png \"stability_1.png\" border=1 %}\n{% asset_img stability_2.png \"stability_2.png\" %}\n{% asset_img stability_3.png \"stability_3.png\" %}\n{% asset_img stability_4.png \"stability_4.png\" %}\n{% asset_img stability_4_1.png \"stability_4_1.png\" %}\n{% asset_img stability_4_2.png \"stability_4_2.png\" %}\n{% asset_img stability_4_3.png \"stability_4_3.png\" %}\n{% asset_img stability_4_4.png \"stability_4_4.png\" %}\n\n\n\n\n\n\n","source":"_posts/summary2019.md","raw":"title: 留给2019\ndate: 2020-01-06 20:46:22\ncategories:\n- Business Strategy\n- Management\ntags:\n- tech\n- mgnt\n- career\n---\n\n转眼一年过去了，总结一下。确实发现了不少问题。也想清楚了不少问题。花了不少时间在纠结和思考，乐观一点想，这算是一个蜕变的过程吧。\n\n##方向篇：\n\n之前也在和不少人谈，觉得这些年，做了业务，做了技术，但是自己的定位到底在哪儿呢？年轻时候，一直想着做寄一个技术的领导者，去改变世界。这之后可以说一直在沿着这个轨迹走，一个技术的BU head，两任CTO都是技术领导者，但这一年，很明显的感到瓶颈来了。这个瓶颈就在于前半句做到了，但是后半句一直没有，怎么也做不到。我到底改变了什么？在这件事情上，就很纠结。团队一直在50人上下徘徊，似乎这也成了自己一个无法逾越的坎儿。从某种意义上讲，我的公司挑选是正确的，地域的选择也没有问题。这是薪酬一路上涨的原因。也确实随之能力也在不断提升。不过忽略了业务与行业的选择问题。也就是后面一件事，去“改变”。从业务也就是公司（或者个人），到行业，自己是否应该把更多精力放到寻求改变里面呢？回到了一个老生常谈的问题，一到业务发展瓶颈期，就退缩了。在TW当时面临的是创建的业务，因为边界不清，始终得不到认可。然后在杏树林，吸取了经验教训，但其实问题并没有得到改善。刚开始做数据业务的时候，其实更多还是波哥在做，要不是他后来离开了，可能也轮不到我来主导业务。但问题存在于销售端的控制上，缺了这个部分，总觉得不踏实。后来药企这摊事儿，也是努力了好久，做了好多，但是最后因为认可问题，一拍两散。准确的说并不能说一拍两散，遇升还是在最后妥协了的。说明有些事情，力争发展是有可能。然而退缩的是自己，一方面担心搞不定。一方面对财富的增值太看重了。我觉得这可能导致了自己长期上的发展僵局。所以目前自己最重要的是对于方向的决心和信心。\n\n2020年，有两件事情是一定要做的。第一个就是作为技术领导者本身，在AI上一定要取得突破。我觉得自己选择的这个技术数据驱动，提升技术团队效率这个方向蛮有意思的，确实很喜欢。应用Cloud Native + AI技术实现真正的效率提升，代码和测试等一系列自动化编写。我觉得这个的确有可能改变世界些什么。说真的，我是笃信的。然而这个会遇上另一个矛盾，那就是作为业务领导者，因为改变技术本身似乎并不能改造社会，创造多么大的社会价值。或者，我知道机器人的引用，的确是各个公司需要的，因为他们的确在改造传统行业。可技术人员是个相对高级的工种，改造他们，作为技术领导者，他们愿意吗？当然这里也有一个分支方向，就是通过AI赋能Ops，不断寻找和提升Ops的效率，提升系统自动操作性（而不是restaurants，我不觉得restaurants的owner会为更多的功能买账，他们不是搞IT的！！！\n\n2020年，另一件事情，就是纯业务，比如医生经纪人这样的纯业务领域。很有趣，自己之前做了不少，但想想，可能和数据项目当时的状态类似。我有一个很信任我的人，很愿意和我工作的好朋友在。然而我好像也贡献不了太多的东西。哦，也许运营是我可以做的。不过最关键的医院销售端，似乎还没太搞定。这么说来，这个的确有点意思，这不就又是当年的搭配么，我（用户运营），金尹（医生），华丰（私立医院）。不过这个东西和技术差异真的蛮大的，也是一直下不了决心的原因。2020年，需要考虑考虑。最后还有一件和业务相关的事情，就是我的关于美容美发行业的技术化改造。again，这事儿因为没有初期的伙伴。可能更是八字没一撇。\n\n这两个想法，四个子想法，接下来要做个计划，留给2020年！否则肯定达不成结果！\n\n##方法论：\n\n+\t提升管理OKR方法论：对于国内还摸不着头脑的OKR来说，可能OKR的掌握自己越来越有效了。运用OKR来指导团队管理，还是很明显的。\n+\t提升质量的方法论：自己开始能够运用Tech Solution的四个基础方法，来规范技术解决方案。2020年，有望通过这一些列的技术解决方案提出技术的质量体系。我一直认为，研发的质量体系，应该是可以穷尽的。如果能够积累出一整套tech solution的方面，以后就可以不用这种bug一点一点出的笨办法，就可以把数据和案例驱动，变成最佳实践驱动，这样会大大加快速度。所以这也是2020年，有可能实现的一个重要目标\n+\t提升的纯技术管理方法论：Cloud Native：这个应该是最近要重点攻读的部分，因为感觉做了这么久的技术敏捷，一直没给自己找到个合理的原理来解释。Cloud Native因为继承了大量技术方法和管理方法。觉得应该会有点突破。\n\n##技术篇：\n\n+\t云端函数计算：2019年做了两个函数计算的项目，都有始有终了。虽然都不大，但是让自己对于函数计算有了比较好的初始实践。一个是部署在Aliyun上的团队index，一个是部署在GCP上的Jenkins每天总结和Wunderlist总结。Serverless的场景在我看来比较适合DevOps和数据团队做定期数据ETL和数据计算。至少这是目前我认为很合适的领域。至于其他的目前不打算探索。\n+\t从Code Function到Excel再到Data Studio的数据提取、计算到展现闭环打通。未来很多自动化都可以在这个基础上搞了。这回事OKR的重要基础，让OKR的数据收集工作变得相当简单。我认为这个以被写进OKR方法论。\n+\t开始对Python做了一些系统性的总结，这两年用python也比较多了，每次总是要去查，很烦。当然一方面说明还是用得少。训练太少。另一方面，觉得有可能形成一些自己的机械记忆会更好些。\n\n\n##展望篇：\n\n最后展望一下2020年的几个可能的成果\n\n###方向上要思考的事情：\n\n+\tCloud Native + AI完善的管理方法（可以自己不是大公司的管理者，或者没有被大公司管理者认可，在这方面也许可以明确一下方向）\n+\tAI帮助Ops（算了，这个方向是在没精力就算了，虽然有趣，但还是在给别人做嫁衣，最后的结果还是要依靠于业务的发展）\n+\t医生经纪人，这个先写，等等华丰\n+\t美容美发（算了，在有医生经纪人之前，暂时不考虑）\n\n###技术方法论要提高的：\n\n+\t解决质量问题，有没有可能出现最佳实践\n+\tCloud Native方法\n+\tPython进阶语法和编程的进一步总结，需要做些题\n+\t对AWS有更深入的使用，（估计是有八九没戏，虽然是公司使用的主要AWS平台，但是交集太少）\n+\t对GCP有更深入的使用，这个如果AI能做的话，就比较有可能。\n\n所以综合来说，2020年，属于自己的AI项目势在必行。这个会让自己在2020的技术上进一大步。加油吧2020！\n\n**附录本年度的主要工作结果**\n\n{% asset_img stability_1.png \"stability_1.png\" border=1 %}\n{% asset_img stability_2.png \"stability_2.png\" %}\n{% asset_img stability_3.png \"stability_3.png\" %}\n{% asset_img stability_4.png \"stability_4.png\" %}\n{% asset_img stability_4_1.png \"stability_4_1.png\" %}\n{% asset_img stability_4_2.png \"stability_4_2.png\" %}\n{% asset_img stability_4_3.png \"stability_4_3.png\" %}\n{% asset_img stability_4_4.png \"stability_4_4.png\" %}\n\n\n\n\n\n\n","slug":"summary2019","published":1,"updated":"2020-01-06T14:34:33.288Z","_id":"ck52fvcrb0000o3z4jxpb5pna","comments":1,"layout":"post","photos":[],"link":"","content":"<p>转眼一年过去了，总结一下。确实发现了不少问题。也想清楚了不少问题。花了不少时间在纠结和思考，乐观一点想，这算是一个蜕变的过程吧。</p>\n<h2 id=\"方向篇：\"><a href=\"#方向篇：\" class=\"headerlink\" title=\"方向篇：\"></a>方向篇：</h2><p>之前也在和不少人谈，觉得这些年，做了业务，做了技术，但是自己的定位到底在哪儿呢？年轻时候，一直想着做寄一个技术的领导者，去改变世界。这之后可以说一直在沿着这个轨迹走，一个技术的BU head，两任CTO都是技术领导者，但这一年，很明显的感到瓶颈来了。这个瓶颈就在于前半句做到了，但是后半句一直没有，怎么也做不到。我到底改变了什么？在这件事情上，就很纠结。团队一直在50人上下徘徊，似乎这也成了自己一个无法逾越的坎儿。从某种意义上讲，我的公司挑选是正确的，地域的选择也没有问题。这是薪酬一路上涨的原因。也确实随之能力也在不断提升。不过忽略了业务与行业的选择问题。也就是后面一件事，去“改变”。从业务也就是公司（或者个人），到行业，自己是否应该把更多精力放到寻求改变里面呢？回到了一个老生常谈的问题，一到业务发展瓶颈期，就退缩了。在TW当时面临的是创建的业务，因为边界不清，始终得不到认可。然后在杏树林，吸取了经验教训，但其实问题并没有得到改善。刚开始做数据业务的时候，其实更多还是波哥在做，要不是他后来离开了，可能也轮不到我来主导业务。但问题存在于销售端的控制上，缺了这个部分，总觉得不踏实。后来药企这摊事儿，也是努力了好久，做了好多，但是最后因为认可问题，一拍两散。准确的说并不能说一拍两散，遇升还是在最后妥协了的。说明有些事情，力争发展是有可能。然而退缩的是自己，一方面担心搞不定。一方面对财富的增值太看重了。我觉得这可能导致了自己长期上的发展僵局。所以目前自己最重要的是对于方向的决心和信心。</p>\n<p>2020年，有两件事情是一定要做的。第一个就是作为技术领导者本身，在AI上一定要取得突破。我觉得自己选择的这个技术数据驱动，提升技术团队效率这个方向蛮有意思的，确实很喜欢。应用Cloud Native + AI技术实现真正的效率提升，代码和测试等一系列自动化编写。我觉得这个的确有可能改变世界些什么。说真的，我是笃信的。然而这个会遇上另一个矛盾，那就是作为业务领导者，因为改变技术本身似乎并不能改造社会，创造多么大的社会价值。或者，我知道机器人的引用，的确是各个公司需要的，因为他们的确在改造传统行业。可技术人员是个相对高级的工种，改造他们，作为技术领导者，他们愿意吗？当然这里也有一个分支方向，就是通过AI赋能Ops，不断寻找和提升Ops的效率，提升系统自动操作性（而不是restaurants，我不觉得restaurants的owner会为更多的功能买账，他们不是搞IT的！！！</p>\n<p>2020年，另一件事情，就是纯业务，比如医生经纪人这样的纯业务领域。很有趣，自己之前做了不少，但想想，可能和数据项目当时的状态类似。我有一个很信任我的人，很愿意和我工作的好朋友在。然而我好像也贡献不了太多的东西。哦，也许运营是我可以做的。不过最关键的医院销售端，似乎还没太搞定。这么说来，这个的确有点意思，这不就又是当年的搭配么，我（用户运营），金尹（医生），华丰（私立医院）。不过这个东西和技术差异真的蛮大的，也是一直下不了决心的原因。2020年，需要考虑考虑。最后还有一件和业务相关的事情，就是我的关于美容美发行业的技术化改造。again，这事儿因为没有初期的伙伴。可能更是八字没一撇。</p>\n<p>这两个想法，四个子想法，接下来要做个计划，留给2020年！否则肯定达不成结果！</p>\n<h2 id=\"方法论：\"><a href=\"#方法论：\" class=\"headerlink\" title=\"方法论：\"></a>方法论：</h2><ul>\n<li>提升管理OKR方法论：对于国内还摸不着头脑的OKR来说，可能OKR的掌握自己越来越有效了。运用OKR来指导团队管理，还是很明显的。</li>\n<li>提升质量的方法论：自己开始能够运用Tech Solution的四个基础方法，来规范技术解决方案。2020年，有望通过这一些列的技术解决方案提出技术的质量体系。我一直认为，研发的质量体系，应该是可以穷尽的。如果能够积累出一整套tech solution的方面，以后就可以不用这种bug一点一点出的笨办法，就可以把数据和案例驱动，变成最佳实践驱动，这样会大大加快速度。所以这也是2020年，有可能实现的一个重要目标</li>\n<li>提升的纯技术管理方法论：Cloud Native：这个应该是最近要重点攻读的部分，因为感觉做了这么久的技术敏捷，一直没给自己找到个合理的原理来解释。Cloud Native因为继承了大量技术方法和管理方法。觉得应该会有点突破。</li>\n</ul>\n<h2 id=\"技术篇：\"><a href=\"#技术篇：\" class=\"headerlink\" title=\"技术篇：\"></a>技术篇：</h2><ul>\n<li>云端函数计算：2019年做了两个函数计算的项目，都有始有终了。虽然都不大，但是让自己对于函数计算有了比较好的初始实践。一个是部署在Aliyun上的团队index，一个是部署在GCP上的Jenkins每天总结和Wunderlist总结。Serverless的场景在我看来比较适合DevOps和数据团队做定期数据ETL和数据计算。至少这是目前我认为很合适的领域。至于其他的目前不打算探索。</li>\n<li>从Code Function到Excel再到Data Studio的数据提取、计算到展现闭环打通。未来很多自动化都可以在这个基础上搞了。这回事OKR的重要基础，让OKR的数据收集工作变得相当简单。我认为这个以被写进OKR方法论。</li>\n<li>开始对Python做了一些系统性的总结，这两年用python也比较多了，每次总是要去查，很烦。当然一方面说明还是用得少。训练太少。另一方面，觉得有可能形成一些自己的机械记忆会更好些。</li>\n</ul>\n<h2 id=\"展望篇：\"><a href=\"#展望篇：\" class=\"headerlink\" title=\"展望篇：\"></a>展望篇：</h2><p>最后展望一下2020年的几个可能的成果</p>\n<h3 id=\"方向上要思考的事情：\"><a href=\"#方向上要思考的事情：\" class=\"headerlink\" title=\"方向上要思考的事情：\"></a>方向上要思考的事情：</h3><ul>\n<li>Cloud Native + AI完善的管理方法（可以自己不是大公司的管理者，或者没有被大公司管理者认可，在这方面也许可以明确一下方向）</li>\n<li>AI帮助Ops（算了，这个方向是在没精力就算了，虽然有趣，但还是在给别人做嫁衣，最后的结果还是要依靠于业务的发展）</li>\n<li>医生经纪人，这个先写，等等华丰</li>\n<li>美容美发（算了，在有医生经纪人之前，暂时不考虑）</li>\n</ul>\n<h3 id=\"技术方法论要提高的：\"><a href=\"#技术方法论要提高的：\" class=\"headerlink\" title=\"技术方法论要提高的：\"></a>技术方法论要提高的：</h3><ul>\n<li>解决质量问题，有没有可能出现最佳实践</li>\n<li>Cloud Native方法</li>\n<li>Python进阶语法和编程的进一步总结，需要做些题</li>\n<li>对AWS有更深入的使用，（估计是有八九没戏，虽然是公司使用的主要AWS平台，但是交集太少）</li>\n<li>对GCP有更深入的使用，这个如果AI能做的话，就比较有可能。</li>\n</ul>\n<p>所以综合来说，2020年，属于自己的AI项目势在必行。这个会让自己在2020的技术上进一大步。加油吧2020！</p>\n<p><strong>附录本年度的主要工作结果</strong></p>\n<img src=\"/2020/01/06/summary2019/stability_1.png\" title=\"stability_1.png border=1\">\n<img src=\"/2020/01/06/summary2019/stability_2.png\" title=\"stability_2.png\">\n<img src=\"/2020/01/06/summary2019/stability_3.png\" title=\"stability_3.png\">\n<img src=\"/2020/01/06/summary2019/stability_4.png\" title=\"stability_4.png\">\n<img src=\"/2020/01/06/summary2019/stability_4_1.png\" title=\"stability_4_1.png\">\n<img src=\"/2020/01/06/summary2019/stability_4_2.png\" title=\"stability_4_2.png\">\n<img src=\"/2020/01/06/summary2019/stability_4_3.png\" title=\"stability_4_3.png\">\n<img src=\"/2020/01/06/summary2019/stability_4_4.png\" title=\"stability_4_4.png\">\n","site":{"data":{}},"excerpt":"","more":"<p>转眼一年过去了，总结一下。确实发现了不少问题。也想清楚了不少问题。花了不少时间在纠结和思考，乐观一点想，这算是一个蜕变的过程吧。</p>\n<h2 id=\"方向篇：\"><a href=\"#方向篇：\" class=\"headerlink\" title=\"方向篇：\"></a>方向篇：</h2><p>之前也在和不少人谈，觉得这些年，做了业务，做了技术，但是自己的定位到底在哪儿呢？年轻时候，一直想着做寄一个技术的领导者，去改变世界。这之后可以说一直在沿着这个轨迹走，一个技术的BU head，两任CTO都是技术领导者，但这一年，很明显的感到瓶颈来了。这个瓶颈就在于前半句做到了，但是后半句一直没有，怎么也做不到。我到底改变了什么？在这件事情上，就很纠结。团队一直在50人上下徘徊，似乎这也成了自己一个无法逾越的坎儿。从某种意义上讲，我的公司挑选是正确的，地域的选择也没有问题。这是薪酬一路上涨的原因。也确实随之能力也在不断提升。不过忽略了业务与行业的选择问题。也就是后面一件事，去“改变”。从业务也就是公司（或者个人），到行业，自己是否应该把更多精力放到寻求改变里面呢？回到了一个老生常谈的问题，一到业务发展瓶颈期，就退缩了。在TW当时面临的是创建的业务，因为边界不清，始终得不到认可。然后在杏树林，吸取了经验教训，但其实问题并没有得到改善。刚开始做数据业务的时候，其实更多还是波哥在做，要不是他后来离开了，可能也轮不到我来主导业务。但问题存在于销售端的控制上，缺了这个部分，总觉得不踏实。后来药企这摊事儿，也是努力了好久，做了好多，但是最后因为认可问题，一拍两散。准确的说并不能说一拍两散，遇升还是在最后妥协了的。说明有些事情，力争发展是有可能。然而退缩的是自己，一方面担心搞不定。一方面对财富的增值太看重了。我觉得这可能导致了自己长期上的发展僵局。所以目前自己最重要的是对于方向的决心和信心。</p>\n<p>2020年，有两件事情是一定要做的。第一个就是作为技术领导者本身，在AI上一定要取得突破。我觉得自己选择的这个技术数据驱动，提升技术团队效率这个方向蛮有意思的，确实很喜欢。应用Cloud Native + AI技术实现真正的效率提升，代码和测试等一系列自动化编写。我觉得这个的确有可能改变世界些什么。说真的，我是笃信的。然而这个会遇上另一个矛盾，那就是作为业务领导者，因为改变技术本身似乎并不能改造社会，创造多么大的社会价值。或者，我知道机器人的引用，的确是各个公司需要的，因为他们的确在改造传统行业。可技术人员是个相对高级的工种，改造他们，作为技术领导者，他们愿意吗？当然这里也有一个分支方向，就是通过AI赋能Ops，不断寻找和提升Ops的效率，提升系统自动操作性（而不是restaurants，我不觉得restaurants的owner会为更多的功能买账，他们不是搞IT的！！！</p>\n<p>2020年，另一件事情，就是纯业务，比如医生经纪人这样的纯业务领域。很有趣，自己之前做了不少，但想想，可能和数据项目当时的状态类似。我有一个很信任我的人，很愿意和我工作的好朋友在。然而我好像也贡献不了太多的东西。哦，也许运营是我可以做的。不过最关键的医院销售端，似乎还没太搞定。这么说来，这个的确有点意思，这不就又是当年的搭配么，我（用户运营），金尹（医生），华丰（私立医院）。不过这个东西和技术差异真的蛮大的，也是一直下不了决心的原因。2020年，需要考虑考虑。最后还有一件和业务相关的事情，就是我的关于美容美发行业的技术化改造。again，这事儿因为没有初期的伙伴。可能更是八字没一撇。</p>\n<p>这两个想法，四个子想法，接下来要做个计划，留给2020年！否则肯定达不成结果！</p>\n<h2 id=\"方法论：\"><a href=\"#方法论：\" class=\"headerlink\" title=\"方法论：\"></a>方法论：</h2><ul>\n<li>提升管理OKR方法论：对于国内还摸不着头脑的OKR来说，可能OKR的掌握自己越来越有效了。运用OKR来指导团队管理，还是很明显的。</li>\n<li>提升质量的方法论：自己开始能够运用Tech Solution的四个基础方法，来规范技术解决方案。2020年，有望通过这一些列的技术解决方案提出技术的质量体系。我一直认为，研发的质量体系，应该是可以穷尽的。如果能够积累出一整套tech solution的方面，以后就可以不用这种bug一点一点出的笨办法，就可以把数据和案例驱动，变成最佳实践驱动，这样会大大加快速度。所以这也是2020年，有可能实现的一个重要目标</li>\n<li>提升的纯技术管理方法论：Cloud Native：这个应该是最近要重点攻读的部分，因为感觉做了这么久的技术敏捷，一直没给自己找到个合理的原理来解释。Cloud Native因为继承了大量技术方法和管理方法。觉得应该会有点突破。</li>\n</ul>\n<h2 id=\"技术篇：\"><a href=\"#技术篇：\" class=\"headerlink\" title=\"技术篇：\"></a>技术篇：</h2><ul>\n<li>云端函数计算：2019年做了两个函数计算的项目，都有始有终了。虽然都不大，但是让自己对于函数计算有了比较好的初始实践。一个是部署在Aliyun上的团队index，一个是部署在GCP上的Jenkins每天总结和Wunderlist总结。Serverless的场景在我看来比较适合DevOps和数据团队做定期数据ETL和数据计算。至少这是目前我认为很合适的领域。至于其他的目前不打算探索。</li>\n<li>从Code Function到Excel再到Data Studio的数据提取、计算到展现闭环打通。未来很多自动化都可以在这个基础上搞了。这回事OKR的重要基础，让OKR的数据收集工作变得相当简单。我认为这个以被写进OKR方法论。</li>\n<li>开始对Python做了一些系统性的总结，这两年用python也比较多了，每次总是要去查，很烦。当然一方面说明还是用得少。训练太少。另一方面，觉得有可能形成一些自己的机械记忆会更好些。</li>\n</ul>\n<h2 id=\"展望篇：\"><a href=\"#展望篇：\" class=\"headerlink\" title=\"展望篇：\"></a>展望篇：</h2><p>最后展望一下2020年的几个可能的成果</p>\n<h3 id=\"方向上要思考的事情：\"><a href=\"#方向上要思考的事情：\" class=\"headerlink\" title=\"方向上要思考的事情：\"></a>方向上要思考的事情：</h3><ul>\n<li>Cloud Native + AI完善的管理方法（可以自己不是大公司的管理者，或者没有被大公司管理者认可，在这方面也许可以明确一下方向）</li>\n<li>AI帮助Ops（算了，这个方向是在没精力就算了，虽然有趣，但还是在给别人做嫁衣，最后的结果还是要依靠于业务的发展）</li>\n<li>医生经纪人，这个先写，等等华丰</li>\n<li>美容美发（算了，在有医生经纪人之前，暂时不考虑）</li>\n</ul>\n<h3 id=\"技术方法论要提高的：\"><a href=\"#技术方法论要提高的：\" class=\"headerlink\" title=\"技术方法论要提高的：\"></a>技术方法论要提高的：</h3><ul>\n<li>解决质量问题，有没有可能出现最佳实践</li>\n<li>Cloud Native方法</li>\n<li>Python进阶语法和编程的进一步总结，需要做些题</li>\n<li>对AWS有更深入的使用，（估计是有八九没戏，虽然是公司使用的主要AWS平台，但是交集太少）</li>\n<li>对GCP有更深入的使用，这个如果AI能做的话，就比较有可能。</li>\n</ul>\n<p>所以综合来说，2020年，属于自己的AI项目势在必行。这个会让自己在2020的技术上进一大步。加油吧2020！</p>\n<p><strong>附录本年度的主要工作结果</strong></p>\n<img src=\"/2020/01/06/summary2019/stability_1.png\" title=\"stability_1.png border=1\">\n<img src=\"/2020/01/06/summary2019/stability_2.png\" title=\"stability_2.png\">\n<img src=\"/2020/01/06/summary2019/stability_3.png\" title=\"stability_3.png\">\n<img src=\"/2020/01/06/summary2019/stability_4.png\" title=\"stability_4.png\">\n<img src=\"/2020/01/06/summary2019/stability_4_1.png\" title=\"stability_4_1.png\">\n<img src=\"/2020/01/06/summary2019/stability_4_2.png\" title=\"stability_4_2.png\">\n<img src=\"/2020/01/06/summary2019/stability_4_3.png\" title=\"stability_4_3.png\">\n<img src=\"/2020/01/06/summary2019/stability_4_4.png\" title=\"stability_4_4.png\">\n"}],"PostAsset":[{"_id":"source/_posts/deep-learning-ai-1/stuctured_unstructured.png","slug":"stuctured_unstructured.png","post":"ck3offco70006rlfylf0ka009","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning-ai-1/supervised_learning.png","slug":"supervised_learning.png","post":"ck3offco70006rlfylf0ka009","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-3-1/hl50.png","slug":"hl50.png","post":"ck3offcoe000crlfy2nyexxce","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片05.jpg","slug":"幻灯片05.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/iso27001/doc.png","slug":"doc.png","post":"ck3offcon000krlfylh1g08kf","modified":0,"renderable":0},{"_id":"source/_posts/about-data/data.png","slug":"data.png","post":"ck3offcnx0000rlfy7xr4m9ix","modified":0,"renderable":0},{"_id":"source/_posts/aliyun-server-less/aliyun.png","slug":"aliyun.png","post":"ck3offco20001rlfyuas4uis2","modified":0,"renderable":0},{"_id":"source/_posts/collection-n-set/russell_paradox.png","slug":"russell_paradox.png","post":"ck3offco80007rlfyaew1b5tz","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning-sequence-models-w3/error_analysis.png","slug":"error_analysis.png","post":"ck3offcoc000arlfyh5p4aan2","modified":0,"renderable":0},{"_id":"source/_posts/object-detection/startyolo.png","slug":"startyolo.png","post":"ck3offcou000prlfyantaeowz","modified":0,"renderable":0},{"_id":"source/_posts/html2ppt/test.h","slug":"test.h","post":"ck3offcoy000wrlfyp3uya4nq","modified":0,"renderable":0},{"_id":"source/_posts/python3upgrade/unicode.png","slug":"unicode.png","post":"ck3offcoz000zrlfyds494d56","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning-ai-2-1/closing.jpg","slug":"closing.jpg","post":"ck3offco90008rlfy8wrn1jx0","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning-ai-2-1/jupyter.png","slug":"jupyter.png","post":"ck3offco90008rlfy8wrn1jx0","modified":0,"renderable":0},{"_id":"source/_posts/four-things/mgnt.png","slug":"mgnt.png","post":"ck3offcoh000frlfy5uagjyb9","modified":0,"renderable":0},{"_id":"source/_posts/four-things/mgnt2.png","slug":"mgnt2.png","post":"ck3offcoh000frlfy5uagjyb9","modified":0,"renderable":0},{"_id":"source/_posts/tech-driven/ocr.png","slug":"ocr.png","post":"ck3offcp00010rlfytmtobdp2","modified":0,"renderable":0},{"_id":"source/_posts/tech-driven/report.jpg","slug":"report.jpg","post":"ck3offcp00010rlfytmtobdp2","modified":0,"renderable":0},{"_id":"source/_posts/datamodel/efficency24n48.png","slug":"efficency24n48.png","post":"ck3offco60005rlfyxds2anvi","modified":0,"renderable":0},{"_id":"source/_posts/datamodel/efficency48h.png","slug":"efficency48h.png","post":"ck3offco60005rlfyxds2anvi","modified":0,"renderable":0},{"_id":"source/_posts/datamodel/poisson_distribution.png","slug":"poisson_distribution.png","post":"ck3offco60005rlfyxds2anvi","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-3-1/hl3_4.png","slug":"hl3_4.png","post":"ck3offcoe000crlfy2nyexxce","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-3-1/hl5_20.png","slug":"hl5_20.png","post":"ck3offcoe000crlfy2nyexxce","modified":0,"renderable":0},{"_id":"source/_posts/exo/core.jpg","slug":"core.jpg","post":"ck3offcof000drlfyr6r1f9k4","modified":0,"renderable":0},{"_id":"source/_posts/exo/cover.jpeg","slug":"cover.jpeg","post":"ck3offcof000drlfyr6r1f9k4","modified":0,"renderable":0},{"_id":"source/_posts/exo/dashboard.png","slug":"dashboard.png","post":"ck3offcof000drlfyr6r1f9k4","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-course2-2-1/gradient_descent.png","slug":"gradient_descent.png","post":"ck3offcoh000grlfyinn51ze0","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-course2-2-1/mini_batch_gradient_descent.png","slug":"mini_batch_gradient_descent.png","post":"ck3offcoh000grlfyinn51ze0","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-course2-2-1/stochastic_gradient_descent.png","slug":"stochastic_gradient_descent.png","post":"ck3offcoh000grlfyinn51ze0","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning-sequence-models-w2/featurized.png","slug":"featurized.png","post":"ck3offcom000jrlfyietbw3bm","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning-sequence-models-w2/rnn.png","slug":"rnn.png","post":"ck3offcom000jrlfyietbw3bm","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning-sequence-models-w2/sentiment.png","slug":"sentiment.png","post":"ck3offcom000jrlfyietbw3bm","modified":0,"renderable":0},{"_id":"source/_posts/iso27001/doc_list.png","slug":"doc_list.png","post":"ck3offcon000krlfylh1g08kf","modified":0,"renderable":0},{"_id":"source/_posts/iso27001/list.png","slug":"list.png","post":"ck3offcon000krlfylh1g08kf","modified":0,"renderable":0},{"_id":"source/_posts/hello-world2/cover.jpg","slug":"cover.jpg","post":"ck3offcoq000orlfy6cpgno7z","modified":0,"renderable":0},{"_id":"source/_posts/hello-world2/img1.jpg","slug":"img1.jpg","post":"ck3offcoq000orlfy6cpgno7z","modified":0,"renderable":0},{"_id":"source/_posts/hello-world2/img2.jpg","slug":"img2.jpg","post":"ck3offcoq000orlfy6cpgno7z","modified":0,"renderable":0},{"_id":"source/_posts/okr-review/OKR_review.png","slug":"OKR_review.png","post":"ck3offcov000rrlfyqe0j8z40","modified":0,"renderable":0},{"_id":"source/_posts/okr-review/fortynight_review.png","slug":"fortynight_review.png","post":"ck3offcov000rrlfyqe0j8z40","modified":0,"renderable":0},{"_id":"source/_posts/okr-review/okr_timeline.png","slug":"okr_timeline.png","post":"ck3offcov000rrlfyqe0j8z40","modified":0,"renderable":0},{"_id":"source/_posts/no-back-end-sys/hubot.jpg","slug":"hubot.jpg","post":"ck3offcp60018rlfyrp5okrhf","modified":0,"renderable":0},{"_id":"source/_posts/no-back-end-sys/siri_cort_now.jpg","slug":"siri_cort_now.jpg","post":"ck3offcp60018rlfyrp5okrhf","modified":0,"renderable":0},{"_id":"source/_posts/no-back-end-sys/terminal.png","slug":"terminal.png","post":"ck3offcp60018rlfyrp5okrhf","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning-ai-1/nerual_network.png","slug":"nerual_network.png","post":"ck3offco70006rlfylf0ka009","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning-ai-1/why_takeoff.png","slug":"why_takeoff.png","post":"ck3offco70006rlfylf0ka009","modified":0,"renderable":0},{"_id":"source/_posts/unicode/CJK.png","slug":"CJK.png","post":"ck3offcp50016rlfyoa93cvwt","modified":0,"renderable":0},{"_id":"source/_posts/unicode/ascii.png","slug":"ascii.png","post":"ck3offcp50016rlfyoa93cvwt","modified":0,"renderable":0},{"_id":"source/_posts/unicode/character_in_word.png","slug":"character_in_word.png","post":"ck3offcp50016rlfyoa93cvwt","modified":0,"renderable":0},{"_id":"source/_posts/unicode/example.png","slug":"example.png","post":"ck3offcp50016rlfyoa93cvwt","modified":0,"renderable":0},{"_id":"source/_posts/unicode/utf8.png","slug":"utf8.png","post":"ck3offcp50016rlfyoa93cvwt","modified":0,"renderable":0},{"_id":"source/_posts/hippa-part1/evolution1996.jpg","slug":"evolution1996.jpg","post":"ck3offcox000vrlfy06dgucxl","modified":0,"renderable":0},{"_id":"source/_posts/hippa-part1/evolution1997.jpg","slug":"evolution1997.jpg","post":"ck3offcox000vrlfy06dgucxl","modified":0,"renderable":0},{"_id":"source/_posts/hippa-part1/evolution2009.jpg","slug":"evolution2009.jpg","post":"ck3offcox000vrlfy06dgucxl","modified":0,"renderable":0},{"_id":"source/_posts/hippa-part1/evolution2013.jpg","slug":"evolution2013.jpg","post":"ck3offcox000vrlfy06dgucxl","modified":0,"renderable":0},{"_id":"source/_posts/hippa-part1/hippa.jpg","slug":"hippa.jpg","post":"ck3offcox000vrlfy06dgucxl","modified":0,"renderable":0},{"_id":"source/_posts/hippa-part1/security.jpg","slug":"security.jpg","post":"ck3offcox000vrlfy06dgucxl","modified":0,"renderable":0},{"_id":"source/_posts/cto-summary/RnD.png","slug":"RnD.png","post":"ck3offco40003rlfy401c9ncz","modified":0,"renderable":0},{"_id":"source/_posts/cto-summary/branding.png","slug":"branding.png","post":"ck3offco40003rlfy401c9ncz","modified":0,"renderable":0},{"_id":"source/_posts/cto-summary/client.png","slug":"client.png","post":"ck3offco40003rlfy401c9ncz","modified":0,"renderable":0},{"_id":"source/_posts/cto-summary/operation.png","slug":"operation.png","post":"ck3offco40003rlfy401c9ncz","modified":0,"renderable":0},{"_id":"source/_posts/cto-summary/overall.png","slug":"overall.png","post":"ck3offco40003rlfy401c9ncz","modified":0,"renderable":0},{"_id":"source/_posts/cto-summary/strategy.png","slug":"strategy.png","post":"ck3offco40003rlfy401c9ncz","modified":0,"renderable":0},{"_id":"source/_posts/cto-summary/team.png","slug":"team.png","post":"ck3offco40003rlfy401c9ncz","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片01.jpg","slug":"幻灯片01.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片02.jpg","slug":"幻灯片02.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片03.jpg","slug":"幻灯片03.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片04.jpg","slug":"幻灯片04.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片06.jpg","slug":"幻灯片06.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片07.jpg","slug":"幻灯片07.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片08.jpg","slug":"幻灯片08.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片09.jpg","slug":"幻灯片09.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片10.jpg","slug":"幻灯片10.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片11.jpg","slug":"幻灯片11.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片12.jpg","slug":"幻灯片12.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片13.jpg","slug":"幻灯片13.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片14.jpg","slug":"幻灯片14.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片15.jpg","slug":"幻灯片15.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片16.jpg","slug":"幻灯片16.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片17.jpg","slug":"幻灯片17.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片18.jpg","slug":"幻灯片18.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片19.jpg","slug":"幻灯片19.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片20.jpg","slug":"幻灯片20.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片21.jpg","slug":"幻灯片21.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片22.jpg","slug":"幻灯片22.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片23.jpg","slug":"幻灯片23.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片24.jpg","slug":"幻灯片24.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片25.jpg","slug":"幻灯片25.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片26.jpg","slug":"幻灯片26.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/prepare-design-thinking/幻灯片27.jpg","slug":"幻灯片27.jpg","post":"ck3offcox000urlfyju6c2vyi","modified":0,"renderable":0},{"_id":"source/_posts/gcp-study/sql_network.png","slug":"sql_network.png","post":"ck3offcol000irlfy7ch67v00","modified":0,"renderable":0},{"_id":"source/_posts/summary2019/stability_4.png","slug":"stability_4.png","post":"ck52fvcrb0000o3z4jxpb5pna","modified":0,"renderable":0},{"_id":"source/_posts/summary2019/stability_2.png","slug":"stability_2.png","post":"ck52fvcrb0000o3z4jxpb5pna","modified":0,"renderable":0},{"_id":"source/_posts/summary2019/stability_1.png","slug":"stability_1.png","post":"ck52fvcrb0000o3z4jxpb5pna","modified":0,"renderable":0},{"_id":"source/_posts/summary2019/stability_3.png","slug":"stability_3.png","post":"ck52fvcrb0000o3z4jxpb5pna","modified":0,"renderable":0},{"_id":"source/_posts/summary2019/stability_4_1.png","slug":"stability_4_1.png","post":"ck52fvcrb0000o3z4jxpb5pna","modified":0,"renderable":0},{"_id":"source/_posts/summary2019/stability_4_2.png","slug":"stability_4_2.png","post":"ck52fvcrb0000o3z4jxpb5pna","modified":0,"renderable":0},{"_id":"source/_posts/summary2019/stability_4_3.png","slug":"stability_4_3.png","post":"ck52fvcrb0000o3z4jxpb5pna","modified":0,"renderable":0},{"_id":"source/_posts/summary2019/stability_4_4.png","slug":"stability_4_4.png","post":"ck52fvcrb0000o3z4jxpb5pna","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ck3offcnx0000rlfy7xr4m9ix","category_id":"ck3offcqb001arlfy70uip1z8","_id":"ck3offct1004mrlfyhismy604"},{"post_id":"ck3offco20001rlfyuas4uis2","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct2004orlfy0hbp0quj"},{"post_id":"ck3offco30002rlfyu2bqncx4","category_id":"ck3offcqb001arlfy70uip1z8","_id":"ck3offct2004qrlfyonkc9jcc"},{"post_id":"ck3offco40003rlfy401c9ncz","category_id":"ck3offcqb001arlfy70uip1z8","_id":"ck3offct3004srlfys2u366ny"},{"post_id":"ck3offco50004rlfyrn3blxpw","category_id":"ck3offcqb001arlfy70uip1z8","_id":"ck3offct3004urlfyk1zt7jz7"},{"post_id":"ck3offco60005rlfyxds2anvi","category_id":"ck3offcqb001arlfy70uip1z8","_id":"ck3offct3004wrlfyrceufnmy"},{"post_id":"ck3offco70006rlfylf0ka009","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct3004yrlfyciaxdxr2"},{"post_id":"ck3offco80007rlfyaew1b5tz","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct30050rlfyjssijb0p"},{"post_id":"ck3offco90008rlfy8wrn1jx0","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct30052rlfynq0rq033"},{"post_id":"ck3offcoa0009rlfyblfg9521","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct30054rlfy7q4w6bue"},{"post_id":"ck3offcoc000arlfyh5p4aan2","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct30056rlfyv5u6u5jd"},{"post_id":"ck3offcod000brlfyndh7s1mi","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct40058rlfyrh1ddjyu"},{"post_id":"ck3offcoe000crlfy2nyexxce","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct4005arlfyoyyxsxwz"},{"post_id":"ck3offcof000drlfyr6r1f9k4","category_id":"ck3offcqb001arlfy70uip1z8","_id":"ck3offct4005crlfyomdodrdx"},{"post_id":"ck3offcog000erlfy5zii1622","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct4005erlfy1cl91uxj"},{"post_id":"ck3offcoh000frlfy5uagjyb9","category_id":"ck3offcqb001arlfy70uip1z8","_id":"ck3offct4005grlfyas4nqad3"},{"post_id":"ck3offcoh000grlfyinn51ze0","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct4005irlfy8xz4e4a4"},{"post_id":"ck3offcoj000hrlfyo9pe8xvr","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct4005krlfyzpgiadwv"},{"post_id":"ck3offcol000irlfy7ch67v00","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct4005mrlfyj8tx70us"},{"post_id":"ck3offcom000jrlfyietbw3bm","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct5005orlfydbzod993"},{"post_id":"ck3offcon000krlfylh1g08kf","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct5005qrlfyffe3zht5"},{"post_id":"ck3offcoo000lrlfyv3ukx06r","category_id":"ck3offcr7002grlfyw5r5j0i9","_id":"ck3offct5005srlfyu0khwrwd"},{"post_id":"ck3offcop000mrlfyd7enfz6c","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct5005urlfyub1s4pkq"},{"post_id":"ck3offcop000nrlfyv36n66p4","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct6005wrlfyx8e3feqq"},{"post_id":"ck3offcoq000orlfy6cpgno7z","category_id":"ck3offcr7002grlfyw5r5j0i9","_id":"ck3offct6005yrlfyrlonztjt"},{"post_id":"ck3offcou000prlfyantaeowz","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct60060rlfyauwnqwl4"},{"post_id":"ck3offcou000qrlfy8hq4f83l","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct60062rlfyn1432xdg"},{"post_id":"ck3offcov000rrlfyqe0j8z40","category_id":"ck3offcqb001arlfy70uip1z8","_id":"ck3offct60064rlfyke13pp0w"},{"post_id":"ck3offcov000srlfy4htgkoph","category_id":"ck3offcrf002urlfyf3eo8fi1","_id":"ck3offct60066rlfyp9ezb34v"},{"post_id":"ck3offcow000trlfyeo0vtb90","category_id":"ck3offcqb001arlfy70uip1z8","_id":"ck3offct70068rlfyicm33s9m"},{"post_id":"ck3offcox000vrlfy06dgucxl","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct7006arlfygmlbaep7"},{"post_id":"ck3offcoy000wrlfyp3uya4nq","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct7006crlfy65i3kx39"},{"post_id":"ck3offcoy000xrlfyna7ldnm5","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct8006erlfyrrg6zne1"},{"post_id":"ck3offcoz000yrlfy9x3vfls3","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct8006grlfyq24wv0ki"},{"post_id":"ck3offcoz000zrlfyds494d56","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct9006irlfytky9qzkr"},{"post_id":"ck3offcp00010rlfytmtobdp2","category_id":"ck3offcrl0038rlfy8vs04tf6","_id":"ck3offct9006krlfyyla42vb3"},{"post_id":"ck3offcp10011rlfy0balahsp","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct9006mrlfyaojj5x3s"},{"post_id":"ck3offcp10012rlfy9o4kx11v","category_id":"ck3offcrf002urlfyf3eo8fi1","_id":"ck3offct9006orlfycp9d9d8e"},{"post_id":"ck3offcp30013rlfy6dbvvpcc","category_id":"ck3offcrf002urlfyf3eo8fi1","_id":"ck3offct9006qrlfyp7q1atwk"},{"post_id":"ck3offcp40014rlfyxr1gwe18","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offct9006srlfy6vkmwwng"},{"post_id":"ck3offcp40015rlfy57llaje3","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offcta006urlfyh3peyev5"},{"post_id":"ck3offcp50016rlfyoa93cvwt","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offcta006wrlfyolhd4mof"},{"post_id":"ck3offcp50017rlfy8jxau9o9","category_id":"ck3offcrf002urlfyf3eo8fi1","_id":"ck3offcta006yrlfyqgl0lms5"},{"post_id":"ck3offcp60018rlfyrp5okrhf","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offcta0070rlfyyjvjbj2l"},{"post_id":"ck3offcp70019rlfy9r24rtcl","category_id":"ck3offcqm001crlfy903zrlxh","_id":"ck3offcta0072rlfyxsn7gium"},{"post_id":"ck52fvcrb0000o3z4jxpb5pna","category_id":"ck3offcrf002urlfyf3eo8fi1","_id":"ck52gfim10002o3z48h3c9jii"},{"post_id":"ck52fvcrb0000o3z4jxpb5pna","category_id":"ck52jqb9m0001mxz4113q6kt7","_id":"ck52jqb9r0003mxz4frpfqagq"}],"PostTag":[{"post_id":"ck3offcnx0000rlfy7xr4m9ix","tag_id":"ck3offcqj001brlfyuh0zawss","_id":"ck3offct1004lrlfyvdjw8qvp"},{"post_id":"ck3offco20001rlfyuas4uis2","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct2004nrlfycniwdy9e"},{"post_id":"ck3offco20001rlfyuas4uis2","tag_id":"ck3offcqn001frlfy7w0qkwdv","_id":"ck3offct2004prlfy3j9shxu9"},{"post_id":"ck3offco30002rlfyu2bqncx4","tag_id":"ck3offcqj001brlfyuh0zawss","_id":"ck3offct3004rrlfy9pdxr28u"},{"post_id":"ck3offco40003rlfy401c9ncz","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct3004trlfyz53q2c3g"},{"post_id":"ck3offco40003rlfy401c9ncz","tag_id":"ck3offcqj001brlfyuh0zawss","_id":"ck3offct3004vrlfynqgi97qn"},{"post_id":"ck3offco50004rlfyrn3blxpw","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct3004xrlfyt7nol8fl"},{"post_id":"ck3offco50004rlfyrn3blxpw","tag_id":"ck3offcqj001brlfyuh0zawss","_id":"ck3offct3004zrlfyvi6a9ovx"},{"post_id":"ck3offco60005rlfyxds2anvi","tag_id":"ck3offcqw001rrlfy2z0i9ima","_id":"ck3offct30051rlfyahq9ifxd"},{"post_id":"ck3offco60005rlfyxds2anvi","tag_id":"ck3offcqj001brlfyuh0zawss","_id":"ck3offct30053rlfywhim5o96"},{"post_id":"ck3offco70006rlfylf0ka009","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct30055rlfyt6ks79ju"},{"post_id":"ck3offco70006rlfylf0ka009","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offct40057rlfyf3nonfzc"},{"post_id":"ck3offco80007rlfyaew1b5tz","tag_id":"ck3offcr0001zrlfyujuke3ok","_id":"ck3offct40059rlfyair8l2x2"},{"post_id":"ck3offco90008rlfy8wrn1jx0","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct4005brlfyvabi7s2s"},{"post_id":"ck3offco90008rlfy8wrn1jx0","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offct4005drlfy6kitnmo5"},{"post_id":"ck3offcoa0009rlfyblfg9521","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct4005frlfyq7eiv0a8"},{"post_id":"ck3offcoa0009rlfyblfg9521","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offct4005hrlfywdl9myv9"},{"post_id":"ck3offcoc000arlfyh5p4aan2","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct4005jrlfyr8dnz88v"},{"post_id":"ck3offcoc000arlfyh5p4aan2","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offct4005lrlfysttupedn"},{"post_id":"ck3offcod000brlfyndh7s1mi","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct4005nrlfy6dvyn09x"},{"post_id":"ck3offcod000brlfyndh7s1mi","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offct5005prlfyk1umrjdw"},{"post_id":"ck3offcoe000crlfy2nyexxce","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct5005rrlfyggtrq2ed"},{"post_id":"ck3offcoe000crlfy2nyexxce","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offct5005trlfy84mq4rqa"},{"post_id":"ck3offcof000drlfyr6r1f9k4","tag_id":"ck3offcqw001rrlfy2z0i9ima","_id":"ck3offct5005vrlfyxzpmhclz"},{"post_id":"ck3offcof000drlfyr6r1f9k4","tag_id":"ck3offcqj001brlfyuh0zawss","_id":"ck3offct6005xrlfy9f5aygws"},{"post_id":"ck3offcog000erlfy5zii1622","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct6005zrlfyo1wg5x0d"},{"post_id":"ck3offcog000erlfy5zii1622","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offct60061rlfykg3pnls0"},{"post_id":"ck3offcoh000frlfy5uagjyb9","tag_id":"ck3offcqw001rrlfy2z0i9ima","_id":"ck3offct60063rlfy9ajirmfc"},{"post_id":"ck3offcoh000frlfy5uagjyb9","tag_id":"ck3offcqj001brlfyuh0zawss","_id":"ck3offct60065rlfy8pj4tbbx"},{"post_id":"ck3offcoh000grlfyinn51ze0","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct70067rlfyajrupyci"},{"post_id":"ck3offcoh000grlfyinn51ze0","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offct70069rlfy3sjs2eac"},{"post_id":"ck3offcoj000hrlfyo9pe8xvr","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct7006brlfypnivifgj"},{"post_id":"ck3offcol000irlfy7ch67v00","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct7006drlfywao8mklu"},{"post_id":"ck3offcol000irlfy7ch67v00","tag_id":"ck3offcqn001frlfy7w0qkwdv","_id":"ck3offct8006frlfysg8987pu"},{"post_id":"ck3offcom000jrlfyietbw3bm","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct9006hrlfynvbxaddy"},{"post_id":"ck3offcom000jrlfyietbw3bm","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offct9006jrlfymkhyj6el"},{"post_id":"ck3offcon000krlfylh1g08kf","tag_id":"ck3offcrm003brlfyu1oz1lu6","_id":"ck3offct9006lrlfywl2y76qo"},{"post_id":"ck3offcoo000lrlfyv3ukx06r","tag_id":"ck3offcrn003drlfyssk4smby","_id":"ck3offct9006nrlfyld1dqdzc"},{"post_id":"ck3offcop000mrlfyd7enfz6c","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct9006prlfygdsy3aey"},{"post_id":"ck3offcop000mrlfyd7enfz6c","tag_id":"ck3offcrq003hrlfy0m52fx1i","_id":"ck3offct9006rrlfylwd2s092"},{"post_id":"ck3offcop000nrlfyv36n66p4","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offct9006trlfyzr4ag25t"},{"post_id":"ck3offcoq000orlfy6cpgno7z","tag_id":"ck3offcrn003drlfyssk4smby","_id":"ck3offcta006vrlfyhraxjlv5"},{"post_id":"ck3offcou000prlfyantaeowz","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offcta006xrlfy9h6jlt8r"},{"post_id":"ck3offcou000prlfyantaeowz","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offcta006zrlfyptddrtus"},{"post_id":"ck3offcou000qrlfy8hq4f83l","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offcta0071rlfysxqd5ywb"},{"post_id":"ck3offcov000rrlfyqe0j8z40","tag_id":"ck3offcqw001rrlfy2z0i9ima","_id":"ck3offctb0073rlfy0klq6t94"},{"post_id":"ck3offcov000rrlfyqe0j8z40","tag_id":"ck3offcqj001brlfyuh0zawss","_id":"ck3offctb0074rlfy97fhg99l"},{"post_id":"ck3offcov000srlfy4htgkoph","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctb0075rlfyqf0meko7"},{"post_id":"ck3offcov000srlfy4htgkoph","tag_id":"ck3offcrx003vrlfyy7cidud9","_id":"ck3offctb0076rlfy1ubj23an"},{"post_id":"ck3offcow000trlfyeo0vtb90","tag_id":"ck3offcqj001brlfyuh0zawss","_id":"ck3offctb0077rlfyxkrlb2fn"},{"post_id":"ck3offcox000urlfyju6c2vyi","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctb0078rlfyi250icby"},{"post_id":"ck3offcox000vrlfy06dgucxl","tag_id":"ck3offcry003yrlfyew29g7e3","_id":"ck3offctb0079rlfyehev511t"},{"post_id":"ck3offcoy000wrlfyp3uya4nq","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctb007arlfyw6fkk1fc"},{"post_id":"ck3offcoy000xrlfyna7ldnm5","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctb007brlfyhx0rry2h"},{"post_id":"ck3offcoy000xrlfyna7ldnm5","tag_id":"ck3offcrx003vrlfyy7cidud9","_id":"ck3offctb007crlfyn4fppw3q"},{"post_id":"ck3offcoz000yrlfy9x3vfls3","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctb007drlfyybqa6nja"},{"post_id":"ck3offcoz000yrlfy9x3vfls3","tag_id":"ck3offcrx003vrlfyy7cidud9","_id":"ck3offctb007erlfy87r0e2e6"},{"post_id":"ck3offcoz000zrlfyds494d56","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctb007frlfyu4gksy1u"},{"post_id":"ck3offcoz000zrlfyds494d56","tag_id":"ck3offcrx003vrlfyy7cidud9","_id":"ck3offctb007grlfykwmgzhyo"},{"post_id":"ck3offcp00010rlfytmtobdp2","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctb007hrlfygy6epliu"},{"post_id":"ck3offcp00010rlfytmtobdp2","tag_id":"ck3offcs30047rlfyrf7so4gc","_id":"ck3offctb007irlfynstpiloy"},{"post_id":"ck3offcp10011rlfy0balahsp","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctb007jrlfyjopmgp6j"},{"post_id":"ck3offcp10012rlfy9o4kx11v","tag_id":"ck3offcs40049rlfydqpctfoc","_id":"ck3offctc007krlfy9pynvevf"},{"post_id":"ck3offcp30013rlfy6dbvvpcc","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctc007lrlfyj8m4q911"},{"post_id":"ck3offcp40014rlfyxr1gwe18","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctc007mrlfyk53pqlum"},{"post_id":"ck3offcp40015rlfy57llaje3","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctc007nrlfylvxrsuys"},{"post_id":"ck3offcp40015rlfy57llaje3","tag_id":"ck3offcqz001xrlfy09952t90","_id":"ck3offctd007orlfydpp0h6ml"},{"post_id":"ck3offcp50016rlfyoa93cvwt","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctd007prlfyvxdytr3u"},{"post_id":"ck3offcp50016rlfyoa93cvwt","tag_id":"ck3offcrx003vrlfyy7cidud9","_id":"ck3offctd007qrlfy860ucj2f"},{"post_id":"ck3offcp50017rlfy8jxau9o9","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctd007rrlfyr10rw3dr"},{"post_id":"ck3offcp50017rlfy8jxau9o9","tag_id":"ck3offcs30047rlfyrf7so4gc","_id":"ck3offctd007srlfy9g4vuz8a"},{"post_id":"ck3offcp60018rlfyrp5okrhf","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctd007trlfyb21w0jc4"},{"post_id":"ck3offcp70019rlfy9r24rtcl","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck3offctd007urlfyxx403uo3"},{"post_id":"ck3offcp70019rlfy9r24rtcl","tag_id":"ck3offcrx003vrlfyy7cidud9","_id":"ck3offctd007vrlfys9buo0uw"},{"post_id":"ck52fvcrb0000o3z4jxpb5pna","tag_id":"ck3offcqm001drlfyq8e9wxo8","_id":"ck52gfim10001o3z4z8gtplm3"},{"post_id":"ck52fvcrb0000o3z4jxpb5pna","tag_id":"ck3offcqj001brlfyuh0zawss","_id":"ck52jqb9m0000mxz4yde1peot"},{"post_id":"ck52fvcrb0000o3z4jxpb5pna","tag_id":"ck3offcrq003hrlfy0m52fx1i","_id":"ck52jqb9n0002mxz4efuqc5il"},{"post_id":"ck3offco40003rlfy401c9ncz","tag_id":"ck3offcrq003hrlfy0m52fx1i","_id":"ck52jrbmb0004mxz4immu8iuk"}],"Tag":[{"name":"mgnt","_id":"ck3offcqj001brlfyuh0zawss"},{"name":"tech","_id":"ck3offcqm001drlfyq8e9wxo8"},{"name":"cloud","_id":"ck3offcqn001frlfy7w0qkwdv"},{"name":"book","_id":"ck3offcqw001rrlfy2z0i9ima"},{"name":"deeplearning","_id":"ck3offcqz001xrlfy09952t90"},{"name":"math","_id":"ck3offcr0001zrlfyujuke3ok"},{"name":"Security","_id":"ck3offcrm003brlfyu1oz1lu6"},{"name":"misc","_id":"ck3offcrn003drlfyssk4smby"},{"name":"career","_id":"ck3offcrq003hrlfy0m52fx1i"},{"name":"python","_id":"ck3offcrx003vrlfyy7cidud9"},{"name":"security","_id":"ck3offcry003yrlfyew29g7e3"},{"name":"med","_id":"ck3offcs30047rlfyrf7so4gc"},{"name":"operation","_id":"ck3offcs40049rlfydqpctfoc"}]}}